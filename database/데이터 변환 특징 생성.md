## 데이터 변환 특징 생성

---

- 범주형 데이터 변환
    - One-hot Encoding 기반의 변환을 통해 데이터 처리
- 변수 결합 및 분해 기반 특징 구축
    - 기존 데이터를 활용하여 변수 간의 조합을 통해 신규 특징 구축
- 차원 축소 기반 특징 생성
    - 고차원 원시 데이터를 저차원 데이터로 축소하도록 새로운 특징을 생성
    - 주성분 분석
    - 군집 분석

### Feature Creation

- 원본 데이터의 조합, 변환 등을 기반하여 새로운 특징들을 구축 및 생성하는 방법
    - 특징이란 원 데이터의 변환을 통해 생성되는 숫자적인 표현으로 모델링에 적용하기 위해 만들어진 데이터의 새로운 컬럼들, 변수를 의미
- 원본 데이터로 특징을 새롭게 생성하여 분석 과정 내 성능과 효율성을 확보하고자 함
    - 수많은 데이터를 기반으로 적절한 특징들을 생성해서 모델링 학습 과정 내 활용 가능한 유의미한 재료로 활용하며, 이를 통해 연산 비용의 효율성을 추구하고, 모델의 성능을 확보함

### 목적 및 필요성

<img width="755" alt="feature_creation" src="https://github.com/zacinthepark/TIL/assets/86648892/978c98d5-8744-461d-88d9-8cd44d8d8822">

- 특징 생성이라 불리는 과정은 feature engineering과 밀접한 관련이 있음
- Feature Engineering: 원시 데이터로부터 적절하고 올바른 특징들을 만들어내는 일련의 과정
- 데이터 분석 경진대회 주요 수상자들이 많이 언급하는 부분이 Feature Engineering, 적절한 특징을 생성하는 부분이다
- 분석 알고리즘 이전에 데이터로부터 좋은 특징을 확보하자

### 특징 생성 방안

<img width="757" alt="feature_creation_1" src="https://github.com/zacinthepark/TIL/assets/86648892/a9dc9357-4212-4944-9526-f23e16e0bafc">

### 범주 인코딩

<img width="715" alt="encoding" src="https://github.com/zacinthepark/TIL/assets/86648892/46dda168-6d6d-4147-9c3c-c77ab725bca2">

- 범주형 데이터의 알고리즘 적용을 위한 수치형 변환
- 많이 알려진 것이 One-hot Encoding
- One-hot Encoding의 경우 범주 수가 너무 많을 경우 만들어야 할 변수의 개수도 늘어나기에 이를 고려하며 시행해야함

### 결합 기반 특징 생성

<img width="756" alt="add_divide_subtract_multiply" src="https://github.com/zacinthepark/TIL/assets/86648892/a36b0296-f47d-4eef-a43b-0fa009a98e5b">

- 변수 간의 결합을 통해 새로운 의미를 지닌 특징을 생성
- 파생변수를 생성할 때 많이 고려되는 방안

### 분해 기반 특징 생성

<img width="714" alt="seperate" src="https://github.com/zacinthepark/TIL/assets/86648892/a5c7a3e2-7274-4dd8-bb82-6f377d2f46c5">

- 변수의 분해를 통해 새로운 의미를 지닌 특징을 생성
- 이력성 데이터 등에 사용

### 차원 축소 목적 특징 생성

1. PCA (Principal Component Analysis)

<img width="750" alt="pca" src="https://github.com/zacinthepark/TIL/assets/86648892/8c425baa-d030-45b7-858a-ad4c70aa14ec">

- 변수들이 지닌 정보를 최대한 확보하는 저차원 데이터로 생성
- PC1은 원본 데이터의 분포를 가장 많이 보존하는 성분으로 구성
- PC2는 PC1과 서로 독립적이고, 가장 많은 분포를 보존하는 성분으로 구성

2. 군집 분석 (Featurization via Clustering)

<img width="755" alt="clustering" src="https://github.com/zacinthepark/TIL/assets/86648892/917f03ad-7705-4d1b-8a77-d1f9441efa0d">

- 군집 분석 기반의 고차원 데이터를 하나의 특징으로 차원 축소
- 일반적으로 군집 분석이란, 여러 변수를 지닌 레코드들끼리의 유사성을 보고, 유사한 데이터들끼리 하나의 군집으로 구성하는 비지도학습의 대표적 방법
- 획득한 군집 결과를 stacking하여 분류나 회귀 등 추후에 문제 해결을 위한 입력 변수로 활용
- 유의할 점은 생성한 특징, 각 군집 결과가 해당 원래 관측치들을 잘 반영한, 올바른 군집 결과로 도출되어야만 향후 모델링에 결과와 품질을 확보

### 실습

#### 범주 인코딩

- 숫자로 표현되지 않은 데이터를 수치형으로 표현하고 모델링에 적용하기 위한 과정

```python
import numpy as np
import pandas as pd
```

```python
# 데이터 로딩 및 개요 확인
data = pd.read_csv('./data/encoding_sample_data.csv')
# data = pd.read_csv('./data/encoding_sample_data.csv', encoding = 'cp949')
data.head()
```

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>purchase_YN</th>
      <th>gender</th>
      <th>city</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2020-10-18 23:29</td>
      <td>1</td>
      <td>M</td>
      <td>서울</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2020-11-15 1:32</td>
      <td>0</td>
      <td>F</td>
      <td>대전</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2020-11-25 4:28</td>
      <td>0</td>
      <td>M</td>
      <td>광주</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2020-10-24 10:06</td>
      <td>0</td>
      <td>M</td>
      <td>대구</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2020-11-12 1:41</td>
      <td>0</td>
      <td>F</td>
      <td>대전</td>
    </tr>
  </tbody>
</table>
</div>

```python
data['city'].value_counts()
```

<pre>
city
대전    6
서울    4
광주    2
부산    2
대구    1
Name: count, dtype: int64
</pre>

```python
# city라는 범주형 변수 one_hot Encoding
# Pandas의 get_dummies 함수 활용하여 쉽게 구현 가능

encoding_data = data.copy()
encoding_data = pd.get_dummies(encoding_data, columns=['city'])
encoding_data.head()

# 기존 city 변수 내 5개의 범주가 존재
# get_dummies 함수를 통해 원본 데이터의 city 변수 대신 각 범주별 변수가 생성 (1개 변수 -> 5개 변수)
```

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>purchase_YN</th>
      <th>gender</th>
      <th>city_광주</th>
      <th>city_대구</th>
      <th>city_대전</th>
      <th>city_부산</th>
      <th>city_서울</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2020-10-18 23:29</td>
      <td>1</td>
      <td>M</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2020-11-15 1:32</td>
      <td>0</td>
      <td>F</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2020-11-25 4:28</td>
      <td>0</td>
      <td>M</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2020-10-24 10:06</td>
      <td>0</td>
      <td>M</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2020-11-12 1:41</td>
      <td>0</td>
      <td>F</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>

- One-hot Encoding은 각 범주의 요소마다 별도 컬럼으로 생성하여 True, False를 표현
- 기계 학습의 많은 알고리즘은 수치형 데이터를 입력값으로 받아야함
- 따라서, 범주형 변수의 One-hot Encoding 기법을 활용하여 기계학습 적용의 제약점을 해소 가능

#### 결합 및 분해 기반 특징 생성

```python
# 시간대 별 파악 목적
# 어느 시간대 혹은 요일 별로 구매 결정이 높아지는지 확인하고자 할 때

creation_data = data.copy()
creation_data.info()
```

<pre>
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 15 entries, 0 to 14
Data columns (total 4 columns):
 #   Column       Non-Null Count  Dtype 
---  ------       --------------  ----- 
 0   date         15 non-null     object
 1   purchase_YN  15 non-null     int64 
 2   gender       15 non-null     object
 3   city         15 non-null     object
dtypes: int64(1), object(3)
memory usage: 612.0+ bytes
</pre>

```python
# 기존 범주형 변수인 date 컬럼을 datetime 형식으로 변환
creation_data['date'] = pd.to_datetime(creation_data['date'])
creation_data.head()
```

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>purchase_YN</th>
      <th>gender</th>
      <th>city</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2020-10-18 23:29:00</td>
      <td>1</td>
      <td>M</td>
      <td>서울</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2020-11-15 01:32:00</td>
      <td>0</td>
      <td>F</td>
      <td>대전</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2020-11-25 04:28:00</td>
      <td>0</td>
      <td>M</td>
      <td>광주</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2020-10-24 10:06:00</td>
      <td>0</td>
      <td>M</td>
      <td>대구</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2020-11-12 01:41:00</td>
      <td>0</td>
      <td>F</td>
      <td>대전</td>
    </tr>
  </tbody>
</table>
</div>

```python
# 데이터 타입 확인
creation_data.info()
```

<pre>
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 15 entries, 0 to 14
Data columns (total 4 columns):
 #   Column       Non-Null Count  Dtype         
---  ------       --------------  -----         
 0   date         15 non-null     datetime64[ns]
 1   purchase_YN  15 non-null     int64         
 2   gender       15 non-null     object        
 3   city         15 non-null     object        
dtypes: datetime64[ns](1), int64(1), object(2)
memory usage: 612.0+ bytes
</pre>

```python
# date 컬럼을 연/월/일/요일 등의 의미를 지닌 변수로 분해
creation_data['year'] = creation_data['date'].dt.year  # 연도
creation_data['month'] = creation_data['date'].dt.month  # 월
creation_data['day'] = creation_data['date'].dt.day  # 일
creation_data['hour'] = creation_data['date'].dt.hour  # 시간
creation_data['dayofweek'] = creation_data['date'].dt.dayofweek  # 요일 (월 = 0)
creation_data.head()
```

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>purchase_YN</th>
      <th>gender</th>
      <th>city</th>
      <th>year</th>
      <th>month</th>
      <th>day</th>
      <th>hour</th>
      <th>dayofweek</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2020-10-18 23:29:00</td>
      <td>1</td>
      <td>M</td>
      <td>서울</td>
      <td>2020</td>
      <td>10</td>
      <td>18</td>
      <td>23</td>
      <td>6</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2020-11-15 01:32:00</td>
      <td>0</td>
      <td>F</td>
      <td>대전</td>
      <td>2020</td>
      <td>11</td>
      <td>15</td>
      <td>1</td>
      <td>6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2020-11-25 04:28:00</td>
      <td>0</td>
      <td>M</td>
      <td>광주</td>
      <td>2020</td>
      <td>11</td>
      <td>25</td>
      <td>4</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2020-10-24 10:06:00</td>
      <td>0</td>
      <td>M</td>
      <td>대구</td>
      <td>2020</td>
      <td>10</td>
      <td>24</td>
      <td>10</td>
      <td>5</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2020-11-12 01:41:00</td>
      <td>0</td>
      <td>F</td>
      <td>대전</td>
      <td>2020</td>
      <td>11</td>
      <td>12</td>
      <td>1</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>

```python
# 오전/오후의 의미를 지닌 변수 생성을 위한 결합 방안 (아래 예제는 변수 내 Group으로 결합)
# AM PM 구분 변수 생성
creation_data['ampm'] = 'AM'
creation_data.loc[creation_data['hour'] > 12, 'ampm'] = 'PM'
creation_data
```

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>purchase_YN</th>
      <th>gender</th>
      <th>city</th>
      <th>year</th>
      <th>month</th>
      <th>day</th>
      <th>hour</th>
      <th>dayofweek</th>
      <th>ampm</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2020-10-18 23:29:00</td>
      <td>1</td>
      <td>M</td>
      <td>서울</td>
      <td>2020</td>
      <td>10</td>
      <td>18</td>
      <td>23</td>
      <td>6</td>
      <td>PM</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2020-11-15 01:32:00</td>
      <td>0</td>
      <td>F</td>
      <td>대전</td>
      <td>2020</td>
      <td>11</td>
      <td>15</td>
      <td>1</td>
      <td>6</td>
      <td>AM</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2020-11-25 04:28:00</td>
      <td>0</td>
      <td>M</td>
      <td>광주</td>
      <td>2020</td>
      <td>11</td>
      <td>25</td>
      <td>4</td>
      <td>2</td>
      <td>AM</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2020-10-24 10:06:00</td>
      <td>0</td>
      <td>M</td>
      <td>대구</td>
      <td>2020</td>
      <td>10</td>
      <td>24</td>
      <td>10</td>
      <td>5</td>
      <td>AM</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2020-11-12 01:41:00</td>
      <td>0</td>
      <td>F</td>
      <td>대전</td>
      <td>2020</td>
      <td>11</td>
      <td>12</td>
      <td>1</td>
      <td>3</td>
      <td>AM</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2020-11-12 01:41:00</td>
      <td>1</td>
      <td>M</td>
      <td>광주</td>
      <td>2020</td>
      <td>11</td>
      <td>12</td>
      <td>1</td>
      <td>3</td>
      <td>AM</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2020-11-12 01:41:00</td>
      <td>0</td>
      <td>M</td>
      <td>대전</td>
      <td>2020</td>
      <td>11</td>
      <td>12</td>
      <td>1</td>
      <td>3</td>
      <td>AM</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2020-11-12 01:41:00</td>
      <td>1</td>
      <td>M</td>
      <td>대전</td>
      <td>2020</td>
      <td>11</td>
      <td>12</td>
      <td>1</td>
      <td>3</td>
      <td>AM</td>
    </tr>
    <tr>
      <th>8</th>
      <td>2020-11-12 01:41:00</td>
      <td>0</td>
      <td>M</td>
      <td>서울</td>
      <td>2020</td>
      <td>11</td>
      <td>12</td>
      <td>1</td>
      <td>3</td>
      <td>AM</td>
    </tr>
    <tr>
      <th>9</th>
      <td>2020-11-12 01:41:00</td>
      <td>1</td>
      <td>M</td>
      <td>서울</td>
      <td>2020</td>
      <td>11</td>
      <td>12</td>
      <td>1</td>
      <td>3</td>
      <td>AM</td>
    </tr>
    <tr>
      <th>10</th>
      <td>2020-11-12 01:41:00</td>
      <td>0</td>
      <td>M</td>
      <td>대전</td>
      <td>2020</td>
      <td>11</td>
      <td>12</td>
      <td>1</td>
      <td>3</td>
      <td>AM</td>
    </tr>
    <tr>
      <th>11</th>
      <td>2020-11-12 01:41:00</td>
      <td>1</td>
      <td>M</td>
      <td>대전</td>
      <td>2020</td>
      <td>11</td>
      <td>12</td>
      <td>1</td>
      <td>3</td>
      <td>AM</td>
    </tr>
    <tr>
      <th>12</th>
      <td>2020-11-12 01:41:00</td>
      <td>0</td>
      <td>M</td>
      <td>부산</td>
      <td>2020</td>
      <td>11</td>
      <td>12</td>
      <td>1</td>
      <td>3</td>
      <td>AM</td>
    </tr>
    <tr>
      <th>13</th>
      <td>2020-11-12 01:41:00</td>
      <td>1</td>
      <td>F</td>
      <td>서울</td>
      <td>2020</td>
      <td>11</td>
      <td>12</td>
      <td>1</td>
      <td>3</td>
      <td>AM</td>
    </tr>
    <tr>
      <th>14</th>
      <td>2020-11-12 01:41:00</td>
      <td>0</td>
      <td>M</td>
      <td>부산</td>
      <td>2020</td>
      <td>11</td>
      <td>12</td>
      <td>1</td>
      <td>3</td>
      <td>AM</td>
    </tr>
  </tbody>
</table>
</div>

- 연/월/일 등을 분석가 기준에 맞추어 시간대의 의미를 지닌 신규 파생 변수로 분해 및 결합 가능
- dayofweek 변수의 경우 평일/주말로 구분하는 신규 파생 변수로 분해 및 결합 가능

#### 차원 축소 기반 특징 생성 (1): PCA (주성분 분석)

- 여러 개의 변수를 지닌 고차원 데이터를 저차원으로 변환하도록 주성분들을 생성하는 알고리즘
- 원 변수들이 지닌 정보를 최대한 확보하는 저차원 데이터로 생성하는 방법

```python
# 데이터 로딩
cancer = pd.read_csv('./data/wdbc.data', header=None)

# 데이터 컬럼명 지정
cancer.columns = ['id', 'diagnosis', 'radius1', 'texture1', 'perimeter1', 'area1', 'smoothness1', 'compactness1',
                  'concavity1', 'concave_points1', 'symmetry1', 'fractal_dimentsion1', 'radius2', 'texture2',
                  'perimeter2', 'area2', 'smoothness2', 'compactness2', 'concavity2', 'concave_points2', 'symmetry2',
                  'fractal_dimension2', 'radius3', 'texture3', 'perimeter3', 'area3', 'smoothness3', 'compactness3', 
                  'concavity3', 'concave_points3', 'symmetry3', 'fractal_dimension3']

# ID를 인덱스화
cancer = cancer.set_index('id')
cancer
```

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>diagnosis</th>
      <th>radius1</th>
      <th>texture1</th>
      <th>perimeter1</th>
      <th>area1</th>
      <th>smoothness1</th>
      <th>compactness1</th>
      <th>concavity1</th>
      <th>concave_points1</th>
      <th>symmetry1</th>
      <th>...</th>
      <th>radius3</th>
      <th>texture3</th>
      <th>perimeter3</th>
      <th>area3</th>
      <th>smoothness3</th>
      <th>compactness3</th>
      <th>concavity3</th>
      <th>concave_points3</th>
      <th>symmetry3</th>
      <th>fractal_dimension3</th>
    </tr>
    <tr>
      <th>id</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>842302</th>
      <td>M</td>
      <td>17.99</td>
      <td>10.38</td>
      <td>122.80</td>
      <td>1001.0</td>
      <td>0.11840</td>
      <td>0.27760</td>
      <td>0.30010</td>
      <td>0.14710</td>
      <td>0.2419</td>
      <td>...</td>
      <td>25.380</td>
      <td>17.33</td>
      <td>184.60</td>
      <td>2019.0</td>
      <td>0.16220</td>
      <td>0.66560</td>
      <td>0.7119</td>
      <td>0.2654</td>
      <td>0.4601</td>
      <td>0.11890</td>
    </tr>
    <tr>
      <th>842517</th>
      <td>M</td>
      <td>20.57</td>
      <td>17.77</td>
      <td>132.90</td>
      <td>1326.0</td>
      <td>0.08474</td>
      <td>0.07864</td>
      <td>0.08690</td>
      <td>0.07017</td>
      <td>0.1812</td>
      <td>...</td>
      <td>24.990</td>
      <td>23.41</td>
      <td>158.80</td>
      <td>1956.0</td>
      <td>0.12380</td>
      <td>0.18660</td>
      <td>0.2416</td>
      <td>0.1860</td>
      <td>0.2750</td>
      <td>0.08902</td>
    </tr>
    <tr>
      <th>84300903</th>
      <td>M</td>
      <td>19.69</td>
      <td>21.25</td>
      <td>130.00</td>
      <td>1203.0</td>
      <td>0.10960</td>
      <td>0.15990</td>
      <td>0.19740</td>
      <td>0.12790</td>
      <td>0.2069</td>
      <td>...</td>
      <td>23.570</td>
      <td>25.53</td>
      <td>152.50</td>
      <td>1709.0</td>
      <td>0.14440</td>
      <td>0.42450</td>
      <td>0.4504</td>
      <td>0.2430</td>
      <td>0.3613</td>
      <td>0.08758</td>
    </tr>
    <tr>
      <th>84348301</th>
      <td>M</td>
      <td>11.42</td>
      <td>20.38</td>
      <td>77.58</td>
      <td>386.1</td>
      <td>0.14250</td>
      <td>0.28390</td>
      <td>0.24140</td>
      <td>0.10520</td>
      <td>0.2597</td>
      <td>...</td>
      <td>14.910</td>
      <td>26.50</td>
      <td>98.87</td>
      <td>567.7</td>
      <td>0.20980</td>
      <td>0.86630</td>
      <td>0.6869</td>
      <td>0.2575</td>
      <td>0.6638</td>
      <td>0.17300</td>
    </tr>
    <tr>
      <th>84358402</th>
      <td>M</td>
      <td>20.29</td>
      <td>14.34</td>
      <td>135.10</td>
      <td>1297.0</td>
      <td>0.10030</td>
      <td>0.13280</td>
      <td>0.19800</td>
      <td>0.10430</td>
      <td>0.1809</td>
      <td>...</td>
      <td>22.540</td>
      <td>16.67</td>
      <td>152.20</td>
      <td>1575.0</td>
      <td>0.13740</td>
      <td>0.20500</td>
      <td>0.4000</td>
      <td>0.1625</td>
      <td>0.2364</td>
      <td>0.07678</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>926424</th>
      <td>M</td>
      <td>21.56</td>
      <td>22.39</td>
      <td>142.00</td>
      <td>1479.0</td>
      <td>0.11100</td>
      <td>0.11590</td>
      <td>0.24390</td>
      <td>0.13890</td>
      <td>0.1726</td>
      <td>...</td>
      <td>25.450</td>
      <td>26.40</td>
      <td>166.10</td>
      <td>2027.0</td>
      <td>0.14100</td>
      <td>0.21130</td>
      <td>0.4107</td>
      <td>0.2216</td>
      <td>0.2060</td>
      <td>0.07115</td>
    </tr>
    <tr>
      <th>926682</th>
      <td>M</td>
      <td>20.13</td>
      <td>28.25</td>
      <td>131.20</td>
      <td>1261.0</td>
      <td>0.09780</td>
      <td>0.10340</td>
      <td>0.14400</td>
      <td>0.09791</td>
      <td>0.1752</td>
      <td>...</td>
      <td>23.690</td>
      <td>38.25</td>
      <td>155.00</td>
      <td>1731.0</td>
      <td>0.11660</td>
      <td>0.19220</td>
      <td>0.3215</td>
      <td>0.1628</td>
      <td>0.2572</td>
      <td>0.06637</td>
    </tr>
    <tr>
      <th>926954</th>
      <td>M</td>
      <td>16.60</td>
      <td>28.08</td>
      <td>108.30</td>
      <td>858.1</td>
      <td>0.08455</td>
      <td>0.10230</td>
      <td>0.09251</td>
      <td>0.05302</td>
      <td>0.1590</td>
      <td>...</td>
      <td>18.980</td>
      <td>34.12</td>
      <td>126.70</td>
      <td>1124.0</td>
      <td>0.11390</td>
      <td>0.30940</td>
      <td>0.3403</td>
      <td>0.1418</td>
      <td>0.2218</td>
      <td>0.07820</td>
    </tr>
    <tr>
      <th>927241</th>
      <td>M</td>
      <td>20.60</td>
      <td>29.33</td>
      <td>140.10</td>
      <td>1265.0</td>
      <td>0.11780</td>
      <td>0.27700</td>
      <td>0.35140</td>
      <td>0.15200</td>
      <td>0.2397</td>
      <td>...</td>
      <td>25.740</td>
      <td>39.42</td>
      <td>184.60</td>
      <td>1821.0</td>
      <td>0.16500</td>
      <td>0.86810</td>
      <td>0.9387</td>
      <td>0.2650</td>
      <td>0.4087</td>
      <td>0.12400</td>
    </tr>
    <tr>
      <th>92751</th>
      <td>B</td>
      <td>7.76</td>
      <td>24.54</td>
      <td>47.92</td>
      <td>181.0</td>
      <td>0.05263</td>
      <td>0.04362</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.1587</td>
      <td>...</td>
      <td>9.456</td>
      <td>30.37</td>
      <td>59.16</td>
      <td>268.6</td>
      <td>0.08996</td>
      <td>0.06444</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>0.2871</td>
      <td>0.07039</td>
    </tr>
  </tbody>
</table>
<p>569 rows × 31 columns</p>
</div>

```python
# Input 및 Target 구분
input_df = cancer.drop(['diagnosis'], axis=1)  # 열을 추출해야 하므로 axis=1로 지정
print(np.shape(input_df))

target_df = cancer[['diagnosis']]
print(np.shape(target_df))
```

<pre>
(569, 30)
(569, 1)
</pre>

```python
# 30개의 독립변수(Input 변수)로 이루어진 데이터를 주성분 분석
# 주성분 분석 수행 이전, 각 변수의 스케일이 서로 다르기 때문에 표준화 수행
# 원 데이터의 측정값들의 기준, 수치형 변수의 범위들이 모두 다르기 때문
from sklearn.preprocessing import StandardScaler
std_scaler = StandardScaler()

# Input값을 scaling 실행 후 변환
std_scaler.fit(input_df)
input_scaled = std_scaler.transform(input_df)

# array 형태로 반환됨
input_scaled
```

<pre>
array([[ 1.09706398, -2.07333501,  1.26993369, ...,  2.29607613,
         2.75062224,  1.93701461],
       [ 1.82982061, -0.35363241,  1.68595471, ...,  1.0870843 ,
        -0.24388967,  0.28118999],
       [ 1.57988811,  0.45618695,  1.56650313, ...,  1.95500035,
         1.152255  ,  0.20139121],
       ...,
       [ 0.70228425,  2.0455738 ,  0.67267578, ...,  0.41406869,
        -1.10454895, -0.31840916],
       [ 1.83834103,  2.33645719,  1.98252415, ...,  2.28998549,
         1.91908301,  2.21963528],
       [-1.80840125,  1.22179204, -1.81438851, ..., -1.74506282,
        -0.04813821, -0.75120669]])
</pre>

```python
# 주성분 분석 수행
from sklearn.decomposition import PCA

# 2개 주성분만 유지시키도록 수행
# 30개 변수의 데이터를 2개의 주성분으로 남도록 변환

pca = PCA(n_components=2)
pca.fit(input_scaled)
X_pca = pca.transform(input_scaled)
X_pca
```

<pre>
array([[ 9.19283683,  1.94858307],
       [ 2.3878018 , -3.76817174],
       [ 5.73389628, -1.0751738 ],
       ...,
       [ 1.25617928, -1.90229671],
       [10.37479406,  1.67201011],
       [-5.4752433 , -0.67063679]])
</pre>

```python
# PCA 수행된 Input 데이터 확인
# 2개의 주성분 확인
X_pca_df = pd.DataFrame(X_pca, columns = ['pc1', 'pc2'])
X_pca_df
```

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>pc1</th>
      <th>pc2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>9.192837</td>
      <td>1.948583</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2.387802</td>
      <td>-3.768172</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5.733896</td>
      <td>-1.075174</td>
    </tr>
    <tr>
      <th>3</th>
      <td>7.122953</td>
      <td>10.275589</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3.935302</td>
      <td>-1.948072</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>564</th>
      <td>6.439315</td>
      <td>-3.576817</td>
    </tr>
    <tr>
      <th>565</th>
      <td>3.793382</td>
      <td>-3.584048</td>
    </tr>
    <tr>
      <th>566</th>
      <td>1.256179</td>
      <td>-1.902297</td>
    </tr>
    <tr>
      <th>567</th>
      <td>10.374794</td>
      <td>1.672010</td>
    </tr>
    <tr>
      <th>568</th>
      <td>-5.475243</td>
      <td>-0.670637</td>
    </tr>
  </tbody>
</table>
<p>569 rows × 2 columns</p>
</div>

- 30개의 변수를 지닌 데이터가 2개의 특징들로 압축되었음

```python
# 2개의 주성분으로 구성된 컬럼들이 Target을 구분하기에 효율적인지 시각화로 확인
import matplotlib.pyplot as plt
import seaborn as sns
```

```python
# 산점도로 2개의 주성분을 시각화
ax = sns.scatterplot(x='pc1', y='pc2', data=X_pca_df)
```

<img width="687" alt="scatterplot1" src="https://github.com/zacinthepark/TIL/assets/86648892/70b36004-8013-44f0-83a0-b4ed80052bb0">

```python
# Target과 확인을 위해 주성분 분석을 수행한 Input 데이터와 기존 Target 데이터를 Merge
# pca_df 생성: 2개의 주성분 (Input) 및 1개의 Target (Diagnosis)
target_df = target_df.reset_index()
pca_df = pd.merge(X_pca_df, target_df, left_index=True, right_index=True, how='inner')
pca_df = pca_df[['pc1', 'pc2', 'diagnosis']]
pca_df
```

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>pc1</th>
      <th>pc2</th>
      <th>diagnosis</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>9.192837</td>
      <td>1.948583</td>
      <td>M</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2.387802</td>
      <td>-3.768172</td>
      <td>M</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5.733896</td>
      <td>-1.075174</td>
      <td>M</td>
    </tr>
    <tr>
      <th>3</th>
      <td>7.122953</td>
      <td>10.275589</td>
      <td>M</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3.935302</td>
      <td>-1.948072</td>
      <td>M</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>564</th>
      <td>6.439315</td>
      <td>-3.576817</td>
      <td>M</td>
    </tr>
    <tr>
      <th>565</th>
      <td>3.793382</td>
      <td>-3.584048</td>
      <td>M</td>
    </tr>
    <tr>
      <th>566</th>
      <td>1.256179</td>
      <td>-1.902297</td>
      <td>M</td>
    </tr>
    <tr>
      <th>567</th>
      <td>10.374794</td>
      <td>1.672010</td>
      <td>M</td>
    </tr>
    <tr>
      <th>568</th>
      <td>-5.475243</td>
      <td>-0.670637</td>
      <td>B</td>
    </tr>
  </tbody>
</table>
<p>569 rows × 3 columns</p>
</div>

```python
# 클래스를 색깔로 구분하여 처음 2개의 주성분으로 Target과 비교
ax = sns.scatterplot(x='pc1', y='pc2', hue='diagnosis', data=pca_df, palette=['green', 'red'])
```

<img width="673" alt="scatterplot2" src="https://github.com/zacinthepark/TIL/assets/86648892/abcdb274-3b93-4d74-bac2-56c9bef4182a">

- 그래프 내 왼쪽, 오른쪽으로 구분됨
- 완벽한 구분으로 보기는 어렵지만, 2개의 주성분으로 봤을 때 어느정도 구분은 이루어짐
- 각 Target 별로 집중 분포된 지역이 있기 때문
- 이처럼 2개의 주성분을 모델링에 활용한다고 하더라도 꽤 나쁘지 않은 결과를 가질 것으로 보여짐
- 또한 30개의 컬럼을 2개만 써도 되도록 바꾼다면, 연산의 효율성은 엄청나게 확보될 것임


- 주성분을 선택하는 다른 방안
- 유지시킬 주성분 개수가 아닌 분산의 설명가능 수준을 비율로 선택 가능
    - pca = PCA(n_components = 0.8):
    - 주성분의 수는 전체 분산의 최소 80% 수준에서 설명하는 수준에서 자동으로 선택
- 이를 통해 수치를 변경하면서 추출되는 주성분의 수 확인 가능하며, 이는 분산에 기초한 주성분 개수를 선택하는 부분에서 얼마나 많은 주성분을 사용할 것인지 확인해야할 때 사용 가능

```python
# 전체 분산의 최소 80% 수준에서 설명하는 수준의 주성분 확보
pca = PCA(n_components=0.8)
pca.fit(input_scaled)
X_pca = pca.transform(input_scaled)
X_pca_df = pd.DataFrame(X_pca)
X_pca_df
```

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>9.192837</td>
      <td>1.948583</td>
      <td>-1.123166</td>
      <td>3.633731</td>
      <td>-1.195110</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2.387802</td>
      <td>-3.768172</td>
      <td>-0.529293</td>
      <td>1.118264</td>
      <td>0.621775</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5.733896</td>
      <td>-1.075174</td>
      <td>-0.551748</td>
      <td>0.912083</td>
      <td>-0.177086</td>
    </tr>
    <tr>
      <th>3</th>
      <td>7.122953</td>
      <td>10.275589</td>
      <td>-3.232790</td>
      <td>0.152547</td>
      <td>-2.960878</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3.935302</td>
      <td>-1.948072</td>
      <td>1.389767</td>
      <td>2.940639</td>
      <td>0.546747</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>564</th>
      <td>6.439315</td>
      <td>-3.576817</td>
      <td>2.459487</td>
      <td>1.177314</td>
      <td>-0.074824</td>
    </tr>
    <tr>
      <th>565</th>
      <td>3.793382</td>
      <td>-3.584048</td>
      <td>2.088476</td>
      <td>-2.506028</td>
      <td>-0.510723</td>
    </tr>
    <tr>
      <th>566</th>
      <td>1.256179</td>
      <td>-1.902297</td>
      <td>0.562731</td>
      <td>-2.089227</td>
      <td>1.809991</td>
    </tr>
    <tr>
      <th>567</th>
      <td>10.374794</td>
      <td>1.672010</td>
      <td>-1.877029</td>
      <td>-2.356031</td>
      <td>-0.033742</td>
    </tr>
    <tr>
      <th>568</th>
      <td>-5.475243</td>
      <td>-0.670637</td>
      <td>1.490443</td>
      <td>-2.299157</td>
      <td>-0.184703</td>
    </tr>
  </tbody>
</table>
<p>569 rows × 5 columns</p>
</div>

- 실행 결과 최소 80% 이상을 설명하는 수준의 주성분은 총 5개까지 도출됨
- 이처럼 분석가들은 주성분의 개수를 직접 지정할 수도, 분산의 설명 비율을 지정해서 주성분을 도출할 수도 있음

#### 차원 축소 기반 특징 생성 (2): clustering (군집 분석)

- 여러 개의 변수를 하나의 변수(군집결과)로 변환 차원 축소

```python
from sklearn.cluster import KMeans
```

```python
# 일부 변수만 선택 (30개 변수 중 15개의 변수만 임의로 선정)
# 즉, 활용할 정보의 양을 절반으로 축소
subset_df = input_df.iloc[:,0:15]
subset_df
```

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>radius1</th>
      <th>texture1</th>
      <th>perimeter1</th>
      <th>area1</th>
      <th>smoothness1</th>
      <th>compactness1</th>
      <th>concavity1</th>
      <th>concave_points1</th>
      <th>symmetry1</th>
      <th>fractal_dimentsion1</th>
      <th>radius2</th>
      <th>texture2</th>
      <th>perimeter2</th>
      <th>area2</th>
      <th>smoothness2</th>
    </tr>
    <tr>
      <th>id</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>842302</th>
      <td>17.99</td>
      <td>10.38</td>
      <td>122.80</td>
      <td>1001.0</td>
      <td>0.11840</td>
      <td>0.27760</td>
      <td>0.30010</td>
      <td>0.14710</td>
      <td>0.2419</td>
      <td>0.07871</td>
      <td>1.0950</td>
      <td>0.9053</td>
      <td>8.589</td>
      <td>153.40</td>
      <td>0.006399</td>
    </tr>
    <tr>
      <th>842517</th>
      <td>20.57</td>
      <td>17.77</td>
      <td>132.90</td>
      <td>1326.0</td>
      <td>0.08474</td>
      <td>0.07864</td>
      <td>0.08690</td>
      <td>0.07017</td>
      <td>0.1812</td>
      <td>0.05667</td>
      <td>0.5435</td>
      <td>0.7339</td>
      <td>3.398</td>
      <td>74.08</td>
      <td>0.005225</td>
    </tr>
    <tr>
      <th>84300903</th>
      <td>19.69</td>
      <td>21.25</td>
      <td>130.00</td>
      <td>1203.0</td>
      <td>0.10960</td>
      <td>0.15990</td>
      <td>0.19740</td>
      <td>0.12790</td>
      <td>0.2069</td>
      <td>0.05999</td>
      <td>0.7456</td>
      <td>0.7869</td>
      <td>4.585</td>
      <td>94.03</td>
      <td>0.006150</td>
    </tr>
    <tr>
      <th>84348301</th>
      <td>11.42</td>
      <td>20.38</td>
      <td>77.58</td>
      <td>386.1</td>
      <td>0.14250</td>
      <td>0.28390</td>
      <td>0.24140</td>
      <td>0.10520</td>
      <td>0.2597</td>
      <td>0.09744</td>
      <td>0.4956</td>
      <td>1.1560</td>
      <td>3.445</td>
      <td>27.23</td>
      <td>0.009110</td>
    </tr>
    <tr>
      <th>84358402</th>
      <td>20.29</td>
      <td>14.34</td>
      <td>135.10</td>
      <td>1297.0</td>
      <td>0.10030</td>
      <td>0.13280</td>
      <td>0.19800</td>
      <td>0.10430</td>
      <td>0.1809</td>
      <td>0.05883</td>
      <td>0.7572</td>
      <td>0.7813</td>
      <td>5.438</td>
      <td>94.44</td>
      <td>0.011490</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>926424</th>
      <td>21.56</td>
      <td>22.39</td>
      <td>142.00</td>
      <td>1479.0</td>
      <td>0.11100</td>
      <td>0.11590</td>
      <td>0.24390</td>
      <td>0.13890</td>
      <td>0.1726</td>
      <td>0.05623</td>
      <td>1.1760</td>
      <td>1.2560</td>
      <td>7.673</td>
      <td>158.70</td>
      <td>0.010300</td>
    </tr>
    <tr>
      <th>926682</th>
      <td>20.13</td>
      <td>28.25</td>
      <td>131.20</td>
      <td>1261.0</td>
      <td>0.09780</td>
      <td>0.10340</td>
      <td>0.14400</td>
      <td>0.09791</td>
      <td>0.1752</td>
      <td>0.05533</td>
      <td>0.7655</td>
      <td>2.4630</td>
      <td>5.203</td>
      <td>99.04</td>
      <td>0.005769</td>
    </tr>
    <tr>
      <th>926954</th>
      <td>16.60</td>
      <td>28.08</td>
      <td>108.30</td>
      <td>858.1</td>
      <td>0.08455</td>
      <td>0.10230</td>
      <td>0.09251</td>
      <td>0.05302</td>
      <td>0.1590</td>
      <td>0.05648</td>
      <td>0.4564</td>
      <td>1.0750</td>
      <td>3.425</td>
      <td>48.55</td>
      <td>0.005903</td>
    </tr>
    <tr>
      <th>927241</th>
      <td>20.60</td>
      <td>29.33</td>
      <td>140.10</td>
      <td>1265.0</td>
      <td>0.11780</td>
      <td>0.27700</td>
      <td>0.35140</td>
      <td>0.15200</td>
      <td>0.2397</td>
      <td>0.07016</td>
      <td>0.7260</td>
      <td>1.5950</td>
      <td>5.772</td>
      <td>86.22</td>
      <td>0.006522</td>
    </tr>
    <tr>
      <th>92751</th>
      <td>7.76</td>
      <td>24.54</td>
      <td>47.92</td>
      <td>181.0</td>
      <td>0.05263</td>
      <td>0.04362</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.1587</td>
      <td>0.05884</td>
      <td>0.3857</td>
      <td>1.4280</td>
      <td>2.548</td>
      <td>19.15</td>
      <td>0.007189</td>
    </tr>
  </tbody>
</table>
<p>569 rows × 15 columns</p>
</div>

```python
# K-Means는 데이터 간의 거리를 기반으로 하여 가까운 데이터끼리 묶어서 군집으로 도출하는 알고리즘
# 그렇기에 데이터 간 범위 차이를 scaling(정규화)를 통해 해소
std_scaler.fit(subset_df)
subset_input_scaled = std_scaler.transform(subset_df)
subset_input_scaled
```

<pre>
array([[ 1.09706398, -2.07333501,  1.26993369, ...,  2.83303087,
         2.48757756, -0.21400165],
       [ 1.82982061, -0.35363241,  1.68595471, ...,  0.26332697,
         0.74240195, -0.60535085],
       [ 1.57988811,  0.45618695,  1.56650313, ...,  0.8509283 ,
         1.18133606, -0.29700501],
       ...,
       [ 0.70228425,  2.0455738 ,  0.67267578, ...,  0.27669279,
         0.1806983 , -0.37934168],
       [ 1.83834103,  2.33645719,  1.98252415, ...,  1.43852964,
         1.0095027 , -0.17299998],
       [-1.80840125,  1.22179204, -1.81438851, ..., -0.15744905,
        -0.46615196,  0.04934236]])
</pre>

```python
# K-Means 클러스터링 활용
# 군집 Label 수 설정
k = 5
model = KMeans(n_clusters=k, random_state=10)
```

```python
# scaling한 데이터를 fit하여 모델 학습
model.fit(subset_input_scaled)

# 클러스터링 결과를 타겟 변수와 비교하기 위하여 원 데이터에 컬럼으로 생성
target_df['cluster'] = model.fit_predict(subset_input_scaled)
```

```python
# 최종 데이터 확인
target_df
```

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>diagnosis</th>
      <th>cluster</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>842302</td>
      <td>M</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>842517</td>
      <td>M</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>84300903</td>
      <td>M</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>84348301</td>
      <td>M</td>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>84358402</td>
      <td>M</td>
      <td>2</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>564</th>
      <td>926424</td>
      <td>M</td>
      <td>0</td>
    </tr>
    <tr>
      <th>565</th>
      <td>926682</td>
      <td>M</td>
      <td>2</td>
    </tr>
    <tr>
      <th>566</th>
      <td>926954</td>
      <td>M</td>
      <td>2</td>
    </tr>
    <tr>
      <th>567</th>
      <td>927241</td>
      <td>M</td>
      <td>0</td>
    </tr>
    <tr>
      <th>568</th>
      <td>92751</td>
      <td>B</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
<p>569 rows × 3 columns</p>
</div>

```python
# 15개의 임의의 변수로 만들어진 하나의 특징(군집결과)과 기존 Target 변수 비교
pd.crosstab(target_df.diagnosis, target_df.cluster)
```

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>cluster</th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
    </tr>
    <tr>
      <th>diagnosis</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>B</th>
      <td>0</td>
      <td>111</td>
      <td>0</td>
      <td>229</td>
      <td>17</td>
    </tr>
    <tr>
      <th>M</th>
      <td>36</td>
      <td>5</td>
      <td>93</td>
      <td>26</td>
      <td>52</td>
    </tr>
  </tbody>
</table>
</div>

- 0, 2, 4 클러스터는 양성, 1, 3 클러스터는 음성으로 모여있는 것을 확인할 수 있음
- 임의의 15개 변수만을 활용한 하나의 특징(군집결과)이 Target 구분에 효과적일 것임을 예측 가능함
- 15개보다 많은 정보를 사용하거나, 군집 모델을 조금 더 정교하게 만든다면 더 좋은 결과를 얻을 수 있을 것으로 기대
- 이처럼 많은 변수를 하나의 특징으로 구성하고, 입력 데이터의 차원을 줄인다면 모델 연산 비용 절감에 효과적
