```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from scipy import stats
import statsmodels.api as sm  # 회귀분석
```

```python
# 보스턴 집값 데이터 로딩

# 데이터 로딩
data_url = 'http://lib.stat.cmu.edu/datasets/boston'
raw_df = pd.read_csv(data_url, sep=r'\s+', skiprows=22, header=None)
data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
target = raw_df.values[1::2, 2]
housing = pd.merge(pd.DataFrame(data), pd.DataFrame(target), left_index=True, right_index=True, how='inner')
housing.columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 
                  'PTRATIO', 'B', 'LSTAT', 'MEDV']

# 데이터 copy
housing_data = housing.copy()
housing_data
```

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
      <th>MEDV</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.00632</td>
      <td>18.0</td>
      <td>2.31</td>
      <td>0.0</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>1.0</td>
      <td>296.0</td>
      <td>15.3</td>
      <td>396.90</td>
      <td>4.98</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.02731</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>396.90</td>
      <td>9.14</td>
      <td>21.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.02729</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>392.83</td>
      <td>4.03</td>
      <td>34.7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.03237</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>394.63</td>
      <td>2.94</td>
      <td>33.4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.06905</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>7.147</td>
      <td>54.2</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>396.90</td>
      <td>5.33</td>
      <td>36.2</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>501</th>
      <td>0.06263</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0.0</td>
      <td>0.573</td>
      <td>6.593</td>
      <td>69.1</td>
      <td>2.4786</td>
      <td>1.0</td>
      <td>273.0</td>
      <td>21.0</td>
      <td>391.99</td>
      <td>9.67</td>
      <td>22.4</td>
    </tr>
    <tr>
      <th>502</th>
      <td>0.04527</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0.0</td>
      <td>0.573</td>
      <td>6.120</td>
      <td>76.7</td>
      <td>2.2875</td>
      <td>1.0</td>
      <td>273.0</td>
      <td>21.0</td>
      <td>396.90</td>
      <td>9.08</td>
      <td>20.6</td>
    </tr>
    <tr>
      <th>503</th>
      <td>0.06076</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0.0</td>
      <td>0.573</td>
      <td>6.976</td>
      <td>91.0</td>
      <td>2.1675</td>
      <td>1.0</td>
      <td>273.0</td>
      <td>21.0</td>
      <td>396.90</td>
      <td>5.64</td>
      <td>23.9</td>
    </tr>
    <tr>
      <th>504</th>
      <td>0.10959</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0.0</td>
      <td>0.573</td>
      <td>6.794</td>
      <td>89.3</td>
      <td>2.3889</td>
      <td>1.0</td>
      <td>273.0</td>
      <td>21.0</td>
      <td>393.45</td>
      <td>6.48</td>
      <td>22.0</td>
    </tr>
    <tr>
      <th>505</th>
      <td>0.04741</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0.0</td>
      <td>0.573</td>
      <td>6.030</td>
      <td>80.8</td>
      <td>2.5050</td>
      <td>1.0</td>
      <td>273.0</td>
      <td>21.0</td>
      <td>396.90</td>
      <td>7.88</td>
      <td>11.9</td>
    </tr>
  </tbody>
</table>
<p>506 rows × 14 columns</p>
</div>


#### 단순 회귀

$$ MEDV = \beta_0 + \beta_1 LSTAT + e $$

- 주택가격을 하위소득계층 비율로 설명하는 단순 회귀식
- 종속변수: MEDV
- 독립변수: LSTAT

##### 산점도로 변수 간 관계 파악

```python
sns.scatterplot(x=housing_data['LSTAT'], y=housing_data['MEDV'])
plt.show()
```

<img width="858" alt="reg1" src="https://github.com/zacinthepark/TIL/assets/86648892/d4308b97-31e2-4e21-b451-5174ce21ea36">

- 비선형 패턴
- 하지만 우선은 단순 회귀(1차식)으로 설명해보자

```python
# 종속변수와 독립변수 설정
MEDV = housing_data['MEDV'].values
LSTAT = housing_data['LSTAT'].values
```

##### 독립변수에 상수항 결합

```python
# 회귀모형 수식을 간단하게 만들기 위해 상수항을 독립변수에 추가해주어야함
# 상수항이 결합이 된다면, 원소가 1인 데이터가 추가됨
# 상수항이 결합되어야 가중치(beta_0)가 행렬곱으로 들어왔을 때, 살아남을 수 있음 -> 수식이 매우 간단해짐
# 일반적으로 선형회귀는 늘 상수항 결합을 함

LSTAT = sm.add_constant(LSTAT)
```

```python
LSTAT
```

<pre>
array([[1.  , 4.98],
       [1.  , 9.14],
       [1.  , 4.03],
       ...,
       [1.  , 5.64],
       [1.  , 6.48],
       [1.  , 7.88]])
</pre>

##### 단순 선형 회귀 분석

```python
linear_mod = sm.OLS(MEDV, LSTAT)  # statsmodels의 OLS를 이용하여 종속변수, 독립변수를 인수로 추가
```

```python
linear_mod
```

<pre>
<statsmodels.regression.linear_model.OLS at 0x1399a3e90>
</pre>

```python
linear_result = linear_mod.fit()  # linear_mod의 결과 확인을 위해 fit 메서드를 활용
```

```python
linear_result
```

<pre>
<statsmodels.regression.linear_model.RegressionResultsWrapper at 0x139933950>
</pre>

```python
print(linear_result.summary())
```

<pre>
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.544
Model:                            OLS   Adj. R-squared:                  0.543
Method:                 Least Squares   F-statistic:                     601.6
Date:                Mon, 19 Feb 2024   Prob (F-statistic):           5.08e-88
Time:                        07:22:46   Log-Likelihood:                -1641.5
No. Observations:                 506   AIC:                             3287.
Df Residuals:                     504   BIC:                             3295.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         34.5538      0.563     61.415      0.000      33.448      35.659
x1            -0.9500      0.039    -24.528      0.000      -1.026      -0.874
==============================================================================
Omnibus:                      137.043   Durbin-Watson:                   0.892
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              291.373
Skew:                           1.453   Prob(JB):                     5.36e-64
Kurtosis:                       5.319   Cond. No.                         29.7
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>

- R-squared: 결정계수
- F-statistic: 모델의 설명력
- x1: 여기서는 LSTAT에 해당
    - t: t 검정을 했을 때 절대값 24.5만큼의 값
    - `P>|t|`: p-value가 0에 가까워 귀무가설을 기각하고 dependent variable에 영향을 준다는 것을 알 수 있음 

```python
# 단순 회귀일 때 f-statistic을 sqrt하면 t 통계량 절대값과 같음을 확인
np.sqrt(linear_result.fvalue)
```

<pre>
24.527899851187733
</pre>

```python
# R 스타일로 리그레션을 하고 싶다면 from_formula를 이용할 수 있음
linear_mod = sm.OLS.from_formula('MEDV ~ LSTAT', data=housing_data)  # 인수: '종속 변수 ~ 독립변수1 + 독립변수2 + ... + 독립변수N', data
linear_result = linear_mod.fit()
```

```python
print(linear_result.summary())
```

<pre>
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   MEDV   R-squared:                       0.544
Model:                            OLS   Adj. R-squared:                  0.543
Method:                 Least Squares   F-statistic:                     601.6
Date:                Mon, 19 Feb 2024   Prob (F-statistic):           5.08e-88
Time:                        07:29:12   Log-Likelihood:                -1641.5
No. Observations:                 506   AIC:                             3287.
Df Residuals:                     504   BIC:                             3295.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     34.5538      0.563     61.415      0.000      33.448      35.659
LSTAT         -0.9500      0.039    -24.528      0.000      -1.026      -0.874
==============================================================================
Omnibus:                      137.043   Durbin-Watson:                   0.892
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              291.373
Skew:                           1.453   Prob(JB):                     5.36e-64
Kurtosis:                       5.319   Cond. No.                         29.7
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>

- 동일 종속변수와 독립변수에 대해 `sm.OLS`와 `sm.OLS.from_formula`를 이용한 회귀 분석 결과값은 동일하다

##### 잔차 확인

```python
# 잔차의 분포 확인
sns.histplot(linear_result.resid)
plt.show()
```

<img width="856" alt="reg2" src="https://github.com/zacinthepark/TIL/assets/86648892/3b14d959-c7b4-48fb-b575-ad6b4cd49e3c">

#### 선형화를 통한 회귀분석

##### 상관관계 및 분포 확인

```python
# 하위계층비율과 주택가격 관계 확인
sns.pairplot(housing_data[['LSTAT', 'MEDV']])
plt.show()
```

<img width="856" alt="reg3" src="https://github.com/zacinthepark/TIL/assets/86648892/375f588c-2e09-4ded-a4a1-50de0ccedfd1">

```python
# LSTAT의 log 변환
sns.scatterplot(x=np.log(housing_data['LSTAT']), y=housing_data['MEDV'])
plt.show()
```

<img width="858" alt="reg4" src="https://github.com/zacinthepark/TIL/assets/86648892/805ab8d9-706e-4111-bc1b-2665ca8007b5">

##### 회귀 분석

```python
# 로그 변환 후 회귀분석 진행
log_linear_mod = sm.OLS.from_formula('MEDV ~ np.log(LSTAT)', data=housing_data)
log_linear_result = log_linear_mod.fit()
```

```python
# summary table 출력
print(log_linear_result.summary())
```

<pre>
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   MEDV   R-squared:                       0.665
Model:                            OLS   Adj. R-squared:                  0.664
Method:                 Least Squares   F-statistic:                     1000.
Date:                Mon, 19 Feb 2024   Prob (F-statistic):          9.28e-122
Time:                        07:35:10   Log-Likelihood:                -1563.6
No. Observations:                 506   AIC:                             3131.
Df Residuals:                     504   BIC:                             3140.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
=================================================================================
                    coef    std err          t      P>|t|      [0.025      0.975]
---------------------------------------------------------------------------------
Intercept        52.1248      0.965     54.004      0.000      50.228      54.021
np.log(LSTAT)   -12.4810      0.395    -31.627      0.000     -13.256     -11.706
==============================================================================
Omnibus:                      126.181   Durbin-Watson:                   0.918
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              323.855
Skew:                           1.237   Prob(JB):                     4.74e-71
Kurtosis:                       6.039   Cond. No.                         11.5
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>

$$ MEDV = \beta_0 + \beta_1\log (LSTAT) + e $$

- 결정계수가 0.1 가량의 설명력이 개선됨
- LSTAT이 1씩 올라갈 때마다 집값은 12.5%씩 떨어진다고 해석할 수 있음

```python
# 잔차 확인
sns.histplot(log_linear_result.resid)
plt.show()
```

<img width="855" alt="reg5" src="https://github.com/zacinthepark/TIL/assets/86648892/fbbde70a-a221-4724-b904-b5c506067b70">

#### 2차 회귀 분석 (Quadratic Regression Model)

```python
sns.pairplot(housing_data[['LSTAT', 'MEDV']])
plt.show()
```

<img width="859" alt="reg6" src="https://github.com/zacinthepark/TIL/assets/86648892/543a075d-07f2-489c-8d53-d51ceb91906f">

- 두 변수 간 커브 모양의 관계가 형성되어 있음
- 이를 고려하기 위해 LSTAT 변수의 2차항을 추가해보자

```python
# 2차 회귀 분석 모델 설정
quadratic_mod = sm.OLS.from_formula('MEDV ~ LSTAT + I(LSTAT ** 2)', data=housing_data)  # LSTAT의 제곱값 추가
quadratic_result = quadratic_mod.fit()
```

```python
# 결과 테이블 출력
print(quadratic_result.summary())
```

<pre>
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   MEDV   R-squared:                       0.641
Model:                            OLS   Adj. R-squared:                  0.639
Method:                 Least Squares   F-statistic:                     448.5
Date:                Mon, 19 Feb 2024   Prob (F-statistic):          1.56e-112
Time:                        07:49:03   Log-Likelihood:                -1581.3
No. Observations:                 506   AIC:                             3169.
Df Residuals:                     503   BIC:                             3181.
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
=================================================================================
                    coef    std err          t      P>|t|      [0.025      0.975]
---------------------------------------------------------------------------------
Intercept        42.8620      0.872     49.149      0.000      41.149      44.575
LSTAT            -2.3328      0.124    -18.843      0.000      -2.576      -2.090
I(LSTAT ** 2)     0.0435      0.004     11.628      0.000       0.036       0.051
==============================================================================
Omnibus:                      107.006   Durbin-Watson:                   0.921
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              228.388
Skew:                           1.128   Prob(JB):                     2.55e-50
Kurtosis:                       5.397   Cond. No.                     1.13e+03
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.13e+03. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre>

```python
# 잔차 확인
sns.histplot(quadratic_result.resid)
plt.show()
```

<img width="856" alt="reg7" src="https://github.com/zacinthepark/TIL/assets/86648892/11336dd2-1cb4-4087-bee3-76aac500f636">

#### 3개 모델 결과 비교 (선형 vs 로그-리니어 vs 2차 회귀)

```python
print('R squared of linear model --> ', np.round(linear_result.rsquared, 2))
print('R squared of log-linear model --> ', np.round(log_linear_result.rsquared, 2))
print('R squared of quadratic model --> ', np.round(quadratic_result.rsquared, 2))
```

<pre>
R squared of linear model -->  0.54
R squared of log-linear model -->  0.66
R squared of quadratic model -->  0.64
</pre>

#### 다중 회귀 (Multiple Regression)

방 갯수를 알려주는 RM 변수 추가하여 분석을 진행

```python
sns.pairplot(housing_data[['LSTAT', 'RM', 'MEDV']])
plt.show()
```

<img width="855" alt="reg8" src="https://github.com/zacinthepark/TIL/assets/86648892/8aac3785-0ad7-4155-9f9e-2fa10537f2d7">

```python
# RM 변수 추가 시 2가지 옵션, 이미 평균값이기에 수치형으로도 볼 수 있음, 그리고 꽤 선형성을 나타냄
# 하지만 엄밀히 말하면 범주형 변수임 (한개 두개 세개 네개... 2.5개와 같은 실수는 취할 수 없음, 이럴 때 binning 처리 가능, 여기선 하지 않음)

multi_model = sm.OLS.from_formula('MEDV ~ RM + LSTAT + I(LSTAT ** 2)', data=housing_data)
multi_result = multi_model.fit()
```

```python
print(multi_result.summary())
```

<pre>
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   MEDV   R-squared:                       0.703
Model:                            OLS   Adj. R-squared:                  0.701
Method:                 Least Squares   F-statistic:                     396.2
Date:                Mon, 19 Feb 2024   Prob (F-statistic):          6.50e-132
Time:                        07:55:42   Log-Likelihood:                -1533.0
No. Observations:                 506   AIC:                             3074.
Df Residuals:                     502   BIC:                             3091.
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
=================================================================================
                    coef    std err          t      P>|t|      [0.025      0.975]
---------------------------------------------------------------------------------
Intercept        11.6896      3.138      3.725      0.000       5.524      17.855
RM                4.2273      0.412     10.267      0.000       3.418       5.036
LSTAT            -1.8486      0.122    -15.136      0.000      -2.089      -1.609
I(LSTAT ** 2)     0.0363      0.003     10.443      0.000       0.030       0.043
==============================================================================
Omnibus:                      123.119   Durbin-Watson:                   0.848
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              411.618
Skew:                           1.105   Prob(JB):                     4.15e-90
Kurtosis:                       6.826   Cond. No.                     4.49e+03
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 4.49e+03. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre>

```python
# 잔차 확인
sns.histplot(multi_result.resid)
plt.show()
```

<img width="857" alt="reg9" src="https://github.com/zacinthepark/TIL/assets/86648892/c3bcb5ea-5e8e-4ef8-8e76-b1931232faa1">

##### 로그항 추가, 2차항 제거

```python
multi_mod2 = sm.OLS.from_formula('MEDV ~ RM + np.log(LSTAT)', data=housing_data)
multi_result2 = multi_mod2.fit()
```

```python
print(multi_result2.summary())
```

<pre>
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   MEDV   R-squared:                       0.707
Model:                            OLS   Adj. R-squared:                  0.706
Method:                 Least Squares   F-statistic:                     607.2
Date:                Mon, 19 Feb 2024   Prob (F-statistic):          7.40e-135
Time:                        07:57:13   Log-Likelihood:                -1529.6
No. Observations:                 506   AIC:                             3065.
Df Residuals:                     503   BIC:                             3078.
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
=================================================================================
                    coef    std err          t      P>|t|      [0.025      0.975]
---------------------------------------------------------------------------------
Intercept        22.8865      3.552      6.443      0.000      15.908      29.865
RM                3.5977      0.423      8.512      0.000       2.767       4.428
np.log(LSTAT)    -9.6855      0.494    -19.597      0.000     -10.656      -8.714
==============================================================================
Omnibus:                      130.413   Durbin-Watson:                   0.850
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              421.915
Skew:                           1.185   Prob(JB):                     2.41e-92
Kurtosis:                       6.794   Cond. No.                         111.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre>

```python
# 잔차 확인
sns.histplot(multi_result2.resid)
plt.show()
```

<img width="856" alt="reg10" src="https://github.com/zacinthepark/TIL/assets/86648892/afbaa809-7fbd-4403-ab6c-53a46d33e238">

##### 성능 확인

```python
print('R squared of Multiple Regression Model (2차항 포함) --> ', np.round(multi_result.rsquared, 2))
print('R squared of Multiple Regression Model (로그 변환항 포함) --> ', np.round(multi_result2.rsquared, 2))
```

<pre>
R squared of Multiple Regression Model (2차항 포함) -->  0.7
R squared of Multiple Regression Model (로그 변환항 포함) -->  0.71
</pre>

```python
# 기존 성능
print('R squared of linear model --> ', np.round(linear_result.rsquared, 2))
print('R squared of log-linear model --> ', np.round(log_linear_result.rsquared, 2))
print('R squared of quadratic model --> ', np.round(quadratic_result.rsquared, 2))
```

<pre>
R squared of linear model -->  0.54
R squared of log-linear model -->  0.66
R squared of quadratic model -->  0.64
</pre>
