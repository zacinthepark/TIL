## 딥러닝 구조와 학습 절차

---

> 1 스케일링 필수 (처리하는 x간 범위 맞추기)<br>
NaN 조치, 가변수화는 머신러닝과 마찬가지로 진행

> 2 모델 구조 선언 및 컴파일(optimizer, loss function 지정)

> 3 학습 및 학습곡선 확인

> 4 예측 및 검증

![deep_learning_structures](https://github.com/zacinthepark/TIL/assets/86648892/cf2dc210-bb7e-4b68-8dcf-8998cf0299f4)

### 가중치와 가중치 조정

- `실제 = 모델 + 오차`
- 모델링을 한다는 것은 오차를 줄이는 가중치, 파라미터를 찾는 과정이다
    - 즉, **최적** 의 모델이란 최적의 가중치(weight)를 찾는 것
    - 가중치의 다른 용어는 파라미터(parameter)
    - 가중치 조정은 다음 단계를 반복한다
        - 조금씩 weight를 조정하며
        - 오차가 줄어드는지를 확인
    - 언제까지?
        - 지정한 횟수만큼
        - 혹은, 더 이상 오차가 줄지 않을 때까지

### 학습 절차

![z_dl_1_1](https://github.com/zacinthepark/TIL/assets/86648892/b50b9fbb-de60-44fc-b3da-520798d29df3)
![z_dl_1_2](https://github.com/zacinthepark/TIL/assets/86648892/5940d737-e91a-4885-8e73-855ae3f74ff2)

- 모델링 학습 과정에서의 어떤 정보들을 **node** , 혹은 **neuron** 이라고 부름
    - $\text{medv} = w_1 \times \text{lstat} + w_0$

- 초기 파라미터 설정은 랜덤하다
- **loss function** 을 통해 오차를 계산
- **optimizer** 는 오차를 줄여주기 위해 어떤 방향으로 가중치를 조정해야할지 찾아주는 역할
- **learning_rate(학습률)** 은 오차를 얼마만큼 줄일지 결정하는 하이퍼파라미터, 가중치를 조정할 때 어느정도의 보폭으로 조정할지 결정
- **forward propagation(순전파)** : 모델로부터 오차를 계산하는 흐름 (오차를 전파한다)
- **backward propagation(역전파)** : 오차로부터 역으로 미분을 통해 파라미터를 조정하는 흐름 (오차를 역으로 전파한다)

> **하나의 Layer에서 다음 Layer로 노드를 연결할 때, loss function을 바탕으로 오차를 줄이는 방향으로, 각 간선에 대해 가중치를 준다고 생각하자**

- 딥러닝 학습 절차
    - 가중치 초기값을 할당 (초기 모델을 만든다)
    - (초기) 모델로 예측한다
    - 오차를 계산한다 (loss function)
    - 오차를 줄이는 방향으로 가중치를 조절한다 (optimizer)
        - 얼마만큼 조절?: learning_rate
    - 다시 처음으로 가서 반복한다 (epochs)

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.metrics import *
from sklearn.preprocessing import MinMaxScaler

from keras.models import Sequential
from keras.layers import Dense
from keras.backend import clear_session
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint
```

```python
path = 'https://raw.githubusercontent.com/DA4BAM/dataset/master/boston.csv'
data = pd.read_csv(path)
data.head()
```

```python
# 학습을 위한 데이터 분할
target = 'medv'
features = ['lstat']
x = data.loc[:, features]
y = data.loc[:, target]
x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=.2, random_state=20)

# 스케일링
scaler = MinMaxScaler()
x_train_s = scaler.fit_transform(x_train)
y_train_s = (y_train - y_train.min())/(y_train.max() - y_train.min())
```

#### 학습절차 시각화 함수

- 왼쪽 부분: 모델의 가중치가 업데이트되는 과정
- 오른쪽 부분: 모델의 오차가 줄어드는 과정

```python
def dl_visualize(ep, lr) :
    clear_session()

    model = Sequential([Dense(1, input_shape=(1,))])

    model.compile(loss='mse', optimizer= Adam(learning_rate=lr))
    mcp = ModelCheckpoint(filepath='/content/{epoch:d}.h5',
                        monitor='val_loss', save_best_only=False, save_weights_only=True)

    history = model.fit(x_train_s, y_train_s, verbose=0, epochs=ep, callbacks=[mcp]).history

    coef, intercept = [], []

    for i in range(ep):
        file = f'/content/{i+1}.h5'
        model.load_weights(file)
        coef.append(np.array(model.weights[0])[0, 0])
        intercept.append(np.array(model.weights[1])[0])

    plt.figure(figsize=(20, 8))
    plt.subplot(1, 2, 1)
    sns.scatterplot(x=x_train_s.reshape(-1,), y=y_train_s, alpha=.5)
    plt.grid()
    plt.xlabel('lstat')
    for i in range(ep):
        x = np.linspace(0, 1, 10)
        y = coef[i]*x + intercept[i]
        plt.plot(x, y, 'r--')
        v = 1.005
        plt.text(v, coef[i]*v + intercept[i], f'ep:{i+1}', color='r')

    plt.subplot(1, 2, 2)
    plt.plot(range(1, ep+1), history['loss'], label='train_err', marker='.')

    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend()
    plt.grid()
    plt.show()
```

```python
dl_visualize(ep=15, lr=0.01)
```

![z_dl_1_3](https://github.com/zacinthepark/TIL/assets/86648892/2b9cf489-2d4b-489e-a2f9-10755e7ad55a)

```python
dl_visualize(ep=20, lr=0.05)
```

![z_dl_1_4](https://github.com/zacinthepark/TIL/assets/86648892/b43dd3ca-2d6c-497e-98e9-d0601404d307)

### 딥러닝 전처리: 스케일링

```python
target = 'medv'
features = ['lstat', 'ptratio', 'crim']
x = data.loc[:, features]
y = data.loc[:, target]
x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=.2, random_state=20)
scaler = MinMaxScaler()
x_train = scaler.fit_transform(x_train)
x_val = scaler.transform(x_val)
```

- `MinMaxScaler`, `StandardScaler`
- 표준화는 이상치가 많을 때 사용한다
- 이상치가 많을 때 정규화를 진행하면 범위가 이상치에 따라 설정되므로 범위 조정의 의미가 떨어짐

#### 1. Normalization (정규화)

**모든 값의 범위** 를 **0 ~ 1** 로 변환

$$
\Large{X_{\text{norm}} = \frac{x - {\text{min}}}{{\text{max}} - {\text{min}}}}
$$

#### 2. Standardazation (표준화)

모든 값을 **평균 = 0, 표준편차 = 1** 로 변환

$$
\Large{X_{\text{z}} = \frac{x - {\text{mean}}}{\text{std}}}
$$

### 딥러닝 구조와 `Dense`

![z_dl_1_5](https://github.com/zacinthepark/TIL/assets/86648892/9c84e7b5-ba0b-4fd8-a3e4-c5af16c66e9f)
![z_dl_1_6](https://github.com/zacinthepark/TIL/assets/86648892/4820aaa7-f89e-45c7-95fa-ee1c3e60aebe)

```python
from keras.models import Sequential
from keras.layers import Dense
# 메모리 정리
clear_session()

# Sequential 타입
model = Sequential([Dense(1, input_shape=(nfeatures, ))])

# 모델요약
model.summary()
```

<pre>
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 1)                 4         
                                                                 
=================================================================
Total params: 4 (16.00 Byte)
Trainable params: 4 (16.00 Byte)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</pre>

- `Dense`는 `input_shape` 모양의 노드들을 연결하여 `output` 개수만큼 출력하는 Layer
    - `input_shape = ( , )`
        - **분석단위** 에 대한 shape
        - 1차원: `(feature수, )`
        - 2차원: `(rows, columns)`
    - `output`: 예측 결과가 1개 변수 (y가 1개 변수)
        - Hidden Layer일 경우 여러 개의 output 출력 가능
- `param` 개수는 `bias(w0)`를 포함한 개수

### Compile

```python
model.compile(optimizer=Adam(learning_rate=0.1), loss='mse')
```

- Compile: 선언된 모델에 대해 몇 가지 설정을 한 후, 컴퓨터가 이해할 수 있는 형태로 변환하는 작업

- `loss` function(오차함수)
    - 오차 계산을 무엇으로 할지 결정
    - 회귀모델은 보통 `mse`로 오차 계산

- `optimizer`
    - 오차를 최소화하도록 가중치를 조절하는 역할
    - `Adam()`의 경우 `learning_rate`의 기본값은 0.001

#### learning_rate에 따른 가중치(w) 한 개의 업데이트 과정

![z_dl_1_7](https://github.com/zacinthepark/TIL/assets/86648892/2e09e694-5564-4138-ba8e-3eeca58d9066)

### 학습

```python
history = model.fit(x_train, y_train, epochs=30, validation_split=0.2).history
```

- `epochs`
    - 가중치 조정 반복 횟수
    - **전체 데이터를 몇 번 학습** 할 것인지 정함

- `validation_split`
    - train 데이터에서 20%를 검증 셋으로 분리
    - 실수가 아닌 정수 형태이면 분리 비율이 아닌 개수로 지정

- `.history`
    - 학습을 수행하는 과정 중에
    - 가중치가 업데이트되면서
    - 그때 그때마다의 성능을 측정하여 기록
    - 학습 시 계산된 오차 기록

<pre>
Epoch 1/30
11/11 [==============================] - 2s 79ms/step - loss: 524.5162 - val_loss: 533.5225
Epoch 2/30
11/11 [==============================] - 0s 14ms/step - loss: 447.7675 - val_loss: 460.8988
Epoch 3/30
11/11 [==============================] - 0s 9ms/step - loss: 382.7074 - val_loss: 400.6927
Epoch 4/30
11/11 [==============================] - 0s 15ms/step - loss: 329.0135 - val_loss: 350.9053
Epoch 5/30
11/11 [==============================] - 0s 24ms/step - loss: 284.9559 - val_loss: 310.3911
Epoch 6/30
11/11 [==============================] - 0s 16ms/step - loss: 251.1486 - val_loss: 279.2166
Epoch 7/30
11/11 [==============================] - 0s 16ms/step - loss: 224.7461 - val_loss: 256.8600
Epoch 8/30
11/11 [==============================] - 0s 17ms/step - loss: 205.9337 - val_loss: 239.9451
Epoch 9/30
11/11 [==============================] - 0s 22ms/step - loss: 191.7909 - val_loss: 225.6369
Epoch 10/30
11/11 [==============================] - 0s 13ms/step - loss: 179.7876 - val_loss: 214.2914
Epoch 11/30
11/11 [==============================] - 0s 15ms/step - loss: 171.0694 - val_loss: 205.0986
Epoch 12/30
11/11 [==============================] - 0s 17ms/step - loss: 162.9418 - val_loss: 196.9515
Epoch 13/30
11/11 [==============================] - 0s 13ms/step - loss: 156.8790 - val_loss: 190.0411
Epoch 14/30
11/11 [==============================] - 0s 23ms/step - loss: 151.5735 - val_loss: 185.0924
Epoch 15/30
11/11 [==============================] - 0s 21ms/step - loss: 147.5000 - val_loss: 180.6171
Epoch 16/30
11/11 [==============================] - 0s 29ms/step - loss: 143.6092 - val_loss: 175.8970
Epoch 17/30
11/11 [==============================] - 0s 18ms/step - loss: 139.9000 - val_loss: 171.8510
Epoch 18/30
11/11 [==============================] - 0s 11ms/step - loss: 136.5045 - val_loss: 168.1901
Epoch 19/30
11/11 [==============================] - 0s 13ms/step - loss: 133.5110 - val_loss: 164.6677
Epoch 20/30
11/11 [==============================] - 0s 33ms/step - loss: 130.3503 - val_loss: 161.1873
Epoch 21/30
11/11 [==============================] - 0s 12ms/step - loss: 127.3840 - val_loss: 157.6157
Epoch 22/30
11/11 [==============================] - 0s 8ms/step - loss: 124.3413 - val_loss: 154.1322
Epoch 23/30
11/11 [==============================] - 0s 17ms/step - loss: 121.5028 - val_loss: 150.9813
Epoch 24/30
11/11 [==============================] - 0s 9ms/step - loss: 118.7959 - val_loss: 147.5975
Epoch 25/30
11/11 [==============================] - 0s 13ms/step - loss: 115.9064 - val_loss: 144.1958
Epoch 26/30
11/11 [==============================] - 0s 8ms/step - loss: 113.3303 - val_loss: 141.3177
Epoch 27/30
11/11 [==============================] - 0s 9ms/step - loss: 110.8993 - val_loss: 138.4735
Epoch 28/30
11/11 [==============================] - 0s 9ms/step - loss: 108.4822 - val_loss: 135.6792
Epoch 29/30
11/11 [==============================] - 0s 9ms/step - loss: 105.9993 - val_loss: 132.7223
Epoch 30/30
11/11 [==============================] - 0s 7ms/step - loss: 103.5586 - val_loss: 129.9243
</pre>

### 학습곡선

![z_dl_1_8](https://github.com/zacinthepark/TIL/assets/86648892/c7f9c9fb-dd31-44d2-8eb2-d1388cf4ee9c)

```python
# 학습곡선 함수
def dl_history_plot(history):
    plt.figure(figsize=(10,6))
    plt.plot(history['loss'], label='train_err', marker='.')
    plt.plot(history['val_loss'], label='val_err', marker='.')

    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend()
    plt.grid()
    plt.show()

dl_history_plot(history)
```

![z_dl_1_9](https://github.com/zacinthepark/TIL/assets/86648892/6417a248-3156-42bd-a18b-7e5b73b4a69b)

### 예측 및 검증

```python
y_pred = model.predict(x_val)

print(f'RMSE: {mean_squared_error(y_val, y_pred, squared=False)}')
print(f'MAE: {mean_absolute_error(y_val, y_pred)}')
print(f'MAPE: {mean_absolute_percentage_error(y_val, y_pred)}')
```

<pre>
RMSE: 8.791942805887539
MAE: 5.889047645120059
MAPE: 0.3023437844818914
</pre>
