## 분류 모델 성능 평가

---

- 분류 모델 평가: 정확도를 높여라
    - 분류 모델은 0인지 1인지를 예측하는 것
    - 실제값도 0과 1이고 예측값도 0과 1임
    - 예측값이 실제값과 많이 같을수록 좋은 모델이라 할 수 있음
    - 정확히 예측한 비율로 모델 성능을 평가

### Confusion Matrix(혼동행렬, 오분류표)

![confusion_matrix_1](https://github.com/zacinthepark/TIL/assets/86648892/6ee554f8-21f9-4d4f-af9b-bad4b4de5f60)
![confusion_matrix_2](https://github.com/zacinthepark/TIL/assets/86648892/5e28d51f-0d9e-4876-9f3c-438985e79490)

- TN(True Negative, 진음성): 음성으로 잘 예측한 것 (음성을 음성이라 예측한 것)
- FP(False Positive, 위양성): 양성으로 잘못 예측한 것 (음성을 양성이라 예측한 것)
- FN(False Negative, 위음성): 음성으로 잘못 예측한 것 (양성을 음성이라 예측한 것)
- TP(True Positive, 진양성): 양성으로 잘 예측한 것 (양성을 양성이라 예측한 것)

- 정밀도와 재현율은 기본적으로 Positive에 대해 이야기함
- Negative에 대한 정밀도와 재현율도 의미를 가짐

### 정확도(Accuracy)

$$\large Accuracy = \frac{TP+TN}{TP+TN+FP+FN}$$

- **정분류율** 이라고 부르기도 함
- 전체 중에서 Positive와 Negative로 정확히 예측한(TN + TP) 비율
- Negative를 Negative로 예측한 경우도 옳은 예측임을 고려하는 평가지표
- 가장 직관적으로 모델 성능을 확인할 수 있는 평가지표

### 정밀도(Precision)

$$\large Precision = \frac{TP}{TP+FP}$$

- Positive로 예측한 것(FP + TP) 중에서 실제 Positive(TP)인 비율
- 예시: 비가 내릴 것으로 예측한 날 중에서 실제 비가 내린 날의 비율, 암이라 예측한 환자 중에서 실제 암인 환자의 비율
- 정밀도가 낮을 경우 발생하는 상황
    - 비가 오지 않는데 비가 온다고 했으니 불필요한 우산을 챙기는 수고 발생
    - 암이 아닌데 암이라 했으니 불필요한 치료 발생

### 재현율(Recall)

$$\large Recall = \frac{TP}{TP+FN}$$

- 실제 Positive(FN + TP) 중에서 Positive로 예측한(TP) 비율
- 민감도(Sensitivity)라고 부르는 경우가 많음
- 예시: 실제 비가 내린 날 중에서 비가 내릴 것으로 예측한 날의 비율, 실제 암인 환자 중에서 암이라고 예측한 환자의 비율
- 재현율이 낮을 경우 발생하는 문제
    - 비가 내리는 날 내리지 않을 것이라 했으니 우산을 챙기지 않아 비를 맞음
    - 암인 사람에게 암이 아니라 했으니 심각한 결과 초래

### 특이도(Specificity)

$$\large Specificity = \frac{TN}{TN+FP}$$

![corona](https://github.com/zacinthepark/TIL/assets/86648892/add6faeb-e0d4-455f-b8c2-acc10aac5b86)

- 실제 Negative(TN + FP) 중에서 Negative로 예측한(TN) 비율
- 예시: 실제 비가 내리지 않은 날 중에서 비가 내리지 않을 것으로 예측한 날의 비율, 실제 암이 아닌 환자 중에서 암이 아니라고 예측한 환자의 비율
- 특이도가 낮을 경우 발생하는 문제
    - 비가 오지 않는데 비가 온다고 했으니 불필요한 우산을 챙기는 수고 발생
    - 암이 아닌데 암이라 했으니 불필요한 치료 발생

### F1 Score

$$\large F1 = \frac{2\times Precision\times Recall}{Precision+Recall}$$

![f1_score](https://github.com/zacinthepark/TIL/assets/86648892/893db8e0-beb7-4752-85b6-49d3c118cfec)
![image](https://github.com/zacinthepark/TIL/assets/86648892/ec2d733e-7c81-478a-ba05-9b640f2137cd)

- 정밀도와 재현율의 조화 평균
- 분자가 같지만 분모가 다를 경우, 즉 관점이 다른 경우 조화 평균이 큰 의미를 가짐
- 정밀도와 재현율이 적절하게 요구될 때 사용

### Code

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

warnings.filterwarnings(action='ignore')
%config InlineBackend.figure_format = 'retina'
```

```python
path = 'https://raw.githubusercontent.com/Jangrae/csv/master/admission_simple.csv'
data = pd.read_csv(path)
```

```python
data.info()
```

<pre>
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 500 entries, 0 to 499
Data columns (total 8 columns):
 #   Column    Non-Null Count  Dtype  
---  ------    --------------  -----  
 0   GRE       500 non-null    int64  
 1   TOEFL     500 non-null    int64  
 2   RANK      500 non-null    int64  
 3   SOP       500 non-null    float64
 4   LOR       500 non-null    float64
 5   GPA       500 non-null    float64
 6   RESEARCH  500 non-null    int64  
 7   ADMIT     500 non-null    int64  
dtypes: float64(3), int64(5)
memory usage: 31.4 KB
</pre>

```python
target = 'ADMIT'

x = data.drop(target, axis=1)
y = data.loc[:, target]

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)

from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier()
model.fit(x_train, y_train)
y_pred = model.predict(x_test)
```

```python
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test, y_pred))
```

<pre>
[[76  8]
 [16 50]]
</pre>

```python
# 혼동행렬 시각화
plt.figure(figsize=(5, 3))
sns.heatmap(confusion_matrix(y_test, y_pred),
            annot=True,
            cmap='Blues',
            cbar=False,
            annot_kws={'size': 15})
plt.show()
```

![cmatrix](https://github.com/zacinthepark/TIL/assets/86648892/36c70864-5731-471b-9dd8-cf3cc447a5e9)

```python
from sklearn.metrics import accuracy_score
print('정확도: ', accuracy_score(y_test, y_pred))
```

<pre>
정확도:  0.84
</pre>

```python
# 참고: 평가 성능(정확도)
model.score(x_test, y_test)
```

<pre>
0.84
</pre>

```python
# 참고: 학습 성능(정확도)
model.score(x_train, y_train)
```

<pre>
0.8885714285714286
</pre>

```python
from sklearn.metrics import precision_score
print('정밀도: ', precision_score(y_test, y_pred))                    # 1(positive)의 precision
print('정밀도: ', precision_score(y_test, y_pred, average='binary'))  # 1(positive)의 precision (default, 1.4.1 ver에서는 None이 default)
print('정밀도: ', precision_score(y_test, y_pred, average=None))      # negative, positive의 precision
print('정밀도: ', precision_score(y_test, y_pred, average='macro'))   # macro_avg
print('정밀도: ', precision_score(y_test, y_pred, average='weighted'))# weighted_avg
```

<pre>
정밀도:  0.8620689655172413
정밀도:  0.8620689655172413
정밀도:  [0.82608696 0.86206897]
정밀도:  0.8440779610194902
정밀도:  0.8419190404797602
</pre>

```python
from sklearn.metrics import recall_score
print('민감도: ', recall_score(y_test, y_pred))               # 1(positive)의 recall
print('민감도: ', recall_score(y_test, y_pred, average=None)) # negative, positive의 recall
```

<pre>
민감도:  0.7575757575757576
민감도:  [0.9047619  0.75757576]
</pre>

```python
from sklearn.metrics import f1_score
print('F1: ', f1_score(y_test, y_pred, average=None))
```

<pre>
F1:  [0.86363636 0.80645161]
</pre>

```python
# support: 데이터의 개수
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))
```

<pre>
              precision    recall  f1-score   support

           0       0.83      0.90      0.86        84
           1       0.86      0.76      0.81        66

    accuracy                           0.84       150
   macro avg       0.84      0.83      0.84       150
weighted avg       0.84      0.84      0.84       150

</pre>

```python
data['ADMIT'].value_counts()
```

<pre>
ADMIT
0    282
1    218
Name: count, dtype: int64
</pre>

### 현실 불균형 데이터에 대해

- 현실적인 데이터는 0이 훨씬 많은 경우가 많다

- 기계 고장 예측 모델의 경우
    - 실제 기계가 고장이 나는 비율을 0.1이라고 한다면 (통계적으로 모집단, 그러니까 전체적으로 고장이 발생하는 빈도)
    - 아무리 좋은 모델을 만들어도 실제 고장 발생비율이 낮기 때문에 TP값은 엄청 적게 나올 것
    - TP값은 커봐야 0.1을 넘지 않을 것
    - 이에 따라 1에 해당하는 성능지표 값들이 0에 비해 낮게 나올 수 있다
    - 하지만 비즈니스적으로 비용을 고려할 때 100번 중에 1번 기계가 고장남을 맞출 수 있는 것은 99번 점검 비용 50번 < 1번 손실 비용 5억 방지로 이득일 수 있음
    - 그러므로 precision이 낮아지더라도 recall 값을 올리는 것이 중요

- 추가적으로 실제 현업에서는 이러한 precision과 recall을 모두 고려하는 F1-score와 같은 지표를 통해 빈도가 낮아도 과적합이 일어나지 않는 경우를 파악하는 과정을 거침
    - 때문에 실제 고장이 나는 비율이 작아도 작은 부분에 대해 예측할 수 있는지를 측정하여 모델의 성능을 측정할 수 있음
