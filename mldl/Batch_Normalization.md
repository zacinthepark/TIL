## Batch Normalization

---

### Why Normalize?

- 신경망에 데이터를 넣을 때 입력값에 정규화를 진행
- 대부분의 데이터는 특성마다 가지는 값의 범위가 다르기 때문
    - 인의 손뼘 길이와 신장의 연관 관계를 알아보기 위한 데이터를 가정
    - 신장을 특성으로 하는 데이터는 대략 `[145,210]` 범위를 가질 것
    - 나머지 특성인 손뼘 길이는 `[15,30]` 정도의 범위를 가질 것

![deep_learning_normalization](https://github.com/zacinthepark/TIL/assets/86648892/b81624e3-0e2f-4176-b651-72a9e7f6437d)

- 왼쪽처럼 생긴 데이터는 비효율적인 최적화 경로를 갖는다는 것이 문제
    - 주로 편차가 큰 특성의 파라미터를 갱신하는 방향으로 학습이 진행됨
    - 상대적으로 적은 편차를 갖는 특성의 영향력은 줄어듬
    - 위 그림에서도 x축 특성의 값을 최적화하는 방향으로만 학습이 진행되고, y축에 나타나는 특성값은 상대적으로 무시되는 것을 볼 수 있음

- 입력값 정규화는 이러한 문제를 해결
    - 정규화는 특성마다의 범위를 동일하게 만들어줌
    - 범위가 비슷해지면 학습 과정에서 모든 특성에 대한 파라미터가 동일하게 개선되어 훨씬 더 **효율적인 최적화 경로** 를 갖게 됨

### ICS (Internal Covariate Shift)

- 입력값은 신경망에 들어가기 전에 적당히 조정해 줄 수 있지만 신경망 내부에서 파라미터와의 연산을 통해 은닉층을 빠져나오는 값은 다른 분포를 가질 것

- 은닉층마다 이런 현상이 생긴다면 학습이 제대로 이루어지지 않을 것

![image](https://github.com/zacinthepark/TIL/assets/86648892/64996ba7-774b-4939-a438-b28b8e96a4f8)

- 이렇게 신경망 내부의 은닉층마다 값의 분포가 달라지는 현상을 ICS(Internal Covariate Shift, 내부 공변량 변화)라고 부름

- 배치 정규화는 ICS를 해결하면 신경망의 성능이 더 좋아지지 않을까라는 아이디어에서 시작

- BN 논문 저자는 딥러닝 구조의 내재적 불안정성은 넷 각 층의 값의 분포(variance)가 일관성없이 계속해서 달라지는 현상에 의한 것이라 주장

### Batch Normalization

![image](https://github.com/zacinthepark/TIL/assets/86648892/3f56d10f-f1f6-4d9c-a8a5-077999c45ccb)

- 배치 정규화는 완전 연결층(Fully Connected Layer)과 활성화 함수 사이에 배치 정규화층(Batch Norm Layer)을 하나 더 삽입

> 1. 평균 계산, 분산 계산, 입력 정규화

- 배치 정규화층에서는 완전 연결층에서 계산된 값을 미니배치 단위로 정규화

- 미니배치 $\textit{B}$의 사이즈가 $\textit{m}$이라면 각 데이터의 평균 $\mu_B$와 분산 $\sigma^2_B$를 구하고 표준화(Standaradization)를 수행하여 $\hat{x}_i$를 구한다

$$
\mu_B = \frac{1}{m} \sum_{i=1}^{m} x_i
$$

$$
\sigma_B^2 = \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu_B)^2
$$

$$
\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
$$

- $x_i$는 입력 데이터
- $\textit{m}$은 배치 크기
- $\mu_B$는 배치 내 입력 데이터의 평균
- $\sigma^2_B$는 배치 내 입력 데이터의 분산
- $\epsilon$은 0으로 나누는 것을 방지하기 위한 작은 상수(예: $10^{-5}$)
- $\hat{x}_i$는 정규화된 데이터

- 이 과정을 통해서 표준정규분포 N(0, 1)로 정규화된 데이터가 됨

- 하지만 표준정규분포에서 68.26%만큼이 `[-1, 1]` 범위에 있고, 95.44%의 값이 `[-2, 2]` 범위에 있음

![activation](https://github.com/zacinthepark/TIL/assets/86648892/c2b02926-83f9-48c2-b0d7-7b2672ee4ddc)

- 입력값이 표준정규분포 형태면 활성화 함수를 적용했을 때 거의 선형성을 띠게 됨

- **Activation 함수의 가장 중요한 역할인 비선형성을 없앨 수 있음**

- 이를 해결하기 위해 스케일링과 시프트를 통해 정규화한 값 $\hat{x}_i$를 적당히 조절

> 2. 스케일링과 시프트

$$
y_i = \gamma_j \hat{x}_i + \beta_j
$$

- $\gamma$와 $\beta$는 학습 가능한 파라미터로, 각각 스케일링과 시프트를 위해 사용

- $y_i$는 최종적으로 변환된 데이터
    - 이 때 곱해지고 더해지는 파라미터 $\gamma$, $\beta$는 각 노드마다 다른 값이 적용됨
    - 특정 층의 노드가 64개라면 배치 정규화층에도 파라미터가 각각 그만큼 있는 것
    - $\gamma_j$, $\beta_j$ ($\textit{j}=1,...,64$)
    - **이 때 $y$의 분포는 $N(\beta_j, \gamma^2_j)$이며, $\gamma_j$, $\beta_j$는 학습을 통해 결정됨**

![image](https://github.com/zacinthepark/TIL/assets/86648892/649fb175-3839-4fed-ad01-fe8bb1b46f18)

배치 정규화를 사용했을 때와 사용하지 않았을 때 학습 데이터셋과 검증 데이터셋에 대한 학습 결과를 위 그림에서 보면 배치 정규화를 했을 때 훨씬 더 좋은 결과를 보여주는 것을 볼 수 있음

### Batch Normalization은 왜 성능을 높여주는가?

링크: https://gradientscience.org/batchnorm/

#### 배치 정규화는 ICS를 해결하는가?

- 2018년에는 배치 정규화가 어떻게 최적화에 도움을 주는지에 대해 연구한 논문이 발표
- 논문에 따르면 배치 정규화가 ICS를 해결해주지도 못한다는 것
- 아래는 배치 정규화를 적용했을 때(오른쪽)와 적용하지 않았을 때(왼쪽), 3번째 층과 11번째 층에 들어오는 값의 분포를 시각화한 것
    - 학습에 사용되는 미니배치마다 그래프를 그림

![image](https://github.com/zacinthepark/TIL/assets/86648892/73b61b00-a497-4173-a705-d6c90dfe9b61)

- 둘 간의 획기적인 변화가 없음
- 3번째 층이든 11번째 층이든 배치 정규화를 사용했을 때와 그렇지 않았을 때 큰 차이가 없음

![image](https://github.com/zacinthepark/TIL/assets/86648892/2d98fe67-d68b-4035-9c56-a3994ab36fc0)

- 논문 저자들은 배치 정규화를 적용한 뒤에 노이즈를 주어 일부러 분포를 흐트린 모델을 추가하여 실험해보기도 함
- 시각화 결과 노이즈가 추가된 모델은 반복 스텝마다 매우 다른 분포를 보임

![image](https://github.com/zacinthepark/TIL/assets/86648892/c4ddb720-abe0-48d7-8aaf-187bb4a291dc)

- 세 모델의 성능을 비교한 학습 그래프
- 노이즈가 추가된 모델은 ICS가 오히려 심해졌음에도 배치 정규화를 적용하지 않은 모델보다 훨씬 더 좋은 성능을 보임
- 논문을 통해 배치 정규화가 ICS를 해결하는지도 알 수 없고, ICS를 해결하는 것이 좋은 성능을 보장하는 것도 아님을 증명

#### Smoothing

![image](https://github.com/zacinthepark/TIL/assets/86648892/1577a2df-57fa-4bd3-b289-da8a917aee54)

- 논문의 저자는 배치 정규화가 스무딩을 만들어준다고 주장
- 배치 정규화를 적용하지 않은 손실 함수가 왼쪽과 같이 생겼다면, 배치 정규화를 적용한 손실 함수는 오른쪽처럼 생겼다는 것

![image](https://github.com/zacinthepark/TIL/assets/86648892/aa3baaf0-6667-49d6-a6c1-ef9c589dc77f)

- 본 실험에서 손실 함수의 평면과 기울기를 나타낸 그래프
- 배치 정규화를 적용했을 때가 그렇지 않을 때보다 훨씬 더 완만한 경사를 가지는 것을 볼 수 있음
- 결론은 배치 정규화가 좋은 성능을 보이는 이유가 ICS를 해결했기 때문이 아니라 스무딩 덕분이라는 것

### Batch Normalization의 장점

> 1. 학습률을 높게 설정해도 최적화가 잘되기에 학습 속도를 빠르게 해줌

> 2. 데이터가 스케일링되므로 신경망으로 하여금 가중치 초기화(weight initialization)이나 하이퍼파라미터 설정에 덜 민감하게 해줌

> 3. 또한 드롭아웃(Dropout)과 같은 스킬을 사용하지 않아도 모델이 일반화(Generalization)되어 오버피팅을 막아줌
