모델링을 한다는 것은 오차를 줄이는 가중치, 파라미터를 찾는 과정이다
초기 파라미터 설정은 랜덤하다
loss function을 통해 오차를 계산
optimizer는 오차를 줄여주기 위해 어떤 방향으로 가중치를 조정해야할지 찾아주는 역할
learning_rate(학습률)은 오차를 얼마만큼 줄일지 결정하는 하이퍼파라미터, 가중치를 조정할 때 어느정도의 보폭으로 조정할지 결정
forward propagation(순전파): 모델로부터 오차를 계산하는 흐름 (오차를 전파한다)
backward propagation(역전파): 오차로부터 역으로 미분을 통해 파라미터를 조정하는 흐름 (오차를 역으로 전파한다)

표준화는 이상치가 많을 때 사용한다
이상치가 많을 때 정규화를 진행하면 범위가 이상치에 따라 설정되므로 범위 조정의 의미가 떨어짐

Dense는 input_shape 모양의 노드들을 연결하여 output 개수만큼 출력하는 Layer
param 개수는 bias(w0)를 포함한 개수
input_shape는 분석단위
compile은 기계어로 변환하는 것

딥러닝은 시각지능, 언어지능, 시계열, 큰 데이터인 경우에 머신러닝에 비해 성능이 좋은 것으로 알려져있다

$-log{x}$는 $x$가 1에 가까울수록 값을 0에 가깝게, $x$가 0에 가까울수록 커지게 만들 수 있음

**하나의 Layer에서 다음 Layer로 노드를 연결할 때, loss function을 바탕으로 오차를 줄이는 방향으로, 각 간선에 대해 가중치를 준다고 생각하자**

binary_crossentropy

- $log{x}$: 1. 곱셈을 덧셈으로 바꿔준다 2. $x$가 0에 가까울수록 $-\infty$, 1일 때 0, 그리고 1 이상일 때 단조증가하는 성격을 가지고 있음
- $-log{x}$ 그래프
- 0~1의 확률값에 관한 예측값에 대하여 $-log{x}$ 그래프를 기반으로 하여 오차를 계산

다중분류

다중분류에서 output layer의 노드의 수는 클래스의 수
setosa, versicolor, virginica에 대한 각각 값이 있다면, 이 중에서 가장 큰 값이 결론
softmax를 통해 예측값을 확률값으로 변환
categorical_crossentropy는 로그우도라고도 한다
배열의 인덱스가 인코딩된 범주

sparse_categorical_crossentropy

- binary_crossentropy처럼 $-log{x}$ 함수를 통해 오차를 계산
- 대신 softmax를 통해 얻은 확률값을 $x$ 값으로 사용
- 1에 가까운 값이라면, 즉, 해당 범주일 확률이 1에 가까우면, 오차는 낮을 것

가중치 업데이트와 기울기 소실

Gradient는 미분한 결과, 기울기, 순간 변화량
Gradient Descent(경사 하강법)은 optimizer의 기본
편미분 기호에 해당하는 값이 음수이면 가중치를 커지게 업데이트할 것
분홍색 선은 시그모이드, 검은색 선은 시그모이드를 미분한 그래프
Sigmoid를 통해 현재 레이어의 결과값을 다음 레이어에 변환하여 전달하면, 역전파 시 기울기가 적음
ReLU는 비선형 함수를 만들어주는 것 외에도 기울기 소실 문제를 줄여줌

Local Minima Problem

기본적인 Gradient Descent 방법으로 발생할 수 있는 문제
