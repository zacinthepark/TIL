## 경사하강법(Gradient Descent)

---

![gradient_descent](https://github.com/zacinthepark/TIL/assets/86648892/d85ae39a-7aab-4cdb-9bf3-7bea98937ff1)

- 딥러닝 알고리즘 학습 시 사용되는 최적화(Optimizer) 방법 중 하나이다

- 딥러닝 알고리즘 학습 시 목표는 예측값과 정답값 간의 차이인 손실 함수의 크기를 최소화시키는 파라미터(가중치 $w$)를 찾는 것

- 학습 데이터 입력을 변경할 수 없기에 손실 함수 값의 변화에 따라 가중치(weight) 또는 편향(bias)을 업데이트시킴

- 가로축은 가중치, 세로축은 손실함수를 의미

- 다음과 과정을 거친다

    - 1: 임의의 파라미터(가중치 $w$)를 정한다

    - 2: 이 가중치에 대한 손실값을 구하고 손실 함수(Loss Function)의 기울기(Gradient)를 구한다

    - 3: 경사하강법(Gradient Descent)을 이용해 파라미터를 업데이트한다
        - 해당 경사의 반대 방향으로 계속 이동시켜 극값에 이를 때까지 반복시키는 것
        - 이동해야하는 방향으로 learning_rate만큼 이동

    - 4: 업데이트된 지점에서 새 손실 함수의 기울기를 구한다

    - 5: 3번 다시 실시

    - 6: 파라미터가 최적값에 도달하면 파라미터 업데이트를 중지한다

- 적절한 step size 선정 문제, Local Minima 문제가 발생할 수 있음

> 경사 하강법 종류에는 3가지가 있다: 배치 경사 하강법, 확률적 경사 하강법, 미니 배치 경사 하강법

### 1. 배치 경사 하강법

배치 경사 하강법(Batch Gradient Descent)은 가장 기본적인 경사 하강법으로 Vanilla Gradient Descent라고 부르기도 합니다. 배치 경사 하강법은 데이터셋 전체를 고려하여 손실함수를 계산합니다. 배치 경사 하강법은 한 번의 Epoch에 모든 파라미터 업데이트를 단 한 번만 수행합니다. 즉, Batch의 개수와 Iteration은 `1`이고 Batch size는 전체 데이터의 개수입니다. 파라미터 업데이트할 때 한 번에 전체 데이터셋을 고려하기 때문에 모델 학습 시 많은 시간과 메모리가 필요하다는 단점이 있습니다.

### 2. 확률적 경사 하강법

확률적 경사 하강법(Stochastic Gradient Descent)은 배치 경사 하강법이 모델 학습 시 많은 시간과 메모리가 필요하다는 단점을 개선하기 위해 제안된 기법입니다. 확률적 경사 하강법은 Batch size를 `1`로 설정하여 파라미터를 업데이트하기 때문에 배치 경사 하강법보다 훨씬 빠르고 적은 메모리로 학습이 진행됩니다.

![stochastic_batch_gradient_descent](https://github.com/zacinthepark/TIL/assets/86648892/96d6488d-94dd-4279-a5fe-6fa26a74aaa1)

위의 그림은 경사 하강법 종류에 따라 최적의 해를 찾아가는 과정을 시각화한 자료입니다. 좌측은 확률적 경사 하강법을, 우측은 배치 경사 하강법을 활용한 경우입니다. 확률적 경사 하강법은 파라미터 값의 업데이트 폭이 불안정하기 때문에 배치 경사 하강법보다 정확도가 낮은 경우가 생길 수도 있습니다. 그럼에도 불구하고, 하나의 데이터(Batch size=`1`)에 대해서만 손실함수를 계산하고 파라미터를 업데이트하면 되기 때문에, 적은 시간과 메모리로도 모델을 학습시킬 수 있다는 장점이 있습니다.

### 3. 미니 배치 경사 하강법

미니 배치 경사 하강법(Mini-Batch Gradient Descent)은 Batch size가 `1`도 전체 데이터 개수도 아닌 경우를 말합니다. 미니 배치 경사 하강법은 배치 경사 하강법보다 모델 학습 속도가 빠르고, 확률적 경사 하강법보다 안정적인 장점이 있습니다. 덕분에, 딥러닝 분야에서 가장 많이 활용하는 경사 하강법입니다. 그럼 Batch size는 어떻게 정하면 좋을까요? 일반적으로 `32`, `64`, `128`과 같이 2의 $n$제곱에 해당하는 값으로 사용하는게 보편적입니다.
