## 딥러닝 이진분류 모델링

---

> 1. 전처리<br>
x: 가변수화, 스케일링

> 2. 모델링<br>
은닉층: `activation='relu'`<br>
출력층: `activation='sigmoid'` (0~1 확률값으로 변환), Node 개수: `1`<br>
컴파일: Loss Function으로 `binary_crossentropy` 사용

> 3. 검증<br>
`np.where(pred > .5, 1, 0)`: 0~1의 확률값인 예측결과 처리

### Loss Function: binary_crossentropy

<p align="center">
    <img width="650" alt="binary_crossentropy" src="https://github.com/zacinthepark/TIL/assets/86648892/d3ccd503-0e39-4e6b-8b46-cbeee47a2388">
</p>

- 이진분류 모델에서 `binary_crossentropy`를 손실함수로 사용

- `sigmoid`를 거쳐 0~1 범위의 확률값 $\hat{y}$와 0 or 1 값을 갖는 $y$값 사이의 오차를 계산

- ${-\frac{1}{n} \sum y \log{\hat{y}} + (1 - y) \log{(1 - \hat{y})}}$

    - $\text{err}_1 = -\log{\hat{y}_1}$: 실제값이 1이라면, 예측값이 1에 가까울수록 오차가 0에 가깝게 만들기

    - $\text{err}_0 = -\log{(1 - \hat{y}_0)}$: 실제값이 0이라면, 예측값이 0에 가까울수록 오차가 0에 가깝게 만들기

### 분류 모델 성능 지표 복습

<p align="center">
  <img width="650" alt="binary_confusion_matrix" src="https://github.com/zacinthepark/TIL/assets/86648892/8ad5ec7f-eeab-4f0c-8373-d3bee1c973ca">
  <img width="650" alt="binary_classification_report" src="https://github.com/zacinthepark/TIL/assets/86648892/519fba73-8496-4c58-99b8-1cb9a1682f7d">
</p>

### Code

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.metrics import *
from sklearn.preprocessing import MinMaxScaler

from keras.models import Sequential
from keras.layers import Dense
from keras.backend import clear_session
from keras.optimizers import Adam
```

```python
# 학습곡선 함수
def dl_history_plot(history):
    plt.figure(figsize=(10, 6))
    plt.plot(history['loss'], label='train_err', marker='.')
    plt.plot(history['val_loss'], label='val_err', marker='.')

    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend()
    plt.grid()
    plt.show()
```

```python
path = "https://raw.githubusercontent.com/DA4BAM/dataset/master/Attrition_train_validation.CSV"
data = pd.read_csv(path)
data['Attrition'] = np.where(data['Attrition'] == 'Yes', 1, 0)
data.head(10)
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Attrition</th>
      <th>Age</th>
      <th>BusinessTravel</th>
      <th>Department</th>
      <th>DistanceFromHome</th>
      <th>Education</th>
      <th>EducationField</th>
      <th>EmployeeNumber</th>
      <th>EnvironmentSatisfaction</th>
      <th>Gender</th>
      <th>...</th>
      <th>OverTime</th>
      <th>PercentSalaryHike</th>
      <th>RelationshipSatisfaction</th>
      <th>StockOptionLevel</th>
      <th>TotalWorkingYears</th>
      <th>TrainingTimesLastYear</th>
      <th>WorkLifeBalance</th>
      <th>YearsAtCompany</th>
      <th>YearsInCurrentRole</th>
      <th>YearsWithCurrManager</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>33</td>
      <td>Travel_Rarely</td>
      <td>Research &amp; Development</td>
      <td>7</td>
      <td>3</td>
      <td>Medical</td>
      <td>817</td>
      <td>3</td>
      <td>Male</td>
      <td>...</td>
      <td>No</td>
      <td>11</td>
      <td>4</td>
      <td>0</td>
      <td>14</td>
      <td>3</td>
      <td>4</td>
      <td>13</td>
      <td>9</td>
      <td>7</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>35</td>
      <td>Travel_Frequently</td>
      <td>Research &amp; Development</td>
      <td>18</td>
      <td>2</td>
      <td>Life Sciences</td>
      <td>1412</td>
      <td>3</td>
      <td>Male</td>
      <td>...</td>
      <td>No</td>
      <td>11</td>
      <td>3</td>
      <td>0</td>
      <td>10</td>
      <td>2</td>
      <td>3</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>42</td>
      <td>Travel_Rarely</td>
      <td>Research &amp; Development</td>
      <td>6</td>
      <td>3</td>
      <td>Medical</td>
      <td>1911</td>
      <td>3</td>
      <td>Male</td>
      <td>...</td>
      <td>No</td>
      <td>13</td>
      <td>2</td>
      <td>1</td>
      <td>18</td>
      <td>3</td>
      <td>4</td>
      <td>13</td>
      <td>7</td>
      <td>7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>46</td>
      <td>Travel_Rarely</td>
      <td>Sales</td>
      <td>2</td>
      <td>3</td>
      <td>Marketing</td>
      <td>1204</td>
      <td>3</td>
      <td>Female</td>
      <td>...</td>
      <td>No</td>
      <td>23</td>
      <td>1</td>
      <td>0</td>
      <td>28</td>
      <td>2</td>
      <td>3</td>
      <td>26</td>
      <td>15</td>
      <td>9</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>39</td>
      <td>Travel_Frequently</td>
      <td>Sales</td>
      <td>20</td>
      <td>3</td>
      <td>Life Sciences</td>
      <td>1812</td>
      <td>3</td>
      <td>Male</td>
      <td>...</td>
      <td>No</td>
      <td>18</td>
      <td>4</td>
      <td>1</td>
      <td>7</td>
      <td>6</td>
      <td>3</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>5</th>
      <td>1</td>
      <td>22</td>
      <td>Travel_Frequently</td>
      <td>Research &amp; Development</td>
      <td>4</td>
      <td>1</td>
      <td>Technical Degree</td>
      <td>593</td>
      <td>3</td>
      <td>Male</td>
      <td>...</td>
      <td>No</td>
      <td>16</td>
      <td>3</td>
      <td>0</td>
      <td>4</td>
      <td>3</td>
      <td>3</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0</td>
      <td>24</td>
      <td>Travel_Rarely</td>
      <td>Research &amp; Development</td>
      <td>21</td>
      <td>2</td>
      <td>Technical Degree</td>
      <td>1551</td>
      <td>3</td>
      <td>Male</td>
      <td>...</td>
      <td>No</td>
      <td>14</td>
      <td>2</td>
      <td>3</td>
      <td>2</td>
      <td>3</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0</td>
      <td>34</td>
      <td>Travel_Rarely</td>
      <td>Research &amp; Development</td>
      <td>8</td>
      <td>3</td>
      <td>Medical</td>
      <td>2068</td>
      <td>2</td>
      <td>Male</td>
      <td>...</td>
      <td>No</td>
      <td>12</td>
      <td>1</td>
      <td>0</td>
      <td>6</td>
      <td>3</td>
      <td>4</td>
      <td>4</td>
      <td>3</td>
      <td>2</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>30</td>
      <td>Travel_Rarely</td>
      <td>Research &amp; Development</td>
      <td>20</td>
      <td>3</td>
      <td>Other</td>
      <td>1084</td>
      <td>3</td>
      <td>Male</td>
      <td>...</td>
      <td>No</td>
      <td>15</td>
      <td>3</td>
      <td>1</td>
      <td>7</td>
      <td>1</td>
      <td>2</td>
      <td>6</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>26</td>
      <td>Travel_Rarely</td>
      <td>Research &amp; Development</td>
      <td>6</td>
      <td>3</td>
      <td>Life Sciences</td>
      <td>686</td>
      <td>3</td>
      <td>Female</td>
      <td>...</td>
      <td>Yes</td>
      <td>13</td>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>2</td>
      <td>3</td>
      <td>3</td>
      <td>2</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>10 rows × 26 columns</p>

----

```python
target = 'Attrition'
```

```python
# 불필요한 변수 제거
data.drop('EmployeeNumber', axis=1, inplace=True)
```

```python
x = data.drop(target, axis=1)
y = data.loc[:, target]
```

```python
# 가변수화
dumm_cols = ['BusinessTravel','Department','Education','EducationField','EnvironmentSatisfaction','Gender',
            'JobRole', 'JobInvolvement', 'JobSatisfaction', 'MaritalStatus', 'OverTime', 'RelationshipSatisfaction',
            'StockOptionLevel','WorkLifeBalance' ]

x = pd.get_dummies(x, columns=dumm_cols ,drop_first=True)
x.head()
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>DistanceFromHome</th>
      <th>MonthlyIncome</th>
      <th>NumCompaniesWorked</th>
      <th>PercentSalaryHike</th>
      <th>TotalWorkingYears</th>
      <th>TrainingTimesLastYear</th>
      <th>YearsAtCompany</th>
      <th>YearsInCurrentRole</th>
      <th>YearsWithCurrManager</th>
      <th>...</th>
      <th>OverTime_Yes</th>
      <th>RelationshipSatisfaction_2</th>
      <th>RelationshipSatisfaction_3</th>
      <th>RelationshipSatisfaction_4</th>
      <th>StockOptionLevel_1</th>
      <th>StockOptionLevel_2</th>
      <th>StockOptionLevel_3</th>
      <th>WorkLifeBalance_2</th>
      <th>WorkLifeBalance_3</th>
      <th>WorkLifeBalance_4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>33</td>
      <td>7</td>
      <td>11691</td>
      <td>0</td>
      <td>11</td>
      <td>14</td>
      <td>3</td>
      <td>13</td>
      <td>9</td>
      <td>7</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>35</td>
      <td>18</td>
      <td>9362</td>
      <td>2</td>
      <td>11</td>
      <td>10</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>42</td>
      <td>6</td>
      <td>13348</td>
      <td>9</td>
      <td>13</td>
      <td>18</td>
      <td>3</td>
      <td>13</td>
      <td>7</td>
      <td>7</td>
      <td>...</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>46</td>
      <td>2</td>
      <td>17048</td>
      <td>8</td>
      <td>23</td>
      <td>28</td>
      <td>2</td>
      <td>26</td>
      <td>15</td>
      <td>9</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>39</td>
      <td>20</td>
      <td>4127</td>
      <td>2</td>
      <td>18</td>
      <td>7</td>
      <td>6</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 53 columns</p>

```python
# 데이터 분할
x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=200, random_state=2022)
```

```python
# Scaling
scaler = MinMaxScaler()
x_train = scaler.fit_transform(x_train)
x_val = scaler.transform(x_val)
```

```python
nfeatures = x_train.shape[1]
nfeatures
```

<pre>
53
</pre>

```python
# 53, 60, 60, 10, 5, 1
clear_session()
model1 = Sequential([Dense(60, input_shape=(nfeatures,), activation='relu'),
                     Dense(60, activation='relu'),
                     Dense(10, activation='relu'),
                     Dense(5, activation='relu'),
                     Dense(1, activation='sigmoid')])
model1.summary()
```

<pre>
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 60)                3240      
                                                                 
 dense_1 (Dense)             (None, 60)                3660      
                                                                 
 dense_2 (Dense)             (None, 10)                610       
                                                                 
 dense_3 (Dense)             (None, 5)                 55        
                                                                 
 dense_4 (Dense)             (None, 1)                 6         
                                                                 
=================================================================
Total params: 7571 (29.57 KB)
Trainable params: 7571 (29.57 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</pre>

```python
model1.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy')
hist = model1.fit(x_train, y_train, epochs=100, validation_split=.2, verbose=0).history
```

```python
dl_history_plot(hist)
```

![z_dl_5_1](https://github.com/zacinthepark/TIL/assets/86648892/91f6c30b-0a45-4514-bcd0-94b9bf1aa8a7)

```python
# 과적합 발생
```

```python
y_pred1 = model1.predict(x_val)
print(y_pred1[:5])
y_pred1 = np.where(y_pred1 >= .5, 1, 0)
print(confusion_matrix(y_val, y_pred1))
print(classification_report(y_val, y_pred1))
```

<pre>
7/7 [==============================] - 0s 3ms/step
[[5.7492068e-04]
 [1.0000000e+00]
 [2.0332253e-07]
 [1.0000000e+00]
 [8.8826305e-01]]
[[161   8]
 [ 14  17]]
              precision    recall  f1-score   support

           0       0.92      0.95      0.94       169
           1       0.68      0.55      0.61        31

    accuracy                           0.89       200
   macro avg       0.80      0.75      0.77       200
weighted avg       0.88      0.89      0.89       200
</pre>

```python
print(data['Attrition'].value_counts(normalize=True))
sns.countplot(x='Attrition', data=data)
plt.grid()
plt.show()
```

![z_dl_5_2](https://github.com/zacinthepark/TIL/assets/86648892/999e1c3c-d960-46b4-a5ac-e97899f65943)

```python
# 53, 26, 13, 6, 3, 1
```

```python
# 53, 26, 13, 6, 1
clear_session()
model2 = Sequential([Dense(26, input_shape=(nfeatures,), activation='relu'),
                     Dense(13, activation='relu'),
                     Dense(6, activation='relu'),
                     Dense(3, activation='relu'),
                     Dense(1, activation='sigmoid')])
model2.summary()
```

<pre>
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 26)                1404      
                                                                 
 dense_1 (Dense)             (None, 13)                351       
                                                                 
 dense_2 (Dense)             (None, 6)                 84        
                                                                 
 dense_3 (Dense)             (None, 3)                 21        
                                                                 
 dense_4 (Dense)             (None, 1)                 4         
                                                                 
=================================================================
Total params: 1864 (7.28 KB)
Trainable params: 1864 (7.28 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</pre>

```python
model2.compile(optimizer=Adam(learning_rate=0.005), loss='binary_crossentropy')
hist = model2.fit(x_train, y_train, epochs=20, validation_split=.2, verbose=0).history
```

```python
dl_history_plot(hist)
```

![z_dl_5_3](https://github.com/zacinthepark/TIL/assets/86648892/399e1f8a-3414-40c7-a952-67f107a2948c)

```python
y_pred2 = model2.predict(x_val)
print(y_pred2[:5])
y_pred2 = np.where(y_pred2 >= .5, 1, 0)
print(classification_report(y_val, y_pred2))
```

<pre>
7/7 [==============================] - 0s 4ms/step
[[4.2781157e-05]
 [9.9991518e-01]
 [2.7836299e-08]
 [9.9963707e-01]
 [2.7594403e-03]]
              precision    recall  f1-score   support

           0       0.90      0.90      0.90       169
           1       0.45      0.45      0.45        31

    accuracy                           0.83       200
   macro avg       0.68      0.68      0.68       200
weighted avg       0.83      0.83      0.83       200
</pre>

#### **Resampling**

- 이직을 하는 인원은 이직을 하지 않는 인원에 비해 적은 것이 당연
- 클래스 불균형을 해소시켜보자

```python
# Over Sampling
from imblearn.over_sampling import RandomOverSampler
```

```python
ros = RandomOverSampler()
x_train_ros, y_train_ros = ros.fit_resample(x_train, y_train)
```

```python
print(type(y_train_ros))
```

<pre>
<class 'pandas.core.series.Series'>
</pre>

```python
print(y_train.value_counts(normalize=True))
print(y_train.value_counts())
```

<pre>
0    0.839048
1    0.160952
Name: Attrition, dtype: float64
0    881
1    169
Name: Attrition, dtype: int64
</pre>

```python
print(y_train_ros.value_counts(normalize=True))
print(y_train_ros.value_counts())
```

<pre>
0    0.5
1    0.5
Name: Attrition, dtype: float64
0    881
1    881
Name: Attrition, dtype: int64
</pre>

```python
# 53, 60, 60, 10, 5, 1
clear_session()
model3 = Sequential([Dense(60, input_shape=(nfeatures,), activation='relu'),
                     Dense(60, activation='relu'),
                     Dense(10, activation='relu'),
                     Dense(5, activation='relu'),
                     Dense(1, activation='sigmoid')])
model3.summary()
```

<pre>
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 60)                3240      
                                                                 
 dense_1 (Dense)             (None, 60)                3660      
                                                                 
 dense_2 (Dense)             (None, 10)                610       
                                                                 
 dense_3 (Dense)             (None, 5)                 55        
                                                                 
 dense_4 (Dense)             (None, 1)                 6         
                                                                 
=================================================================
Total params: 7571 (29.57 KB)
Trainable params: 7571 (29.57 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</pre>

```python
model3.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy')
hist = model3.fit(x_train_ros, y_train_ros, epochs=100, validation_split=.2, verbose=0).history
```

```python
dl_history_plot(hist)
```

![z_dl_5_4](https://github.com/zacinthepark/TIL/assets/86648892/a03e74d6-48ba-4a19-a6c0-b45c16716b77)

```python
y_pred3 = model3.predict(x_val)
print(y_pred3[:5])
y_pred3 = np.where(y_pred3 >= .5, 1, 0)
print(confusion_matrix(y_val, y_pred3))
print(classification_report(y_val, y_pred3))
```

<pre>
7/7 [==============================] - 0s 2ms/step
[[8.8074998e-08]
 [1.0000000e+00]
 [1.5865002e-08]
 [9.7817832e-01]
 [2.9671431e-01]]
[[162   7]
 [ 15  16]]
              precision    recall  f1-score   support

           0       0.92      0.96      0.94       169
           1       0.70      0.52      0.59        31

    accuracy                           0.89       200
   macro avg       0.81      0.74      0.76       200
weighted avg       0.88      0.89      0.88       200
</pre>

```python
# 해당 모델에서는 1에 대한 recall 값이 유의하게 올라가지 않았지만, Upsampling과 같이 클래스 불균형을 해결하면 1에 대한 recall 값을 올리는 것이 정상
```
