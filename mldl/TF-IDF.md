### TF-IDF

주어진 문서를 요약할 때 어떠한 방식으로 하는지에 대한 논의 (Document Summarization)
TF-IDF(term frequency-inverse document frequency)란 코퍼스(corpus, 문서집합)에서 한 단어가 얼마나 중요한지를 수치적으로 나타낸 가중치이다(term-weighting)
TF-IDF는 정보 검색과 텍스트 마이닝에서 이용하는 가중치로, 여러 문서로 이루어진 문서군이 있을 때 어떤 단어가 특정 문서 내에서 얼마나 중요한 것인지를 나타내는 통계적 수치이다

TF: Term Frequency, 많이 나오는 단어일수록 대표 단어일 확률이 높다
IDF: 해당 문서를 위한 대표성이 높을수록 높은 가중치를 부여
    - 100개의 문서 중 1개에서만 나온다면 `100/1`처럼 역수를 통해 높은 가중치를 부여
    - 100개의 문서 중 100개에서 나온다면 `100/100`처럼 역수를 통해 낮은 가중치를 부여

`TF(t, d) * IDF(t, D)`
    - t는 term, d는 대상으로 하고 있는 문서, D는 문서들의 전체 집합

키워드를 정해진 개수만큼 뽑아서, 이를 벡터로 변환하여 처리한다

문서 간 유사도 판별
TF-IDF Vectorizer를 통해 벡터를 만들 것이다
벡터간 각이 좁으면 유사, 90도면 상관없음, 반대를 바라보면 반대 의견을 가진 문서
검색엔진의 원리는 검색어의 벡터와 유사한 벡터들을 정렬하여 보여주기

추천 시스템도 벡터를 통한 유사도 판별 가능
넷플릭스도 끝까지 본 컨텐츠들의 키워드들을 벡터로 만들어 신규 컨텐츠와의 벡터와 비교

### 단어 표현 (Word Representation, Word Embedding, Word Vector)

One-Hot Encoding 방식은 사이즈가 굉장히 크고, 변화를 반영하기에 부적절하다
또한 각 벡터들이 basis vector, 기저 벡터들이 되는데, 이 기저 벡터들은 서로 90도가 되어 연관성을 표현하기 어렵다

각이 0~90일 때는 양의 관계, 90일 때는 관계없음, 90~180일 때는 음의 관계
이를 대변하는 함수가 코사인 함수

벡터들의 길이로 나눠주어 길이에 대한 의존성을 줄여준다
외적은 벡터를 곱해서 또 다른 벡터가 나온다는 것
내적은 벡터를 원소값들을 곱하고 더하여 하나의 값을 만든다 (유사도 판별에 사용)

### 유사도 측정

#### 코사인 유사도

분모는 길이로 나눠주기
분자는 내적값

#### 자카드 유사도

순서까지 고려하는 경우도 있고, 순서를 고려하지 않는 경우도 있다

### 비지도학습

고전적인 언어처리는 비지도학습을 따른다
어떤 데이터를 받으면, 수치화하고, 유사도를 판단하여 해당 데이터를 취할지 말지 결정한다
액션의 시퀀스가 있으면 강화학습 (한 번의 의사결정이 아닌 반복적인 의사결정)

Clustering
1 명확한 정답이 있는 것은 아니다
2 나누는 기준에 따라 다르게 나눌 수 있다

#### Minkowski Distance

일반적인 표기법

#### K-Means

K는 몇 개의 클러스터로 나눌지
K의 숫자를 미리 정해주지 못한다는 것이 단점

#### KNN Clustering

분류가 목적
근접한 K개에 대해서 다수결을 통해 판별

Density-Based는 밀도도 고려
Model-Based는 미리 모델을 세팅하고 해당 데이터가 가장 많을 때로 판별
Spectral은 데이터를 인코딩하여 벡터로 만드는 과정에서 문제가 있을 것이라 보통 예상은 하지만, 이러한 특수한 경우가 있을 수는 있다

Ensemble Approach의 경우 다양한 파라미터를 적용해보고 일정하게 분류되는 것을 적용

### Word Embedding

고차원 저밀도 벡터를 저차원 고밀도 벡터로 변환
One hot encoding은 9개의 카테고리에 해당하는지 여부를 표시 (사이즈 9 벡터)
colors, shapes를 기준으로 사이즈 2 벡터를 생성

S QUARD: 스탠포드가 만듬
Kor QUARD: LG CNS가 만듬
학습시켜서 벡터를 배포하는 것
