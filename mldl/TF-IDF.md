### TF-IDF

주어진 문서를 요약할 때 어떠한 방식으로 하는지에 대한 논의 (Document Summarization)
TF-IDF(term frequency-inverse document frequency)란 코퍼스(corpus, 문서집합)에서 한 단어가 얼마나 중요한지를 수치적으로 나타낸 가중치이다(term-weighting)
TF-IDF는 정보 검색과 텍스트 마이닝에서 이용하는 가중치로, 여러 문서로 이루어진 문서군이 있을 때 어떤 단어가 특정 문서 내에서 얼마나 중요한 것인지를 나타내는 통계적 수치이다

TF: Term Frequency, 많이 나오는 단어일수록 대표 단어일 확률이 높다
IDF: 해당 문서를 위한 대표성이 높을수록 높은 가중치를 부여
    - 100개의 문서 중 1개에서만 나온다면 `100/1`처럼 역수를 통해 높은 가중치를 부여
    - 100개의 문서 중 100개에서 나온다면 `100/100`처럼 역수를 통해 낮은 가중치를 부여

`TF(t, d) * IDF(t, D)`
    - t는 term, d는 대상으로 하고 있는 문서, D는 문서들의 전체 집합

키워드를 정해진 개수만큼 뽑아서, 이를 벡터로 변환하여 처리한다

문서 간 유사도 판별
TF-IDF Vectorizer를 통해 벡터를 만들 것이다
벡터간 각이 좁으면 유사, 90도면 상관없음, 반대를 바라보면 반대 의견을 가진 문서
검색엔진의 원리는 검색어의 벡터와 유사한 벡터들을 정렬하여 보여주기

추천 시스템도 벡터를 통한 유사도 판별 가능
넷플릭스도 끝까지 본 컨텐츠들의 키워드들을 벡터로 만들어 신규 컨텐츠와의 벡터와 비교

### 단어 표현 (Word Representation, Word Embedding, Word Vector)

One-Hot Encoding 방식은 사이즈가 굉장히 크고, 변화를 반영하기에 부적절하다
또한 각 벡터들이 basis vector, 기저 벡터들이 되는데, 이 기저 벡터들은 서로 90도가 되어 연관성을 표현하기 어렵다

각이 0~90일 때는 양의 관계, 90일 때는 관계없음, 90~180일 때는 음의 관계
이를 대변하는 함수가 코사인 함수

벡터들의 길이로 나눠주어 길이에 대한 의존성을 줄여준다
외적은 벡터를 곱해서 또 다른 벡터가 나온다는 것
내적은 벡터를 원소값들을 곱하고 더하여 하나의 값을 만든다 (유사도 판별에 사용)


