## 딥러닝 성능 관리

---

- [수동 작업을 통한 성능 높이기](#수동-작업을-통한-성능-높이기)
- [과적합 방지하기 및 모델 저장하기](#과적합-방지하기-및-모델-저장하기)

> 1. 데이터 정제, 모델 구조 변경, 학습 과정 조정(Epochs, learning_rate)<br>
Epochs의 경우 Model Check Point, Early Stopping을 통해 최적 모델 저장 가능

> 2. 과적합 방지

### 모델링의 목표

- **적절한** 예측력을 얻기 위해, **적절한** 복잡도의 모델을 생성: **일반화** 된 성능이 중요

- 모델의 복잡도란 학습 데이터 안에 포함된 패턴을 모델에 반영한 정도

- 대체로 하이퍼파라미터 조정에 따라 복잡도가 달라짐
    - KNN: `n_neighbors`, `metric`
    - DT: `max_depth`, `min_samples_leaf`
    - XGB: `n_estimators`, `max_depth`, `learning_rate`
    - **DL: Hidden Layer의 수, Node 수, `learning_rate`, `epochs`**

## 수동 작업을 통한 성능 높이기

---

- [데이터 정제](#1-데이터-정제)
- [모델 구조 조정](#2-모델-구조-조정)
- [학습 과정 조정](#3-학습-과정-조정)

### 1. 데이터 정제

<p align="center">
    <img width=500, alt="variance_bias", src="https://github.com/zacinthepark/TIL/assets/86648892/8cf2c211-c350-4f7e-955e-d8da493d99a2">
</p>

- 입력 데이터 정제, 적절한 전처리
- 데이터 늘리기
    - 열: 적절한 Feature 추가 &rarr; 성능 향상 (Bias 줄이기)
        - Bias: 검증 성능이 얼마나 목표에 근접한지
    - 행: 데이터 건수 늘리기 &rarr; 성능 편차 줄이기 (Variance 줄이기)
        - Variance: 랜덤하게 돌린 검증들 간 성능 편차

### 2. 모델 구조 조정

1. Hidden Layer, Node 수 탐색 및 늘리기 (권장): **Elbow Method** (성능 탐색에 있어 팔꿈치처럼 꺾이는 근방을 탐색해라)

2. 반복문: `keras-tuner` 활용

#### 1. Node 개수 Search

```python
def modeling_test1(node) :
    # 노드 수를 입력 받아 모델 선언
    clear_session()
    model = Sequential([Dense(node, input_shape=(nfeatures,), activation='relu'),
                        Dense(1)])

    model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')
    model.fit(x_train, y_train, epochs=50, verbose=False)

    pred = model.predict(x_val)
    mae = mean_absolute_error(y_val, pred)

    return mae
```

```python
# 진행율 표시용
from tqdm import tqdm
```

```python
nodes = [2, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150]
result = []
for n in tqdm(nodes):
    result.append(modeling_test1(n))
```

```
  0%|          | 0/17 [00:00<?, ?it/s]
3/3 [==============================] - 0s 3ms/step

  6%|▌         | 1/17 [00:03<00:50,  3.16s/it]
3/3 [==============================] - 0s 3ms/step

 12%|█▏        | 2/17 [00:06<00:48,  3.21s/it]WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd44ce78550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
3/3 [==============================] - 0s 4ms/step

 18%|█▊        | 3/17 [00:08<00:36,  2.59s/it]WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd44ce7aef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
3/3 [==============================] - 0s 5ms/step

 24%|██▎       | 4/17 [00:10<00:30,  2.33s/it]
3/3 [==============================] - 0s 3ms/step

 29%|██▉       | 5/17 [00:11<00:25,  2.13s/it]
3/3 [==============================] - 0s 3ms/step

 35%|███▌      | 6/17 [00:15<00:27,  2.47s/it]
3/3 [==============================] - 0s 3ms/step

 41%|████      | 7/17 [00:18<00:27,  2.74s/it]
3/3 [==============================] - 0s 3ms/step

 47%|████▋     | 8/17 [00:21<00:25,  2.86s/it]
3/3 [==============================] - 0s 3ms/step

 53%|█████▎    | 9/17 [00:23<00:20,  2.54s/it]
3/3 [==============================] - 0s 3ms/step

 59%|█████▉    | 10/17 [00:26<00:18,  2.70s/it]
3/3 [==============================] - 0s 3ms/step

 65%|██████▍   | 11/17 [00:28<00:15,  2.63s/it]
3/3 [==============================] - 0s 3ms/step

 71%|███████   | 12/17 [00:30<00:12,  2.40s/it]
3/3 [==============================] - 0s 3ms/step

 76%|███████▋  | 13/17 [00:32<00:08,  2.25s/it]
3/3 [==============================] - 0s 3ms/step

 82%|████████▏ | 14/17 [00:36<00:07,  2.60s/it]
3/3 [==============================] - 0s 10ms/step

 88%|████████▊ | 15/17 [00:39<00:05,  2.77s/it]
3/3 [==============================] - 0s 4ms/step

 94%|█████████▍| 16/17 [00:42<00:02,  2.95s/it]
3/3 [==============================] - 0s 3ms/step

100%|██████████| 17/17 [00:45<00:00,  2.69s/it]
```

```python
# y축은 MAE
plt.plot(nodes, result, marker='o')
plt.grid()
plt.show()
```

![z_dl_3_2_1](https://github.com/zacinthepark/TIL/assets/86648892/109980a4-239b-4ab1-b859-1a5133e2322a)

#### 2. Layer 개수 Search

```python
def modeling_test2(layer) :
    # 레이어 리스트 만들기
    # 레이어 수 만큼 리스트에 레이어 추가
    clear_session()

    # 첫번째 레이어는 input_shape가 필요
    layer_list = [Dense(10, input_shape=(nfeatures,), activation='relu')]

    # 주어진 레이어 수에 맞게 레이어 추가
    for i in range(2, layer) :  # 첫번째 레이어, 아웃풋 레이어는 명시적으로 추가하므로 2부터 시작
        layer_list.append(Dense(10, activation='relu'))

    # Output Layer 추가하고 모델 선언
    layer_list.append(Dense(1))
    model = Sequential(layer_list)

    # 레이어 잘 추가된 건지 확인하기 위해 summary 출력
    print(model.summary())

    model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')
    model.fit(x_train, y_train, epochs=50, verbose=False)

    pred = model.predict(x_val)
    mae = mean_absolute_error(y_val, pred)

    return mae
```

```python
layers = list(range(1, 11))
result = []
for l in tqdm(layers):
    result.append(modeling_test2(l))
```

```
  0%|          | 0/10 [00:00<?, ?it/s]

Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 10)                190       
                                                                 
 dense_1 (Dense)             (None, 1)                 11        
                                                                 
=================================================================
Total params: 201 (804.00 Byte)
Trainable params: 201 (804.00 Byte)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None
3/3 [==============================] - 0s 3ms/step

 10%|█         | 1/10 [00:01<00:16,  1.86s/it]

Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 10)                190       
                                                                 
 dense_1 (Dense)             (None, 1)                 11        
                                                                 
=================================================================
Total params: 201 (804.00 Byte)
Trainable params: 201 (804.00 Byte)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None
3/3 [==============================] - 0s 3ms/step

 20%|██        | 2/10 [00:03<00:14,  1.83s/it]

Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 10)                190       
                                                                 
 dense_1 (Dense)             (None, 10)                110       
                                                                 
 dense_2 (Dense)             (None, 1)                 11        
                                                                 
=================================================================
Total params: 311 (1.21 KB)
Trainable params: 311 (1.21 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None
3/3 [==============================] - 0s 3ms/step

 30%|███       | 3/10 [00:07<00:17,  2.54s/it]

Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 10)                190       
                                                                 
 dense_1 (Dense)             (None, 10)                110       
                                                                 
 dense_2 (Dense)             (None, 10)                110       
                                                                 
 dense_3 (Dense)             (None, 1)                 11        
                                                                 
=================================================================
Total params: 421 (1.64 KB)
Trainable params: 421 (1.64 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None
3/3 [==============================] - 0s 3ms/step

 40%|████      | 4/10 [00:09<00:15,  2.53s/it]

Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 10)                190       
                                                                 
 dense_1 (Dense)             (None, 10)                110       
                                                                 
 dense_2 (Dense)             (None, 10)                110       
                                                                 
 dense_3 (Dense)             (None, 10)                110       
                                                                 
 dense_4 (Dense)             (None, 1)                 11        
                                                                 
=================================================================
Total params: 531 (2.07 KB)
Trainable params: 531 (2.07 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None
3/3 [==============================] - 0s 3ms/step

 50%|█████     | 5/10 [00:12<00:13,  2.64s/it]

Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 10)                190       
                                                                 
 dense_1 (Dense)             (None, 10)                110       
                                                                 
 dense_2 (Dense)             (None, 10)                110       
                                                                 
 dense_3 (Dense)             (None, 10)                110       
                                                                 
 dense_4 (Dense)             (None, 10)                110       
                                                                 
 dense_5 (Dense)             (None, 1)                 11        
                                                                 
=================================================================
Total params: 641 (2.50 KB)
Trainable params: 641 (2.50 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None
3/3 [==============================] - 0s 3ms/step

 60%|██████    | 6/10 [00:15<00:11,  2.82s/it]

Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 10)                190       
                                                                 
 dense_1 (Dense)             (None, 10)                110       
                                                                 
 dense_2 (Dense)             (None, 10)                110       
                                                                 
 dense_3 (Dense)             (None, 10)                110       
                                                                 
 dense_4 (Dense)             (None, 10)                110       
                                                                 
 dense_5 (Dense)             (None, 10)                110       
                                                                 
 dense_6 (Dense)             (None, 1)                 11        
                                                                 
=================================================================
Total params: 751 (2.93 KB)
Trainable params: 751 (2.93 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None
3/3 [==============================] - 0s 3ms/step

 70%|███████   | 7/10 [00:19<00:09,  3.26s/it]

Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 10)                190       
                                                                 
 dense_1 (Dense)             (None, 10)                110       
                                                                 
 dense_2 (Dense)             (None, 10)                110       
                                                                 
 dense_3 (Dense)             (None, 10)                110       
                                                                 
 dense_4 (Dense)             (None, 10)                110       
                                                                 
 dense_5 (Dense)             (None, 10)                110       
                                                                 
 dense_6 (Dense)             (None, 10)                110       
                                                                 
 dense_7 (Dense)             (None, 1)                 11        
                                                                 
=================================================================
Total params: 861 (3.36 KB)
Trainable params: 861 (3.36 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None
3/3 [==============================] - 0s 3ms/step

 80%|████████  | 8/10 [00:23<00:06,  3.44s/it]

Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 10)                190       
                                                                 
 dense_1 (Dense)             (None, 10)                110       
                                                                 
 dense_2 (Dense)             (None, 10)                110       
                                                                 
 dense_3 (Dense)             (None, 10)                110       
                                                                 
 dense_4 (Dense)             (None, 10)                110       
                                                                 
 dense_5 (Dense)             (None, 10)                110       
                                                                 
 dense_6 (Dense)             (None, 10)                110       
                                                                 
 dense_7 (Dense)             (None, 10)                110       
                                                                 
 dense_8 (Dense)             (None, 1)                 11        
                                                                 
=================================================================
Total params: 971 (3.79 KB)
Trainable params: 971 (3.79 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None
3/3 [==============================] - 0s 3ms/step

 90%|█████████ | 9/10 [00:30<00:04,  4.42s/it]

Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 10)                190       
                                                                 
 dense_1 (Dense)             (None, 10)                110       
                                                                 
 dense_2 (Dense)             (None, 10)                110       
                                                                 
 dense_3 (Dense)             (None, 10)                110       
                                                                 
 dense_4 (Dense)             (None, 10)                110       
                                                                 
 dense_5 (Dense)             (None, 10)                110       
                                                                 
 dense_6 (Dense)             (None, 10)                110       
                                                                 
 dense_7 (Dense)             (None, 10)                110       
                                                                 
 dense_8 (Dense)             (None, 10)                110       
                                                                 
 dense_9 (Dense)             (None, 1)                 11        
                                                                 
=================================================================
Total params: 1081 (4.22 KB)
Trainable params: 1081 (4.22 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
None
3/3 [==============================] - 0s 4ms/step

100%|██████████| 10/10 [00:35<00:00,  3.51s/it]
```

```python
# y축은 MAE
plt.plot(layers, result, marker='o')
plt.grid()
plt.show()
```

![z_dl_3_2_2](https://github.com/zacinthepark/TIL/assets/86648892/5423a8c0-8e72-4b5f-a287-1ff4928e5433)

### 3. 학습 과정 조정

1. Epochs: 10~50에서 시작 &rarr; Model Check Point, Early Stopping으로 최적 모델 저장 가능

2. Learning Rate: 0.1~0.001 사이에서 시작

## 과적합 방지하기 및 모델 저장하기

---

- [Early Stopping](#early-stopping)
- [Regularization](#regularization)
- [모델 저장하기: 최종 모델 저장하기](#모델-저장하기-최종-모델-저장하기)
- [모델 저장하기: 중간 체크포인트 저장하기](#모델-저장하기-중간-체크포인트-저장하기)
- [Dropout](#dropout)
- [성능관리 실습코드](#성능관리-실습코드)

### 과적합: 왜 문제가 될까?

- 과적합 방지는 검증 성능 최적화와 일맥상통한다

- 과적합은 학습 데이터에만 존재하는 특이한 성질인 가짜 패턴까지 학습한 상태로, 그 외 데이터(모집단 전체)를 적절히 예측하지 못한다

- 딥러닝에서의 과적합 방지를 위한 조절
    - 데이터 건수 늘리기
    - `epochs`와 `learning_rate`
    - hidden layer, node 개수
    - Early Stopping
    - Regularization(규제): L1, L2
    - Dropout

### Early Stopping

<p align="center">
    <img width="500" alt="early_stopping" src="https://github.com/zacinthepark/TIL/assets/86648892/c560a2ab-e374-4f4e-8680-92f9452bcc38">
</p>

```python
from keras.callbacks import EarlyStopping

es = EarlyStopping(monitor='val_loss', min_delta=0, patience=0)

model.fit(x_train, y_train, epochs=100, validation_split=.2, callbacks=[es])
```

- `monitor='val_loss'`: 관심사가 validation error라는 것

- `min_delta`: 현재 모니터링 중인 오차의 최소값에서 이 값보다 커야 줄어야 인정해주는지 지정

- `patience`: 오차의 최소값에서 `min_delta`보다 줄어들지 않는 것을 몇 번 기다려줄 것인지 지정
    - 한 번 줄어들면 patience의 카운트는 초기화됨

- `callbacks`: epoch 단위로 학습이 진행되는동안 중간에 개입할 task 지정

### Regularization

<p align="center">
    <img width="500" alt="compare_regularization" src="https://github.com/zacinthepark/TIL/assets/86648892/fbcc8077-16d4-49aa-9ded-0e51f95d6c67">
</p>

- 파라미터의 수가 많아질수록 딥러닝 모델의 복잡도는 올라간다
- 파라미터에 규제를 가하여 줄여보겠다는 것이 가중치 규제
- 이처럼 파라미터에 규제를 가하여 과적합을 방지하고 모델의 복잡도를 낮추려는 시도
    - 간단히 말해 파라미터를 정리하는 것
- 강도(람다값)가 높을수록 일반화된(단순한) 모델이 됨
- 일반적으로 한 Layer에서 L1이나 L2 규제를 사용했다면, 다른 Layer에서도 동일한 규제 방법 사용
- 모든 층에 규제를 할 수도 있고, 노드가 많은 층게 규제를 할 수도 있고, 여러 실험을 통해 성능 최적화를 진행

#### L1 규제: Lasso

- 오차함수 = 오차 + $\lambda * \sum |w|$
    - $\lambda$: 규제 강도
    - $\sum |w|$: 가중치(파라미터) 절대값의 합

- 효과: 가중치 절대값의 합을 최소화
    - 가중치가 작은 값들을 0으로 만드는 경향

- 일반적인 값의 범위: `0.0001 ~ 0.1`

#### L2 규제: Ridge

- 오차함수 = 오차 + $\lambda * \sum w^2$

- 효과: 가중치 제곱의 합을 최소화
    - 규제 강도에 따라 가중치 영향력을 제어
    - 강도가 크면 큰 가중치가 좀 더 줄어드는 효과, 작은 가중치는 0에 수렴

- 일반적인 값의 범위: `0.001 ~ 0.5`

#### Code

```python
# 은닉칭 안에 옵션으로 지정
from keras.regularizers import l1, l2

model = Sequential([Dense(128, input_shape=(nfeatures,), activation='relu', 
                            kernel_regularizer=l1(0.01)), 
                    Dense(64, activation='relu', 
                            kernel_regularizer=l1(0.01)), 
                    Dense(32, activation='relu', 
                            kernel_regularizer=l1(0.01)), 
                    Dense(1, activation='sigmoid')])
```

### 모델 저장하기: 최종 모델 저장하기

#### 모델 저장하기

- `model.save('filename.h5')`
    - 딥러닝 모델의 메서드로 `.save`가 제공됨
    - 파일이름 `.h5` 파일이 저장됨
    - h5 파일 포맷: 하둡파일 형

#### 모델 로딩하기

- `load_model` 함수는 별도로 불러와야함
- 경로를 맞춰주고 h5 파일을 읽어오면 그대로 사용 가능함

```python
model.save('mnist_model.h5')

from keras.models import load_model
model2 = load_model('mnist_model.h5')
```

### 모델 저장하기: 중간 체크포인트 저장하기

#### ModelCheckPoint

```python
cp_path = '/content/{epoch:03d}.h5'
mcp = ModelCheckPoint(cp_path, monitor='val_loss', verbose=1, save_best_only=True)

# 학습
hist = model.fit(x_train, y_train, epochs=50, validation_split=.2, callbacks=[mcp]).history
```

- `cp_path = '/content/{epoch:03d}.h5'`
    - 모델을 저장할 경로와 모델 파일 이름
    - epoch 번호를 3자리로 표기: 첫번째 epoch &rarr; **001.h5**

- `mcp = ModelCheckPoint(cp_path, monitor='val_loss', verbose=1, save_best_only=True)`
    - `monitor='val_loss'`: validation loss를 기준으로
    - `save_best_only=True`: 이전 체크포인트의 모델들보다 성능이 개선되면 저장

<p align="center">
    <img width="700" alt="mcp" src="https://github.com/zacinthepark/TIL/assets/86648892/f8580e8d-5ba7-4c9e-8bfc-70144c01b1b7">
</p>

- 파일 저장
    - 성능이 개선되는 구간 epoch 1~10까지 파일 저장 (001.h5, 002.h5, 003.h5, ... , 010.h5)
    - epoch 11부터는 성능이 좋은 epoch 10보다 성능이 개선되지 않음

### Dropout

- 과적합을 줄이기 위해 사용되는 정규화(regularization) 기법 중 하나
- 훈련 과정에서 신경망의 일부 뉴런을 임의로 비활성화시킴으로써 모델을 강제로 일반화

#### 학습 시 적용 절차

- 훈련 배치에서 랜덤하게 선택된 일부 뉴런을 제거
- 제거된 뉴런은 해당 배치에 대한 순전파 및 역전파 과정에서 비활성화
- 이를 통해 뉴런들 간의 복잡한 의존성을 줄여줌
- 매 epochs마다 다른 부분 집합의 뉴런을 비활성화: 앙상블 효과

<p align="center">
    <img width="600" alt="dropout_1" src="https://github.com/zacinthepark/TIL/assets/86648892/bfe4e9e2-4ac1-47f7-b708-00e5981beab0">
</p>
<p align="center">
    <img width="600" alt="dropout_2" src="https://github.com/zacinthepark/TIL/assets/86648892/e888ec32-4e7f-4b41-9ee6-52169e41cccf">
</p>

```python
model = Sequential([Dense(128, input_shape=(nfeatures,), activation='relu'), 
                    Dropout(0.4), 
                    Dense(64, activation='relu'), 
                    Dropout(0.4), 
                    Dense(32, activation='relu'), 
                    Dropout(0.4), 
                    Dense(1, activation='sigmoid')])
```

- `0.4`: hidden layer의 노드 중 40%를 임의로 제외시킴
- 보통 0.2~0.5 사이의 범위 지정
    - 조절하면서 찾아야하는 하이퍼파라미터
    - feature가 적을 경우 rate를 낮추고, 많을 경우는 rate를 높이는 시도

### 성능관리 실습코드

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.metrics import *
from sklearn.preprocessing import MinMaxScaler

from keras.models import Sequential
from keras.layers import Dense, Flatten
from keras.backend import clear_session
from keras.optimizers import Adam
from keras.datasets import mnist
```

```python
# 학습곡선 함수
def dl_history_plot(history):
    plt.figure(figsize=(10, 6))
    plt.plot(history['loss'], label='train_err', marker='.')
    plt.plot(history['val_loss'], label='val_err', marker='.')

    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend()
    plt.grid()
    plt.show()
```

#### 과적합을 위한 예제 데이터

- target
- feature: 0 ~ 299

```python
path = "https://raw.githubusercontent.com/DA4BAM/dataset/master/overfit_sample.csv"
data = pd.read_csv(path)
data.head()
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>target</th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>...</th>
      <th>290</th>
      <th>291</th>
      <th>292</th>
      <th>293</th>
      <th>294</th>
      <th>295</th>
      <th>296</th>
      <th>297</th>
      <th>298</th>
      <th>299</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>-1.067</td>
      <td>-1.114</td>
      <td>-0.616</td>
      <td>0.376</td>
      <td>1.090</td>
      <td>0.467</td>
      <td>-0.422</td>
      <td>0.460</td>
      <td>-0.443</td>
      <td>...</td>
      <td>0.220</td>
      <td>-0.339</td>
      <td>0.254</td>
      <td>-0.179</td>
      <td>0.352</td>
      <td>0.125</td>
      <td>0.347</td>
      <td>0.436</td>
      <td>0.958</td>
      <td>-0.824</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>-0.831</td>
      <td>0.271</td>
      <td>1.716</td>
      <td>1.096</td>
      <td>1.731</td>
      <td>-0.197</td>
      <td>1.904</td>
      <td>-0.265</td>
      <td>0.557</td>
      <td>...</td>
      <td>-0.765</td>
      <td>-0.735</td>
      <td>-1.158</td>
      <td>2.554</td>
      <td>0.856</td>
      <td>-1.506</td>
      <td>0.462</td>
      <td>-0.029</td>
      <td>-1.932</td>
      <td>-0.343</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0.099</td>
      <td>1.390</td>
      <td>-0.732</td>
      <td>-1.065</td>
      <td>0.005</td>
      <td>-0.081</td>
      <td>-1.450</td>
      <td>0.317</td>
      <td>-0.624</td>
      <td>...</td>
      <td>-1.311</td>
      <td>0.799</td>
      <td>-1.001</td>
      <td>1.544</td>
      <td>0.575</td>
      <td>-0.309</td>
      <td>-0.339</td>
      <td>-0.148</td>
      <td>-0.646</td>
      <td>0.725</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>-0.989</td>
      <td>-0.916</td>
      <td>-1.343</td>
      <td>0.145</td>
      <td>0.543</td>
      <td>0.636</td>
      <td>1.127</td>
      <td>0.189</td>
      <td>-0.118</td>
      <td>...</td>
      <td>-1.370</td>
      <td>1.093</td>
      <td>0.596</td>
      <td>-0.589</td>
      <td>-0.649</td>
      <td>-0.163</td>
      <td>-0.958</td>
      <td>-1.081</td>
      <td>0.805</td>
      <td>3.401</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0.811</td>
      <td>-1.509</td>
      <td>0.522</td>
      <td>-0.360</td>
      <td>-0.220</td>
      <td>-0.959</td>
      <td>0.334</td>
      <td>-0.566</td>
      <td>-0.656</td>
      <td>...</td>
      <td>-0.178</td>
      <td>0.718</td>
      <td>-1.017</td>
      <td>1.249</td>
      <td>-0.596</td>
      <td>-0.445</td>
      <td>1.751</td>
      <td>1.442</td>
      <td>-0.393</td>
      <td>-0.643</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 301 columns</p>

```python
# 데이터분할: x, y
target = 'target'
x = data.drop(target, axis=1)
y = data.loc[:, target]

# 데이터분할 : train, validation
x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=.2, random_state=20)
```

```python
# 스케일링
scaler = MinMaxScaler()
x_train = scaler.fit_transform(x_train)
x_val = scaler.transform(x_val)
```

```python
# input_shape: feature 수 도출
nfeatures = x_train.shape[1]

# 메모리 정리
clear_session()

# Sequential 타입
model1 = Sequential([Dense(128, input_shape=(nfeatures,), activation='relu'),
                     Dense(64, activation='relu'),
                     Dense(32, activation='relu'),
                     Dense(1, activation='sigmoid')])

# 컴파일
model1.compile(optimizer= Adam(learning_rate=0.001), loss='binary_crossentropy')
```


```python
# 학습
hist = model1.fit(x_train, y_train, epochs=100, validation_split=0.2, verbose=0).history

# 학습결과 그래프
dl_history_plot(hist)
```

![z_dl_7_1](https://github.com/zacinthepark/TIL/assets/86648892/34b045e2-19e8-43d7-801b-1775a6d3e03d)

#### Early Stopping

```python
from keras.callbacks import EarlyStopping
```

```python
# 모델 선언
clear_session()

model2 = Sequential([Dense(128, input_shape=(nfeatures,), activation='relu'),
                      Dense(64, activation='relu'),
                      Dense(32, activation='relu'),
                      Dense(1, activation='sigmoid')])
model2.compile(optimizer= Adam(learning_rate=0.001), loss='binary_crossentropy')

# EarlyStopping 설정
min_de = 0.001
pat = 5

es = EarlyStopping(monitor='val_loss', min_delta=min_de, patience=pat)

# 학습
hist = model2.fit(x_train, y_train, epochs=100, validation_split=0.2, callbacks=[es]).history
dl_history_plot(hist)
```

<pre>
Epoch 1/100
5/5 [==============================] - 3s 170ms/step - loss: 0.6234 - val_loss: 0.5949
Epoch 2/100
5/5 [==============================] - 0s 27ms/step - loss: 0.5720 - val_loss: 0.5825
Epoch 3/100
5/5 [==============================] - 0s 35ms/step - loss: 0.5562 - val_loss: 0.5820
Epoch 4/100
5/5 [==============================] - 0s 38ms/step - loss: 0.5478 - val_loss: 0.5805
Epoch 5/100
5/5 [==============================] - 0s 29ms/step - loss: 0.5407 - val_loss: 0.5819
Epoch 6/100
5/5 [==============================] - 0s 25ms/step - loss: 0.5282 - val_loss: 0.5840
Epoch 7/100
5/5 [==============================] - 0s 35ms/step - loss: 0.5199 - val_loss: 0.5918
Epoch 8/100
5/5 [==============================] - 0s 37ms/step - loss: 0.5057 - val_loss: 0.5881
Epoch 9/100
5/5 [==============================] - 0s 38ms/step - loss: 0.4956 - val_loss: 0.5834
</pre>

![z_dl_7_2](https://github.com/zacinthepark/TIL/assets/86648892/2ab49dad-27b6-4777-9c91-b290ac4e6523)

#### L1 Regularization

```python
# 규제를 위해 필요한 함수 불러오기
from keras.regularizers import l1, l2
```

```python
# 메모리 정리
clear_session()

# Sequential 타입
model4 = Sequential([Dense(128, input_shape=(nfeatures,), activation='relu', kernel_regularizer=l1(0.01)),
                     Dense(64, activation='relu', kernel_regularizer=l1(0.01)),
                     Dense(32, activation='relu', kernel_regularizer=l1(0.01)),
                     Dense(1, activation='sigmoid')])

# 컴파일
model4.compile(optimizer= Adam(learning_rate=0.001), loss='binary_crossentropy')
```

```python
# 학습
hist = model4.fit(x_train, y_train, epochs=100, validation_split=0.2, verbose=0).history

# 학습결과 그래프
dl_history_plot(hist)
```

![z_dl_7_3](https://github.com/zacinthepark/TIL/assets/86648892/f6e74206-8d0a-4e45-9d39-ee195ac690d8)

#### L2 Regularization

```python
# 메모리 정리
clear_session()

# Sequential 타입
model5 = Sequential([Dense(128, input_shape=(nfeatures,), activation='relu', kernel_regularizer=l2(0.05)),
                     Dense(64, activation='relu', kernel_regularizer=l2(0.05)),
                     Dense(32, activation='relu', kernel_regularizer=l2(0.05)),
                     Dense(1, activation='sigmoid')])

# 컴파일
model5.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy')
```

```python
# 학습
hist = model5.fit(x_train, y_train, epochs=100, validation_split=0.2, verbose=0).history

# 학습결과 그래프
dl_history_plot(hist)
```

![z_dl_7_4](https://github.com/zacinthepark/TIL/assets/86648892/b8eef683-2928-4ef1-b336-fcbe16e17f27)

#### Dropout

```python
from keras.layers import Dropout
```

#### Before Dropout

```python
# input_shape: feature 수 도출
nfeatures = x_train.shape[1]

# 메모리 정리
clear_session()

# Sequential 타입
model6 = Sequential([Dense(128, input_shape=(nfeatures,), activation='relu'),
                     Dense(64, activation='relu'),
                     Dense(32, activation='relu'),
                     Dense(1, activation='sigmoid')])

# 컴파일
model6.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy')
```

```python
# 학습
hist = model6.fit(x_train, y_train, epochs=100, validation_split=0.2, verbose=0).history

# 학습결과 그래프
dl_history_plot(hist)
```

![z_dl_7_5](https://github.com/zacinthepark/TIL/assets/86648892/15724304-4902-4aa3-bb08-b93ca59607cb)

#### After Dropout

```python
# 메모리 정리
clear_session()

# Sequential 타입
model6 = Sequential([Dense(128, input_shape=(nfeatures,), activation='relu'), Dropout(0.4),
                     Dense(64, activation='relu'), Dropout(0.4),
                     Dense(32, activation='relu'), Dropout(0.4),
                     Dense(1, activation='sigmoid')])

# 컴파일
model6.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy')
```

```python
# 학습
hist = model6.fit(x_train, y_train, epochs=50, validation_split=0.2, verbose=0).history

# 학습결과 그래프
dl_history_plot(hist)
```

![z_dl_7_6](https://github.com/zacinthepark/TIL/assets/86648892/ffb2c7b9-828a-4428-934c-650c13baf29f)

#### `model.save`

```python
# 모델 저장하기

model1.save('hanky.h5')
```

```python
# 모델 로딩하기

from keras.models import load_model
model2 = load_model('hanky.h5')
```

```python
# 불러온 모델이 학습할 때 데이터 구조와 같은 구조의 데이터를 제공해야 예측 가능
pred2 = model2.predict(x_val)
```

<pre>
2/2 [==============================] - 0s 5ms/step
</pre>

```python
pred2_1 = np.where(pred2 > .5, 1, 0)
```

```python
print(accuracy_score(y_val, pred2_1))
print('-' * 60)
print(confusion_matrix(y_val, pred2_1))
print('-' * 60)
print(classification_report(y_val, pred2_1))
```

<pre>
0.64
------------------------------------------------------------
[[29  6]
 [12  3]]
------------------------------------------------------------
              precision    recall  f1-score   support

           0       0.71      0.83      0.76        35
           1       0.33      0.20      0.25        15

    accuracy                           0.64        50
   macro avg       0.52      0.51      0.51        50
weighted avg       0.60      0.64      0.61        50
</pre>

#### ModelCheckPoint

- 폴더 정리 (위에서 생성한 파일 제거)

```python
import os

def delete_h5_files(directory):
    for filename in os.listdir(directory):
        if filename.endswith(".h5"):
            file_path = os.path.join(directory, filename)
            try:
                os.remove(file_path)
                print(f"Deleted: {file_path}")
            except Exception as e:
                print(f"Error deleting {file_path}: {e}")

# 삭제할 디렉토리 지정
directory_to_delete_from = "/content/"

# 확장자가 .h5인 파일 삭제
delete_h5_files(directory_to_delete_from)
```

<pre>
Deleted: /content/hanky.h5
</pre>

```python
from keras.callbacks import ModelCheckpoint
```

```python
# input_shape: feature 수 도출
nfeatures = x_train.shape[1]

# 메모리 정리
clear_session()

# Sequential 타입
model1 = Sequential([Dense(64, input_shape=(nfeatures,), activation='relu'),
                     Dense(32, activation='relu'),
                     Dense(16, activation='relu'),
                     Dense(1, activation='sigmoid')])

# 컴파일
model1.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy')
```

* 체크포인트 저장

    * 아래 코드에서 ModelCheckpoint 콜백은 검증 데이터의 정확도(val_accuracy)를 기준으로 모델 저장

    * 모델의 성능이 이전에 저장된 모델보다 향상될 때만 저장되도록 save_best_only=True로 설정

    * 또한, verbose=1로 설정하면 모델이 저장될 때마다 콘솔에 메시지가 표시됨.



```python
# 검증 데이터의 정확도(val_accuracy)를 기준으로 모델 저장
# 모델의 성능이 이전에 저장된 모델보다 향상될 때만 저장되도록 save_best_only=True로 설정

cp_path = '/content/{epoch:03d}.h5'
mcp = ModelCheckpoint(cp_path, monitor='val_loss', verbose=1, save_best_only=True)

# 학습
hist = model1.fit(x_train, y_train, epochs=50, validation_split=.2, callbacks=[mcp]).history
```

```
Epoch 1/50
1/5 [=====>........................] - ETA: 4s - loss: 0.5297
Epoch 1: val_loss improved from inf to 0.57060, saving model to /content/001.h5
5/5 [==============================] - 1s 49ms/step - loss: 0.5756 - val_loss: 0.5706
Epoch 2/50
1/5 [=====>........................] - ETA: 0s - loss: 0.6187
Epoch 2: val_loss did not improve from 0.57060
5/5 [==============================] - 0s 9ms/step - loss: 0.5709 - val_loss: 0.5706
Epoch 3/50
1/5 [=====>........................] - ETA: 0s - loss: 0.6698
Epoch 3: val_loss did not improve from 0.57060
5/5 [==============================] - 0s 9ms/step - loss: 0.5684 - val_loss: 0.5711
Epoch 4/50
1/5 [=====>........................] - ETA: 0s - loss: 0.5980
Epoch 4: val_loss did not improve from 0.57060
5/5 [==============================] - 0s 9ms/step - loss: 0.5663 - val_loss: 0.5716
Epoch 5/50
1/5 [=====>........................] - ETA: 0s - loss: 0.5671
Epoch 5: val_loss did not improve from 0.57060
5/5 [==============================] - 0s 10ms/step - loss: 0.5643 - val_loss: 0.5714
Epoch 6/50
1/5 [=====>........................] - ETA: 0s - loss: 0.5601
Epoch 6: val_loss did not improve from 0.57060
5/5 [==============================] - 0s 9ms/step - loss: 0.5625 - val_loss: 0.5713
Epoch 7/50
1/5 [=====>........................] - ETA: 0s - loss: 0.6199
Epoch 7: val_loss did not improve from 0.57060
5/5 [==============================] - 0s 9ms/step - loss: 0.5609 - val_loss: 0.5711
Epoch 8/50
1/5 [=====>........................] - ETA: 0s - loss: 0.3945
Epoch 8: val_loss did not improve from 0.57060
5/5 [==============================] - 0s 9ms/step - loss: 0.5612 - val_loss: 0.5711
Epoch 9/50
1/5 [=====>........................] - ETA: 0s - loss: 0.5892
Epoch 9: val_loss did not improve from 0.57060
5/5 [==============================] - 0s 9ms/step - loss: 0.5585 - val_loss: 0.5706
Epoch 10/50
1/5 [=====>........................] - ETA: 0s - loss: 0.5148
Epoch 10: val_loss improved from 0.57060 to 0.57032, saving model to /content/010.h5
5/5 [==============================] - 0s 14ms/step - loss: 0.5574 - val_loss: 0.5703
Epoch 11/50
1/5 [=====>........................] - ETA: 0s - loss: 0.6135
Epoch 11: val_loss improved from 0.57032 to 0.57007, saving model to /content/011.h5
5/5 [==============================] - 0s 15ms/step - loss: 0.5565 - val_loss: 0.5701
Epoch 12/50
1/5 [=====>........................] - ETA: 0s - loss: 0.5447
Epoch 12: val_loss improved from 0.57007 to 0.56999, saving model to /content/012.h5
5/5 [==============================] - 0s 14ms/step - loss: 0.5553 - val_loss: 0.5700
Epoch 13/50
1/5 [=====>........................] - ETA: 0s - loss: 0.6096
Epoch 13: val_loss improved from 0.56999 to 0.56954, saving model to /content/013.h5
5/5 [==============================] - 0s 14ms/step - loss: 0.5546 - val_loss: 0.5695
Epoch 14/50
1/5 [=====>........................] - ETA: 0s - loss: 0.5288
Epoch 14: val_loss did not improve from 0.56954
5/5 [==============================] - 0s 9ms/step - loss: 0.5536 - val_loss: 0.5697
Epoch 15/50
1/5 [=====>........................] - ETA: 0s - loss: 0.5714
Epoch 15: val_loss improved from 0.56954 to 0.56951, saving model to /content/015.h5
5/5 [==============================] - 0s 15ms/step - loss: 0.5525 - val_loss: 0.5695
Epoch 16/50
1/5 [=====>........................] - ETA: 0s - loss: 0.5670
Epoch 16: val_loss improved from 0.56951 to 0.56924, saving model to /content/016.h5
5/5 [==============================] - 0s 16ms/step - loss: 0.5515 - val_loss: 0.5692
Epoch 17/50
1/5 [=====>........................] - ETA: 0s - loss: 0.5850
Epoch 17: val_loss improved from 0.56924 to 0.56908, saving model to /content/017.h5
5/5 [==============================] - 0s 16ms/step - loss: 0.5506 - val_loss: 0.5691
Epoch 18/50
1/5 [=====>........................] - ETA: 0s - loss: 0.5291
Epoch 18: val_loss improved from 0.56908 to 0.56905, saving model to /content/018.h5
5/5 [==============================] - 0s 14ms/step - loss: 0.5496 - val_loss: 0.5691
Epoch 19/50
1/5 [=====>........................] - ETA: 0s - loss: 0.5011
Epoch 19: val_loss improved from 0.56905 to 0.56905, saving model to /content/019.h5
5/5 [==============================] - 0s 14ms/step - loss: 0.5486 - val_loss: 0.5690
Epoch 20/50
1/5 [=====>........................] - ETA: 0s - loss: 0.5697
Epoch 20: val_loss improved from 0.56905 to 0.56840, saving model to /content/020.h5
5/5 [==============================] - 0s 15ms/step - loss: 0.5476 - val_loss: 0.5684
Epoch 21/50
1/5 [=====>........................] - ETA: 0s - loss: 0.6569
Epoch 21: val_loss improved from 0.56840 to 0.56829, saving model to /content/021.h5
5/5 [==============================] - 0s 14ms/step - loss: 0.5476 - val_loss: 0.5683
Epoch 22/50
1/5 [=====>........................] - ETA: 0s - loss: 0.6009
Epoch 22: val_loss improved from 0.56829 to 0.56809, saving model to /content/022.h5
5/5 [==============================] - 0s 14ms/step - loss: 0.5456 - val_loss: 0.5681
Epoch 23/50
1/5 [=====>........................] - ETA: 0s - loss: 0.5199
Epoch 23: val_loss improved from 0.56809 to 0.56803, saving model to /content/023.h5
5/5 [==============================] - 0s 15ms/step - loss: 0.5447 - val_loss: 0.5680
Epoch 24/50
1/5 [=====>........................] - ETA: 0s - loss: 0.6643
Epoch 24: val_loss improved from 0.56803 to 0.56699, saving model to /content/024.h5
5/5 [==============================] - 0s 14ms/step - loss: 0.5433 - val_loss: 0.5670
Epoch 25/50
1/5 [=====>........................] - ETA: 0s - loss: 0.6222
Epoch 25: val_loss improved from 0.56699 to 0.56591, saving model to /content/025.h5
5/5 [==============================] - 0s 16ms/step - loss: 0.5418 - val_loss: 0.5659
Epoch 26/50
1/5 [=====>........................] - ETA: 0s - loss: 0.4731
Epoch 26: val_loss improved from 0.56591 to 0.56525, saving model to /content/026.h5
5/5 [==============================] - 0s 15ms/step - loss: 0.5402 - val_loss: 0.5653
Epoch 27/50
1/5 [=====>........................] - ETA: 0s - loss: 0.4884
Epoch 27: val_loss improved from 0.56525 to 0.56467, saving model to /content/027.h5
5/5 [==============================] - 0s 15ms/step - loss: 0.5384 - val_loss: 0.5647
Epoch 28/50
1/5 [=====>........................] - ETA: 0s - loss: 0.4968
Epoch 28: val_loss improved from 0.56467 to 0.56426, saving model to /content/028.h5
5/5 [==============================] - 0s 15ms/step - loss: 0.5368 - val_loss: 0.5643
Epoch 29/50
1/5 [=====>........................] - ETA: 0s - loss: 0.5908
Epoch 29: val_loss improved from 0.56426 to 0.56338, saving model to /content/029.h5
5/5 [==============================] - 0s 15ms/step - loss: 0.5353 - val_loss: 0.5634
Epoch 30/50
1/5 [=====>........................] - ETA: 0s - loss: 0.4399
Epoch 30: val_loss improved from 0.56338 to 0.56284, saving model to /content/030.h5
5/5 [==============================] - 0s 14ms/step - loss: 0.5340 - val_loss: 0.5628
Epoch 31/50
1/5 [=====>........................] - ETA: 0s - loss: 0.6809
Epoch 31: val_loss improved from 0.56284 to 0.56240, saving model to /content/031.h5
5/5 [==============================] - 0s 16ms/step - loss: 0.5332 - val_loss: 0.5624
Epoch 32/50
1/5 [=====>........................] - ETA: 0s - loss: 0.6284
Epoch 32: val_loss improved from 0.56240 to 0.56190, saving model to /content/032.h5
5/5 [==============================] - 0s 14ms/step - loss: 0.5314 - val_loss: 0.5619
Epoch 33/50
1/5 [=====>........................] - ETA: 0s - loss: 0.4205
Epoch 33: val_loss did not improve from 0.56190
5/5 [==============================] - 0s 9ms/step - loss: 0.5311 - val_loss: 0.5620
Epoch 34/50
1/5 [=====>........................] - ETA: 0s - loss: 0.5722
Epoch 34: val_loss improved from 0.56190 to 0.56157, saving model to /content/034.h5
5/5 [==============================] - 0s 15ms/step - loss: 0.5291 - val_loss: 0.5616
Epoch 35/50
1/5 [=====>........................] - ETA: 0s - loss: 0.5468
Epoch 35: val_loss improved from 0.56157 to 0.56146, saving model to /content/035.h5
5/5 [==============================] - 0s 14ms/step - loss: 0.5280 - val_loss: 0.5615
Epoch 36/50
1/5 [=====>........................] - ETA: 0s - loss: 0.4861
Epoch 36: val_loss did not improve from 0.56146
5/5 [==============================] - 0s 9ms/step - loss: 0.5267 - val_loss: 0.5615
Epoch 37/50
1/5 [=====>........................] - ETA: 0s - loss: 0.4241
Epoch 37: val_loss improved from 0.56146 to 0.56131, saving model to /content/037.h5
5/5 [==============================] - 0s 14ms/step - loss: 0.5251 - val_loss: 0.5613
Epoch 38/50
1/5 [=====>........................] - ETA: 0s - loss: 0.5524
Epoch 38: val_loss improved from 0.56131 to 0.56092, saving model to /content/038.h5
5/5 [==============================] - 0s 15ms/step - loss: 0.5244 - val_loss: 0.5609
Epoch 39/50
1/5 [=====>........................] - ETA: 0s - loss: 0.5866
Epoch 39: val_loss did not improve from 0.56092
5/5 [==============================] - 0s 9ms/step - loss: 0.5224 - val_loss: 0.5611
Epoch 40/50
1/5 [=====>........................] - ETA: 0s - loss: 0.4292
Epoch 40: val_loss did not improve from 0.56092
5/5 [==============================] - 0s 9ms/step - loss: 0.5216 - val_loss: 0.5614
Epoch 41/50
1/5 [=====>........................] - ETA: 0s - loss: 0.5499
Epoch 41: val_loss improved from 0.56092 to 0.56075, saving model to /content/041.h5
5/5 [==============================] - 0s 14ms/step - loss: 0.5202 - val_loss: 0.5607
Epoch 42/50
1/5 [=====>........................] - ETA: 0s - loss: 0.5016
Epoch 42: val_loss did not improve from 0.56075
5/5 [==============================] - 0s 9ms/step - loss: 0.5193 - val_loss: 0.5610
Epoch 43/50
1/5 [=====>........................] - ETA: 0s - loss: 0.4898
Epoch 43: val_loss did not improve from 0.56075
5/5 [==============================] - 0s 9ms/step - loss: 0.5170 - val_loss: 0.5608
Epoch 44/50
1/5 [=====>........................] - ETA: 0s - loss: 0.4819
Epoch 44: val_loss improved from 0.56075 to 0.56020, saving model to /content/044.h5
5/5 [==============================] - 0s 15ms/step - loss: 0.5163 - val_loss: 0.5602
Epoch 45/50
1/5 [=====>........................] - ETA: 0s - loss: 0.5630
Epoch 45: val_loss improved from 0.56020 to 0.55945, saving model to /content/045.h5
5/5 [==============================] - 0s 15ms/step - loss: 0.5147 - val_loss: 0.5594
Epoch 46/50
1/5 [=====>........................] - ETA: 0s - loss: 0.4806
Epoch 46: val_loss did not improve from 0.55945
5/5 [==============================] - 0s 9ms/step - loss: 0.5132 - val_loss: 0.5598
Epoch 47/50
1/5 [=====>........................] - ETA: 0s - loss: 0.5074
Epoch 47: val_loss did not improve from 0.55945
5/5 [==============================] - 0s 9ms/step - loss: 0.5127 - val_loss: 0.5602
Epoch 48/50
1/5 [=====>........................] - ETA: 0s - loss: 0.4882
Epoch 48: val_loss improved from 0.55945 to 0.55903, saving model to /content/048.h5
5/5 [==============================] - 0s 17ms/step - loss: 0.5105 - val_loss: 0.5590
Epoch 49/50
1/5 [=====>........................] - ETA: 0s - loss: 0.5612
Epoch 49: val_loss improved from 0.55903 to 0.55896, saving model to /content/049.h5
5/5 [==============================] - 0s 15ms/step - loss: 0.5092 - val_loss: 0.5590
Epoch 50/50
1/5 [=====>........................] - ETA: 0s - loss: 0.4864
Epoch 50: val_loss did not improve from 0.55896
5/5 [==============================] - 0s 9ms/step - loss: 0.5072 - val_loss: 0.5597
```

```python
# 학습결과 그래프
dl_history_plot(hist)
```

![z_dl_7_7](https://github.com/zacinthepark/TIL/assets/86648892/bae06b79-4836-4957-8c6b-dc60d63e1006)

```python
directory_to_delete_from = "/content/"
delete_h5_files(directory_to_delete_from)
```

<pre>
Deleted: /content/035.h5
Deleted: /content/045.h5
Deleted: /content/037.h5
Deleted: /content/029.h5
Deleted: /content/025.h5
Deleted: /content/012.h5
Deleted: /content/021.h5
Deleted: /content/048.h5
Deleted: /content/027.h5
Deleted: /content/030.h5
Deleted: /content/001.h5
Deleted: /content/034.h5
Deleted: /content/028.h5
Deleted: /content/049.h5
Deleted: /content/019.h5
Deleted: /content/023.h5
Deleted: /content/010.h5
Deleted: /content/017.h5
Deleted: /content/018.h5
Deleted: /content/031.h5
Deleted: /content/016.h5
Deleted: /content/013.h5
Deleted: /content/038.h5
Deleted: /content/015.h5
Deleted: /content/011.h5
Deleted: /content/041.h5
Deleted: /content/024.h5
Deleted: /content/044.h5
Deleted: /content/026.h5
Deleted: /content/032.h5
Deleted: /content/020.h5
Deleted: /content/022.h5
</pre>
