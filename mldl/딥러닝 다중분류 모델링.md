## 딥러닝 다중분류 모델링

---

> 1. 전처리<br>
x: 가변수화, 스케일링
y: Label Encoding, One-hot Encoding

> 2. 모델링<br>
은닉층: `activation='relu'`<br>
출력층: `activation='softmax'` (0~1 확률값으로 변환), Node 개수: `y의 class 개수`<br>
컴파일: Loss Function으로 정수인코딩한 경우 `sparse_categorical_crossentropy`, 원핫인코딩한 경우 `categorical_crossentropy`

> 3. 검증<br>
`np.argmax(pred, axis=1)`: 클래스 별 확률에 대해서 가장 높은 확률의 클래스 값으로 변환<br>
`np.argmax(y_val, axis=1)`

### Softmax

- 각 Class별 Output Node로 예측한 값을, 하나의 확률값으로 변환

<p align="center">
    <img width="600" alt="softmax" src="https://github.com/zacinthepark/TIL/assets/86648892/c86e271c-a639-4f9d-bf9f-2f96c2959f74">
</p>

- 특성 1: 출력 벡터가 확률 분포를 형성한다
    - 각 요소는 해당 인덱스의 클래스에 대한 확률을 나타내며, 이 모든 확률의 합은 `1`
    - 이 특성으로 인해 **다중 클래스 분류** 문제에 유용
    - softmax에 의해 계산된 출력이 `[0.1, 0.2, 0.7]` 이라면, 첫번째 클래스에 속할 확률 10%, 두번째 클래스 20%, 세번째 클래스 70%임을 나타냄
    - 각 클래스에 대한 예측 확률을 제공함으로써, 모델의 예측이 얼마나 확신할 수 있는지를 수치로 표현

- 특성 2: 입력값 중 하나가 매우 큰 경우, 해당 요소의 확률이 1에 가까워지고, 나머지 요소 확률은 0에 근접함
    - 모델이 특정 클래스에 대해 매우 확신하는 경우

$$\Large
\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{K} e^{x_j}}
$$

- $e$: 자연상수(2.71828...)
- $x_i$: 입력 벡터 $x$의 $i$번째 요소
- $K$: 입력 벡터 $x$의 개수 (클래스의 총 수)
- $e^{x_i}$: i번째 요소의 지수 함수 값

> Why $e^{x_i}$?

> 1. 미분 가능성<br>
$e^{x_i}$ 함수는 전 구간에서 미분 가능하며, 그 미분 값은 $e^{x_i}$ 자신입니다. 이는 신경망에서 역전파 알고리즘을 사용할 때 매우 중요한 특성입니다.<br>
역전파는 신경망의 가중치를 조정하기 위해 손실함수의 기울기(미분)를 계산하는 과정인데, $e^{x_i}$의 이러한 특성 때문에 기울기 계산이 간단하고 효율적으로 이루어질 수 있습니다.

> 2. 계산의 안정성<br>
소프트맥스 함수에서 지수함수를 사용하면, 매우 큰 음수 입력값에 대해서도 안정적으로 계산할 수 있습니다. 입력값이 매우 큰 음수일 경우, $e^{x_i}$는 0에 가까워지지만 실제로 0이 되지는 않습니다.<br>
이는 소프트맥스 함수의 분모가 0이 되어 계산을 수행할 수 없는 상황을 방지합니다.

> 3. 확장성과 일반성<br>
지수함수는 다양한 수치 범위의 입력을 적절히 처리할 수 있으며, 소프트맥스 함수의 출력을 확률로 해석하기 적합한 범위인 [0, 1]로 변환합니다.<br>
이는 모델의 출력을 확률로 해석하는 데 필수적인 특성입니다.

> 4. 지수함수 성질을 활용한 정규화<br>
$e^{x_i}$의 지수적 성장은 소프트맥스 함수가 입력 벡터의 최대값에 민감하게 반응하도록 만듭니다.<br>
즉, 입력 벡터의 요소 중 가장 큰 값이 상대적으로 더 큰 비중을 차지하게 되어, 분류 결정 경계가 더 명확해집니다. 이는 분류 문제에서 확실성을 높이는 데 도움이 됩니다.

### Label Encoding + `sparse_categorical_crossentropy`

#### Label Encoding

```python
from sklearn preprocessing import LabelEncoder

# 선언
int_encoder = LabelEncoder()

# 인코딩
data['Species_encoded'] = int_encoder.fit_transform(data['Species'])
data.head()

# 인코딩된 범주 조회 (classes_: 배열의 인덱스가 인코딩된 정수)
int_encoder.classes_
```

#### sparse_categorical_crossentropy

- `loss='sparse_categorical_crossentropy'`
- y는 인덱스로 사용됨
    - 해당 인덱스의 예측확률로 계산
    - Loss = $-log{\hat{y}}$
- 1에 가까운 값이라면 (해당 범주일 확률이 1에 가까우면), 오차는 낮을 것

<p align="center">
    <img width="600" alt="sparse_categorical_crossentropy" src="https://github.com/zacinthepark/TIL/assets/86648892/666094d3-0c84-49b7-a95f-1dab6cb418ad">
</p>

### One-Hot Encoding + `categorical_crossentropy`

#### One-Hot Encoding

> sklearn<br>
2차원 구조로 입력해야함!

```python
from sklearn.preprocessing import OneHotEncoder

# OneHotEncoder 선언
oh_encoder = OneHotEncoder()

# 데이터를 원핫인코딩하여 변환(input: 2차원)
encoded_y1 = oh_encoder.fit_transform(data[['Species']])

# 변환된 데이터 확인
print(encoded_y1.toarray())
```

> keras<br>
정수 인코딩이 선행되어야함!

```python
from keras.utils import to_categorical

# 이미 정수 인코딩된 y를 이용하여 적용
encoded_y2 = to_categorical(data['Species_encoded'], 3)
print(encoded_y2)
```

#### categorical_crossentropy

- Loss = $-\sum y * log{\hat{y}}$
- 1에 가까운 값이라면 (해당 범주일 확률이 1에 가까우면), 오차는 낮을 것
- `sparse_categorical_crossentropy`와 `categorical_crossentropy` 두 loss function은 수학적으로 동일함

<p align="center">
    <img width="600" alt="categorical_crossentropy" src="https://github.com/zacinthepark/TIL/assets/86648892/634d434e-1955-4b64-9623-81154338cd38">
</p>

### Code

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.metrics import *
from sklearn.preprocessing import MinMaxScaler

from keras.models import Sequential
from keras.layers import Dense
from keras.backend import clear_session
from keras.optimizers import Adam
```

```python
# 학습곡선 함수
def dl_history_plot(history):
    plt.figure(figsize=(10, 6))
    plt.plot(history['loss'], label='train_err', marker='.')
    plt.plot(history['val_loss'], label='val_err', marker='.')

    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend()
    plt.grid()
    plt.show()
```

```python
path = "https://raw.githubusercontent.com/DA4BAM/dataset/master/iris.csv"
data = pd.read_csv(path)
data.head()
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sepal.Length</th>
      <th>Sepal.Width</th>
      <th>Petal.Length</th>
      <th>Petal.Width</th>
      <th>Species</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
  </tbody>
</table>

#### 정수 인코딩

```python
data['Species'] = data['Species'].map({'setosa': 0, 'versicolor': 1, 'virginica': 2})
data.head()
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sepal.Length</th>
      <th>Sepal.Width</th>
      <th>Petal.Length</th>
      <th>Petal.Width</th>
      <th>Species</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

```python
target = 'Species'
x = data.drop(target, axis=1)
y = data.loc[:, target]
```

```python
x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=.3, random_state=20)
```

```python
scaler = MinMaxScaler()
x_train = scaler.fit_transform(x_train)
x_val = scaler.transform(x_val)
```

```python
nfeatures = x_train.shape[1] # num of columns
nfeatures
```

<pre>
4
</pre>

#### 모델링 1

```python
clear_session()
# Output Layer의 수는 클래스의 수
model = Sequential( Dense(3 ,input_shape=(nfeatures,), activation='softmax'))
model.summary()
```

<pre>
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 3)                 15        
                                                                 
=================================================================
Total params: 15 (60.00 Byte)
Trainable params: 15 (60.00 Byte)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</pre>

```python
# Label Encdoing했으므로 sparse_categorical_crossentropy 사용
model.compile(optimizer=Adam(learning_rate=0.1), loss='sparse_categorical_crossentropy')
history = model.fit(x_train, y_train, epochs=50, validation_split=0.2, verbose=0).history
```

```python
dl_history_plot(history)
```

![z_dl_6_1_1](https://github.com/zacinthepark/TIL/assets/86648892/86620098-d6ff-4ab2-88d8-9b8e47efa400)

#### 예측 결과는 softmax로 변환된 값

```python
print(y_val[:5])
```

<pre>
47     0
73     1
74     1
129    2
67     1
Name: Species, dtype: int64
</pre>

```python
pred = model.predict(x_val)
# 범주 0, 1, 2에 해당하는 확률값
print(pred[:5])
```

<pre>
2/2 [==============================] - 0s 12ms/step
[[9.7998595e-01 1.9963343e-02 5.0691204e-05]
 [1.2793014e-02 8.1239522e-01 1.7481172e-01]
 [1.5614334e-02 7.4032456e-01 2.4406102e-01]
 [5.4496684e-04 3.4799841e-01 6.5145665e-01]
 [3.4999363e-02 8.9300138e-01 7.1999408e-02]]
</pre>

##### 행 별로 제일 큰 값을 찾아서 그에 맞게 숫자(0, 1, 2)로 변환

```python
# 5개 행만 살펴보면
np.argmax(pred[:5], axis=1)
```

<pre>
array([0, 1, 1, 2, 1])
</pre>

```python
# 전체에 적용해서 변환
y_pred = pred.argmax(axis=1)
y_pred
```

<pre>
array([0, 1, 1, 2, 1, 1, 2, 0, 2, 0, 2, 1, 1, 0, 0, 2, 0, 1, 2, 1, 1, 2,
       2, 0, 1, 1, 1, 0, 2, 1, 1, 1, 0, 0, 0, 1, 2, 0, 1, 2, 1, 1, 0, 1,
       2])
</pre>

##### 실제값 y_val은 0, 1, 2로 된 1차원 값

```python
print(confusion_matrix(y_val, y_pred))
print(classification_report(y_val, y_pred))
```

<pre>
[[13  0  0]
 [ 0 16  2]
 [ 0  4 10]]
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        13
           1       0.80      0.89      0.84        18
           2       0.83      0.71      0.77        14

    accuracy                           0.87        45
   macro avg       0.88      0.87      0.87        45
weighted avg       0.87      0.87      0.87        45
</pre>

#### 모델링 2

```python
clear_session()
model = Sequential([Dense(8, input_shape=(nfeatures,), activation='relu'),
                    Dense(3, activation='softmax') ])
model.summary()
```

<pre>
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 8)                 40        
                                                                 
 dense_1 (Dense)             (None, 3)                 27        
                                                                 
=================================================================
Total params: 67 (268.00 Byte)
Trainable params: 67 (268.00 Byte)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</pre>

```python
model.compile(optimizer=Adam(learning_rate=0.01), loss='sparse_categorical_crossentropy')
history = model.fit(x_train, y_train, epochs=50, validation_split=0.2, verbose=0).history
```

```python
dl_history_plot(history)
```

![z_dl_6_1_2](https://github.com/zacinthepark/TIL/assets/86648892/0570319d-6a76-4351-9e11-71cef22d3b11)

```python
pred = model.predict(x_val)
y_pred = pred.argmax(axis=1)
```

<pre>
2/2 [==============================] - 0s 9ms/step
</pre>

```python
print(confusion_matrix(y_val, y_pred))
print(classification_report(y_val, y_pred))
```

<pre>
[[13  0  0]
 [ 0 12  6]
 [ 0  3 11]]
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        13
           1       0.80      0.67      0.73        18
           2       0.65      0.79      0.71        14

    accuracy                           0.80        45
   macro avg       0.82      0.82      0.81        45
weighted avg       0.81      0.80      0.80        45
</pre>

#### 모델링 3

```python
clear_session()
model = Sequential([Dense(8, input_shape=(nfeatures,), activation='relu'),
                    Dense(8, activation='relu'),
                    Dense(3, activation='softmax')])
model.summary()
```

<pre>
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 8)                 40        
                                                                 
 dense_1 (Dense)             (None, 8)                 72        
                                                                 
 dense_2 (Dense)             (None, 3)                 27        
                                                                 
=================================================================
Total params: 139 (556.00 Byte)
Trainable params: 139 (556.00 Byte)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</pre>

```python
model.compile(optimizer=Adam(learning_rate=0.01), loss='sparse_categorical_crossentropy')
hist = model.fit(x_train, y_train, epochs=50, validation_split=.2, verbose=0).history
```

```python
dl_history_plot(hist)
```

![z_dl_6_1_3](https://github.com/zacinthepark/TIL/assets/86648892/69a12d85-ceef-4f29-b7f1-c3a1bcc36b1e)

```python
pred = model.predict(x_val)
y_pred = pred.argmax(axis=1)
```

<pre>
2/2 [==============================] - 0s 10ms/step
</pre>

```python
print(confusion_matrix(y_val, y_pred))
print(classification_report(y_val, y_pred))
```

<pre>
[[13  0  0]
 [ 0 17  1]
 [ 0  1 13]]
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        13
           1       0.94      0.94      0.94        18
           2       0.93      0.93      0.93        14

    accuracy                           0.96        45
   macro avg       0.96      0.96      0.96        45
weighted avg       0.96      0.96      0.96        45
</pre>

#### One-Hot Encoding을 활용한 모델링

```python
from keras.utils import to_categorical
```

```python
print(y)
```

<pre>
0      0
1      0
2      0
3      0
4      0
      ..
145    2
146    2
147    2
148    2
149    2
Name: Species, Length: 150, dtype: int64
</pre>

```python
# 클래수의 수를 3으로 지정
y_c = to_categorical(y.values, 3)
```

```python
y_c[:5]
```

<pre>
array([[1., 0., 0.],
       [1., 0., 0.],
       [1., 0., 0.],
       [1., 0., 0.],
       [1., 0., 0.]], dtype=float32)
</pre>

```python
x_train, x_val, y_train, y_val = train_test_split(x, y_c, test_size=.3, random_state=2022)
```

```python
display(x_train.head(3))
print('=' * 30)
print(y_train[:3])
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sepal.Length</th>
      <th>Sepal.Width</th>
      <th>Petal.Length</th>
      <th>Petal.Width</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>136</th>
      <td>6.3</td>
      <td>3.4</td>
      <td>5.6</td>
      <td>2.4</td>
    </tr>
    <tr>
      <th>102</th>
      <td>7.1</td>
      <td>3.0</td>
      <td>5.9</td>
      <td>2.1</td>
    </tr>
    <tr>
      <th>120</th>
      <td>6.9</td>
      <td>3.2</td>
      <td>5.7</td>
      <td>2.3</td>
    </tr>
  </tbody>
</table>

<pre>
==============================
[[0. 0. 1.]
 [0. 0. 1.]
 [0. 0. 1.]]
</pre>

```python
print(y_train.shape)
```

<pre>
(105, 3)
</pre>

```python
scaler = MinMaxScaler()
x_train = scaler.fit_transform(x_train)
x_val = scaler.transform(x_val)
```

```python
nfeatures = x_train.shape[1] # num of columns
nfeatures
```

<pre>
4
</pre>

```python
clear_session()
model = Sequential([Dense(3, input_shape=(nfeatures,), activation='softmax')])
model.summary()
```

<pre>
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 3)                 15        
                                                                 
=================================================================
Total params: 15 (60.00 Byte)
Trainable params: 15 (60.00 Byte)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</pre>

```python
model.compile(optimizer=Adam(learning_rate=0.1), loss='categorical_crossentropy')
history = model.fit(x_train, y_train, epochs=100, validation_split=0.2, verbose=0).history
```

```python
dl_history_plot(history)
```

![z_dl_6_1_4](https://github.com/zacinthepark/TIL/assets/86648892/3d5e3b16-fcf6-4fa1-820c-cb6d0d28210d)

```python
pred = model.predict(x_val)
print(pred[:5])
```

<pre>
2/2 [==============================] - 0s 7ms/step
[[5.9911918e-06 4.9876645e-02 9.5011741e-01]
 [4.7815865e-07 5.7570901e-02 9.4242853e-01]
 [9.7394252e-01 2.6056666e-02 8.0092997e-07]
 [1.9984815e-04 3.2513332e-01 6.7466676e-01]
 [9.8704708e-01 1.2952729e-02 1.9477763e-07]]
</pre>

```python
# 5개 행만 살펴보면
np.argmax(pred[:5], axis=1)
```

<pre>
array([2, 2, 0, 2, 0])
</pre>

```python
y_val
```

<pre>
array([[0., 0., 1.],
       [0., 0., 1.],
       [1., 0., 0.],
       [0., 0., 1.],
       [1., 0., 0.],
       [1., 0., 0.],
       [0., 1., 0.],
       [0., 1., 0.],
       [1., 0., 0.],
       [0., 1., 0.],
       [0., 1., 0.],
       [0., 0., 1.],
       [0., 1., 0.],
       [0., 0., 1.],
       [0., 0., 1.],
       [1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.],
       [0., 0., 1.],
       [0., 1., 0.],
       [1., 0., 0.],
       [1., 0., 0.],
       [0., 0., 1.],
       [1., 0., 0.],
       [0., 0., 1.],
       [0., 0., 1.],
       [0., 0., 1.],
       [1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.],
       [1., 0., 0.],
       [0., 1., 0.],
       [0., 1., 0.],
       [1., 0., 0.],
       [0., 1., 0.],
       [1., 0., 0.],
       [1., 0., 0.],
       [0., 1., 0.],
       [1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.],
       [0., 1., 0.],
       [0., 1., 0.],
       [0., 0., 1.],
       [0., 0., 1.]], dtype=float32)
</pre>

```python
# 전체에 적용해서 변환
y_pred = pred.argmax(axis=1)
y_pred
```

<pre>
array([2, 2, 0, 2, 0, 0, 1, 1, 0, 1, 1, 2, 1, 2, 2, 0, 1, 1, 2, 1, 0, 0,
       2, 0, 2, 1, 2, 0, 2, 2, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 2, 1, 1, 2,
       2])
</pre>

##### 실제값 y_val도 원래 대로 돌려놓기

```python
# y_val도 one-hot encoding된 상태
y_val[:5]
```

<pre>
array([[0., 0., 1.],
       [0., 0., 1.],
       [1., 0., 0.],
       [0., 0., 1.],
       [1., 0., 0.]], dtype=float32)
</pre>

```python
y_val_1 = y_val.argmax(axis=1)
y_val_1
```

<pre>
array([2, 2, 0, 2, 0, 0, 1, 1, 0, 1, 1, 2, 1, 2, 2, 0, 1, 2, 2, 1, 0, 0,
       2, 0, 2, 2, 2, 0, 1, 2, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 2, 1, 1, 2,
       2])
</pre>

```python
print(confusion_matrix(y_val_1, y_pred))
print(classification_report(y_val_1, y_pred))
```

<pre>
[[14  0  0]
 [ 0 14  1]
 [ 0  2 14]]
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        14
           1       0.88      0.93      0.90        15
           2       0.93      0.88      0.90        16

    accuracy                           0.93        45
   macro avg       0.94      0.94      0.94        45
weighted avg       0.93      0.93      0.93        45
</pre>
