## 경사하강법과 가중치 조정

---

### Gradient Descent

<p align="center">
    <img width="700" alt="gradient_descent" src="https://github.com/zacinthepark/TIL/assets/86648892/d85ae39a-7aab-4cdb-9bf3-7bea98937ff1">
</p>

- 딥러닝 알고리즘 학습 시 사용되는 최적화(Optimizer) 방법 중 하나이다

- 딥러닝 알고리즘 학습 시 목표는 예측값과 정답값 간의 차이인 손실 함수의 크기를 최소화시키는 파라미터(가중치 $w$)를 찾는 것

- 학습 데이터 입력을 변경할 수 없기에 손실 함수 값의 변화에 따라 가중치(weight) 또는 편향(bias)을 업데이트시킴

- 가로축은 가중치, 세로축은 손실함수를 의미

- 다음과 과정을 거친다

    - 1: 임의의 파라미터(가중치 $w$)를 정한다

    - 2: 이 가중치에 대한 손실값을 구하고 손실 함수(Loss Function)의 기울기(Gradient)를 구한다

    - 3: 경사하강법(Gradient Descent)을 이용해 파라미터를 업데이트한다
        - 해당 경사의 반대 방향으로 계속 이동시켜 극값에 이를 때까지 반복시키는 것
        - 이동해야하는 방향으로 learning_rate만큼 이동

    - 4: 업데이트된 지점에서 새 손실 함수의 기울기를 구한다

    - 5: 3번 다시 실시

    - 6: 파라미터가 최적값에 도달하면 파라미터 업데이트를 중지한다

### 가중치 업데이트 방식

- Gradient: 기울기(벡터)

- Gradient Descent

    - $w$의 초기값 지정: $w_0$

    - 초기값에서의 기울기(방향) 확인: $\frac{df(w)}{dw}$, $w = w_0$
        - $\frac{df(w)}{dw}$
            - 손실함수의 미분
            - 가중치 w의 변화량과 그 때 f(w)의 변화량의 비율 (f(w) 변화는 w 변화의 몇 배?)
        - 기울기가 `-` 이면 $x$는 오른쪽 (`+` 방향)
        - 기울기가 `+` 이면 $x$는 왼쪽 (`-` 방향)

    - 조금 조정: $\eta * \frac{df(w)}{dw}$
        - $\eta$: eta, 조정하는 비율, Learning Rate

    - **각 Batch마다(Iteration) 가중치를 조정**

- 가중치 업데이트 방식은 아래와 같음

<p align="center">
    <img width="400" alt="weight_update" src="https://github.com/zacinthepark/TIL/assets/86648892/b75919c1-4531-43e0-8b38-d692fcc508f9">
</p>

$$\Large
w_{\text{new}} = w_0 - \eta * \frac{df(w)}{dw}
$$

- 어떤 노드 $z$를 생성하는데 $w_1x_1, w_2x_2, w_3x_3, w_4x_4, ...$와 같은 가중치들이 설정되었다면

- 각 가중치에 대해서
    - 손실함수를 미분한 값을 learning_rate만큼($\eta * \frac{df(w)}{dw}$) 가중치를 조정
    - 각 batch마다 시작 가중치로 나온 결과를 통해 오차를 계산하고(forward propagation), 이를 바탕으로 다시 가중치를 조정(back propagation)

### **역전파와 가중치 조절**

#### 순전파에서의 오차 계산 과정

하나의 레이어를 가정

1. 입력 $x$에 대한 **가중치** $w$의 곱을 계산: $z = wx$

2. 계산된 $z$를 **활성화 함수** 에 적용하여 출력 $y$를 얻음: $y = \sigma(z)$

3. 출력 $y$와 목표 출력 $y_\text{target}$ 사이의 손실 $L$을 계산

#### 역전파

> 손실에 가중치가 얼마나 기여했는가?

**가중치 $w$의 변화에 따른 손실함수의 변화, 즉 오차의 변화율 $\frac{dL}{dw}$를 구한다는 것은, 위의 1번, 2번, 3번 과정을 역으로 고려한 정보들을 종합하여 구하는 것이다**

$$
\frac{dL}{dw} = \frac{dL}{dy} * \frac{dy}{dz} * \frac{dz}{dw}
$$

- (1) $\frac{dL}{dy}$: 해당 출력값일 때 손실, 혹은 오차의 변화율이 어느정도인지 (해당 오차가 얼마나 조정이 필요한 정도인지)

- (2) $\frac{dy}{dz}$: 계산값에 따라 출력값이 얼마나 변했는지 (활성화 함수가 값 변화에 준 영향)

- (3) $\frac{dz}{dw}$: 가중치에 따라 계산값이 얼마나 변했는지

- 결국 순전파 시에 오차가 가중치 계산 &rarr; 활성화 함수 &rarr; 출력값을 통한 오차 계산의 과정을 거쳤듯이

- 역전파 시에는 (1) &rarr; (2) &rarr; (3)의 과정을 통해 결국 해당 가중치가 손실의 변화율에 대해 얼마나 영향을 미쳤는지 구하고, 이를 통해 가중치 조정 정도를 정하는 것

- 같은 과정을 역으로 레이어들을 거치며 수행

#### ReLU와 Sigmoid

> (2) $\frac{dy}{dz}$: 계산값에 따라 출력값이 얼마나 변했는지 (활성화 함수가 값 변화에 준 영향)

- **이처럼 활성화 함수의 도함수는 가중치에 따른 손실함수 기울기 계산에 영향을 끼침**

- sigmoid $\sigma(x)$를 활성화 함수로 사용할 경우 입력값 $x$가 크거나 작을 때 그 기울기가 0에 가까워, 역전파 시에 기울기 소실 문제가 발생함

- $ReLU(x) = max(0, x)$
    - $x > 0$ 일 때, 도함수는 1
    - $x \leq 0$ 일 때, 도함수는 0
    - 역전파 시 양수 입력에 대한 기울기가 네트워크를 통해 깊이 전파될 때 소실되지 않고 유지됨
    - 이는 ReLU 뒤의 레이어에서 계산된 오차 신호가, 네트워크를 거슬러 올라갈 때 입력값이 양수인 노드에 대해서는 변하지 않고 전파됨을 의미
    - 음수 입력을 받는 뉴런은 활성화되지 않으며(도함수가 0), 이로 인해 '죽은 ReLU' 문제가 발생할 수 있음
        - 이를 해결하기 위해 Leaky ReLU와 같은 변형된 ReLU 함수들이 제안되었음

---

### 경사 하강법 종류

> 경사 하강법 종류에는 3가지가 있다: 배치 경사 하강법, 확률적 경사 하강법, 미니 배치 경사 하강법

#### 1. 배치 경사 하강법

배치 경사 하강법(Batch Gradient Descent)은 가장 기본적인 경사 하강법으로 Vanilla Gradient Descent라고 부르기도 합니다. 배치 경사 하강법은 데이터셋 전체를 고려하여 손실함수를 계산합니다. 배치 경사 하강법은 한 번의 Epoch에 모든 파라미터 업데이트를 단 한 번만 수행합니다. 즉, Batch의 개수와 Iteration은 `1`이고 Batch size는 전체 데이터의 개수입니다. 파라미터 업데이트할 때 한 번에 전체 데이터셋을 고려하기 때문에 모델 학습 시 많은 시간과 메모리가 필요하다는 단점이 있습니다.

#### 2. 확률적 경사 하강법

확률적 경사 하강법(Stochastic Gradient Descent)은 배치 경사 하강법이 모델 학습 시 많은 시간과 메모리가 필요하다는 단점을 개선하기 위해 제안된 기법입니다. 확률적 경사 하강법은 Batch size를 `1`로 설정하여 파라미터를 업데이트하기 때문에 배치 경사 하강법보다 훨씬 빠르고 적은 메모리로 학습이 진행됩니다.

<p align="center">
    <img width="400" alt="stochastic_batch_gradient_descent" src="https://github.com/zacinthepark/TIL/assets/86648892/96d6488d-94dd-4279-a5fe-6fa26a74aaa1">
</p>

위의 그림은 경사 하강법 종류에 따라 최적의 해를 찾아가는 과정을 시각화한 자료입니다. 좌측은 확률적 경사 하강법을, 우측은 배치 경사 하강법을 활용한 경우입니다. 확률적 경사 하강법은 파라미터 값의 업데이트 폭이 불안정하기 때문에 배치 경사 하강법보다 정확도가 낮은 경우가 생길 수도 있습니다. 그럼에도 불구하고, 하나의 데이터(Batch size=`1`)에 대해서만 손실함수를 계산하고 파라미터를 업데이트하면 되기 때문에, 적은 시간과 메모리로도 모델을 학습시킬 수 있다는 장점이 있습니다.

#### 3. 미니 배치 경사 하강법

미니 배치 경사 하강법(Mini-Batch Gradient Descent)은 Batch size가 `1`도 전체 데이터 개수도 아닌 경우를 말합니다. 미니 배치 경사 하강법은 배치 경사 하강법보다 모델 학습 속도가 빠르고, 확률적 경사 하강법보다 안정적인 장점이 있습니다. 덕분에, 딥러닝 분야에서 가장 많이 활용하는 경사 하강법입니다. 그럼 Batch size는 어떻게 정하면 좋을까요? 일반적으로 `32`, `64`, `128`과 같이 2의 $n$제곱에 해당하는 값으로 사용하는게 보편적입니다.

### 경사하강법 관련 문제

- Vanishing Gradient
- Local Minima Problem

#### Vanishing Gradient

<p align="center">
  <img width="700" alt="image" src="https://github.com/zacinthepark/TIL/assets/86648892/6c146ae0-7724-4019-8a45-f42e1b28b84b">
</p>
<p align="center">
  <img width="400" alt="image" src="https://github.com/zacinthepark/TIL/assets/86648892/678fe79b-bc43-4130-8822-f018f29a173a">
</p>

- 기울기 소실

    - 네트워크의 깊은 부분으로 갈수록 기울기가 점점 작아져서, 가중치가 거의 또는 전혀 업데이트되지 않게 되는 현상

    - 초기에는 `activation='sigmoid'`를 사용 &rarr; 특히 기울기 소실 문제가 심각

- 기울기 소실 문제를 최소화하기 위한 노력

    - 활성화 함수 조정
        - ReLU(Rectified Linear Unit): 음수 입력은 0 출력, 양수 입력은 그대로 출력 &rarr; 양의 기울기를 유지하여 기울기 소실 문제를 완화
        - ReLU의 변형된 활성화 함수: Leaky ReLU, PReLU, ELU &rarr; 음수 입력에 대해서도 매우 작은 기울기를 허용

    - 그 외 방법들
        - 가중치 초기화 기법
        - 배치 정규화(Batch Normalization)
        - Residual Connections
        - Gradient Clipping

#### Local Minima Problem

<p align="center">
    <img width="600" alt="local_minima_problem" src="https://github.com/zacinthepark/TIL/assets/86648892/c24fd57f-ce9b-44dd-a3fb-dd53d83c23b4">
</p>

Local Minima Problem &rarr; Optimizer

<p align="center">
    <img width="600" alt="local_minima_problem_optimizer" src="https://github.com/zacinthepark/TIL/assets/86648892/c51c7810-276b-445d-aee6-ee0b997a4a62">
</p>
