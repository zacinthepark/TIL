## 성능 최적화

---

> Hidden Layer 수, Node 수, learning_rate, epoch 수, 과적합 방지

- 수동으로 최적의 Hidden Layer, Node 수 탐색 (권장)
    - elbow method: 성능 탐색에 있어 팔꿈치처럼 꺾이는 근방을 탐색해라
- keras-tuner를 이용하여 최적의 Node 수 탐색 가능
- epoch 수는 early stopping을 하거나, mcp를 통해 해결
- 과적합 방지를 위해 가중치 규제를 진행할 수도 있음

### Early Stopping

- 과적합 방지는 검증 성능 최적화와 일맥상통한다
- `monitor='val_loss'`는 관심사가 validation error라는 것
- `min_delta` 값은 현재 모니터링 중인 오차의 최소값에서 이 값보다 커야 줄어야 인정해주는지 지정
- `patience`는 오차의 최소값에서 `min_delta`보다 줄어들지 않는 것을 몇 번 기다려줄 것인지 지정하는 것이며, 한 번 줄어들면 patience의 카운트는 초기화됨

### 가중치 규제

- 파라미터의 수가 많아질수록 딥러닝 모델의 복잡도는 올라간다
- 파라미터에 규제를 가하여 줄여보겠다는 것이 가중치 규제
- 이처럼 파라미터에 규제를 가하여 과적합을 방지하고 모델의 복잡도를 낮추려는 시도
- 간단히 말해 파라미터를 정리하는 것
- 강도(람다값)가 높을수록 일반화된(단순한) 모델이 됨
- 일반적으로 한 Layer에서 L1이나 L2 규제를 사용했다면, 다른 Layer에서도 동일한 규제 방법 사용
- 모든 층에 규제를 할 수도 있고, 노드가 많은 층게 규제를 할 수도 있고, 여러 실험을 통해 성능 최적화를 진행

### L1 규제: Lasso

### L2 규제: Ridge

### Dropout

- 과적합을 줄이기 위해 규제 기법 중 하나
