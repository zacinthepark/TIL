## Batch Size, Epochs, Iterations

---

> 최적화 과정

![gradient_descent](https://github.com/zacinthepark/TIL/assets/86648892/d85ae39a-7aab-4cdb-9bf3-7bea98937ff1)

기계는 다음과 같은 과정을 통해 학습한다

1. 임의의 파라미터(가중치 $w$)를 정한다

2. 이 가중치에 대한 손실값을 구하고 손실 함수(Loss Function)의 기울기(Gradient)를 구한다

3. 경사하강법(Gradient Descent)을 이용해 파라미터를 업데이트한다
    - 경사하강법은 함수의 최솟값을 찾아야 하는 상황에서 사용
        - 손실값의 최소를 찾고, 이 때의 최적 파라미터를 찾는 것이 목표
    - 해당 경사의 반대 방향으로 계속 이동시켜 극값에 이를 때까지 반복시키는 것
    - 이동해야하는 방향으로 learning_rate만큼 이동
    - 1: 적절한 step size 선정 문제, 2: Local Minima 문제가 발생할 수 있음

4. 업데이트된 지점에서 새 손실 함수의 기울기를 구한다

5. 3번 다시 실시

6. 파라미터가 최적값에 도달하면 파라미터 업데이트를 중지한다

#### 학습 과정에서의 문제

- 보통 기계를 학습시킬 때 사용하는 데이터의 양은 매우 많으며 이렇게 많은 데이터를 모두 한꺼번에 모델에 태우게 되면, 컴퓨터가 버티기 힘들다
- 데이터 크기가 너무 크면 메모리가 부족해진다
- 한 번의 학습에 계산되어야 할 파라미터 수가 지나치게 많아져 학습 속도가 느려진다
- 한 번의 계산으로 최적화된 값을 찾는 것은 힘들다

> 따라서, 머신러닝에서 최적화를 할 시에 일반적으로 여러 번의 학습 과정을 거치며, 한 번의 학습 과정에서 사용하는 데이터의 크기를 세분화시킨다

### Batch Size

![bs_1](https://github.com/zacinthepark/TIL/assets/86648892/47e83c84-9704-49ee-9bfd-b3635a0853f6)

- 사전적 의미: (일괄적으로 처리되는) 집단, 무리
- 전체 데이터셋을 여러 작은 그룹으로 나우었을 때 batch size는 **하나의 소그릅에 속하는 데이터 수** 를 의미한다
- 1000개의 수학문제를 20개씩 나눠서 푸는 것으로 생각 (20이 batch size)
    - 문제를 다 풀고 사람이 채점하고 틀린 문제에 대한 이유 등을 학습하는 것처럼 머신러닝도 Batch 크기만큼 데이터를 활용해 모델의 예측값($\hat{y}$)과 실제 정답($y$) 간의 오차를 계산하여 파라미터를 업데이트한다

- Batch Size가 너무 큰 경우: 한 번에 처리해야할 데이터의 양이 많아져, 학습 속도가 느려지고, 메모리 부족 문제가 발생할 수 있다
- Batch Size가 너무 작은 경우: 적은 데이터로 가중치가 자주 업데이트되어 훈련이 불안정해진다

### Epoch

> 1-epoch: 전체 데이터셋을 1회 활용하여 모델을 학습시킴

![bs_2](https://github.com/zacinthepark/TIL/assets/86648892/5d447b85-c094-4654-9a50-a3a5e406c4d1)

> 2-epoch

![bs_3](https://github.com/zacinthepark/TIL/assets/86648892/576fccae-bc9f-4958-9ead-4e024cb523df)

- 사전적 의미: (중요한 사건, 변화들이 일어난) 시대
- 딥러닝에서 epoch는 전체 데이터셋이 신경망을 통과한 횟수, 즉 **모든 데이터셋을 학습하는 횟수** 를 의미
- 1-epoch는 전체 데이터셋이 하나의 신경망에 적용되어 순전파와 역전파를 통해 신경망을 한 번 통과했다는 것을 의미
- 문제집 한 권 전체를 몇 회 풀었는지를 의미
- Epoch 값이 너무 작으면 Underfitting, 너무 크면 Overfitting이 발생할 확률이 높다

### Iteration

![bs_4](https://github.com/zacinthepark/TIL/assets/86648892/b9a4a4cf-422d-46b5-bf88-82090be0a5f1)

- 사전적 의미: (계산, 컴퓨터 처리 절차의) 반복
- **1-epoch를 마치는데 필요한 미니배치의 수** 를 의미
- 다른 말로, **1-epoch를 마치는데 필요한 파라미터 업데이트 횟수**
- Step이라 부르기도 한다
- 각 Batch마다 한 번씩 파라미터 업데이트가 이루어지니 `파라미터 업데이트 횟수 = Batch의 수`가 된다
- 예: 2000개의 데이터를 200개씩 10개의 미니배치로 나눈다면, 1-epoch를 위해선 10-iteration이 필요하며 10번의 파라미터 업데이트가 진행된다는 것
- 예: 2000개의 데이터에 `batch_size=500`일 시 epoch를 20번으로 설정한다면, iteration은 4번이 될 것이고, 전체 데이터셋에 대해선 총 20번의 학습이 이루어졌으며, iteration 기준으론 80번의 학습이 이루어진 셈이다
