# 통계와 친해지기

> 첫 술에 배부를 순 없다. 친해져가며 익히자.

## SPSS (statistical Package for Social Sciences) 기본 앱 기능

- 기술 통계: 교차표, 빈도, 기술 자료, 탐색, 기술 비용 통계 등
- 이변량 통계: 평균, t-검정, 분산 분석, 상관 관계, 비모수 검정 등
- 수치 결과 예측: 선형 회귀 분석
- 집단 식별 예측: 요인 분석, 클러스터 분석

## 분산과 표준편차에 대해

- 세상의 모든 것을 같은 단위로 잴 수 있는 것
    - 평균을 기준으로 얼마나 큰지 작은지
    - 평균이 4인 어떤 값에 대해 1, 2, 3, 4, 5, 6, 7와 같이 척도를 줄 수 있는 것
    - 180cm는 어느정도로 큰지? 180kg는 어느정도로 무거운지?
    - Dimensionless Quantity

- 평균에서의 이탈도

- 분산은 평균과의 편차를 제곱한 것들의 평균
    - 제곱은 음수 방지용
    - 평균과의 편차 합은 항상 0으로 수학적으로 의미가 없음
    - 표준편차는 제곱의 반대 행동을 한 루트를 적용해 힘을 뺀 값

## 중심극한정리에 대해

- 표본평균의 분포의 평균은 모평균의 평균을 따른다
- 표본평균의 분포는 모분산을 샘플크기로 나눈다
    - 표본평균들은 더 빽빽히 모평균을 중심으로 모여있을 것임
- 표본 크기를 극한으로 근사시킬 때, (그리고 해당 표본들의 평균들을 계속 구해서 얻은) 표본평균의 분포는 중심(모평균)으로 모이게 된다

## 표본평균의 분포와 표준오차에 대해

- 분포는 어떤 변수에 대해서 표집을 했을 때 믿을 수 있는 정도(확률)

- 분포는 한 번만 표집해도 생긴다
    - 예를 들어, 평균 연봉을 알고 싶은데, 샘플링을 한 번만 했다고 하자
    - 이게 믿을만한가? 라는 생각이 드는 순간부터 분포가 있는 것
    - 여기서 더 나아가 샘플링을 더 하다보면 중심극한정리에 의해 중심값에 점점 값들이 모일 것임

- 분포의 장점은 작은 정보(평균, 표준편차)로 많은 정보(어떤 수의 크고 작음)을 알 수 있다는 것

> 우리가 표본을 뽑기 전에 모집단에 대한 가정 (모집단의 평균과 표준편차) 을 이미 하였기 때문에, 중심 극한 정리에 의해 표본의 평균이 정규분포 (평균이 모평균이고 분산이 모분산/표본의개수) 를 따르는 것을 이미 알고 있어서, 한번 추출된 표본의 평균이 앞서 이야기한 표본평균의 분포에서 어디쯤에 위치하는지를 표준오차 (표본평균 분포의 표준편차) 와 비교해 보면, 모집단의 가정이 참이라고 했을 때 얼마나 말이 되는 표본인지를 가늠할 수 있다.

- 표집들의 분포의 표준편차가 표준오차
- 표준오차는 왜 필요할까?
- 우리가 표집을 통해 얻은 값이 얼마나 믿을 수 있는지 알고 싶어서

## 신뢰구간에 대해

- 95% 신뢰구간은 100번
- 데이터가 많을수록, 편차가 적을수록 신뢰구간은 좁아짐

![ci_1](https://github.com/zacinthepark/TIL/assets/86648892/2694796f-f9e2-42f7-af0b-4e7f0005e903)

![ci](https://github.com/zacinthepark/TIL/assets/86648892/e4ce664b-d9a7-4b39-9c06-e09912f45f8a)

- 표본평균은 중심극한정리에 의해 모평균을 중심으로 하는 정규분포를 따른다

- 이때 표준오차(표본평균들의 표준편차)에 의해 해당 표본평균 값(통계량)은 그림과 같이 점이 아닌 잔상을 같는 형태가 될 것
    - 샘플사이즈를 늘려 표준오차를 줄여 오차범위를 줄일 수 있음

- 신뢰구간은 저 잔상 중 어디까지를 인정할까에 대한 것
    - 신뢰수준이 커질수록 더 많은 표집 분포의 더 많은 영역을 인정함
    - 과학적으로 통용되는 신뢰수준이 99%, 95%
    - 95% 신뢰구간은 1.96 * SE 를 통해 구함

- 유의한 차이가 있다는 것은 잔상이 겹치지 않는다는 것

## 표본분산 n-1로 나누는 이유에 대해

- 표본분산: `(편차제곱합) / (n-1)`
- 모분산: `(편차제곱합) / (n)`

- 모든 정보의 공개된 모집단에 비해 일부 정보가 공개된 표본의 편차제곱합은 작게 추정됨
    - 그렇기에 더 작은 값인 (n-1)로 나눠줘서 더 작게 추정되는 편차제곱합을 올려줘야함
    - 그렇다면 왜 표본분산(편차제곱)은 모분산보다 값이 작게 추정되는가?

- 표본평균은 절대 모평균과 같은 값이 아니다
    - 표본평균은 표본값들에 더 가까움
    - (평균-표본값)의 제곱인 분산은 (모평균-표본값)에 비해 (표본평균-표본값)은 작게 계산됨
    - 즉, 모평균이 표본평균보다 크거나 작을 수도 있지만, 분산은 제곱을 사용하여 음수건 양수건 무조건 양수로 바꾸므로 모평균과 표본평균에 차이가 있다는 사실이 무조건 표본분산을 크게 만드는 결과를 가져옴

## 자유도(dof)에 대해

- 계산의 자유도
    - 평균의 자유도, 분산의 자유도, 분포의 자유도, 모델의 자유도 ... 모든 곳에 쓸 수 있는 말
    - 계산식으로 나타낼 수 있는 순간 자유도를 갖는다

- 계산식의 서로 독립적인 정보의 수는 자유도를 올린다
    - 미지수

- 계산식의 추정치는 자유도를 낮춘다
    - 어떤 추정치가 계산식에 들어간다면, 산식의 결과가 특정값으로 수렴되거나 고정됨

- 쉽게 생각한다면 (계산식의 미지수 수) - (계산식의 추정지 수) 로 생각할 수 있다

- 분산 분석의 자유도는?
    - 집단간 분산
        - (전체 평균 - X1의 평균) + (전체 평균 - X2의 평균) + ... (전체 평균 - Xk의 평균)
        - k개의 독립된 정보
        - 전체 평균이라는 1개의 추정치
        - 자유도는 k-1

## 공분산과 상관에 대해

- 분산의 단위는 제곱
    - 평균이 171 센티미터라면 분산은 25 제곱 센티미터
    - 이렇게 단위가 바뀌기에 평균에 비해 분산은 우리가 받아들이기에 직관적이지 않을 수 있다

- 분산, Variance는 곧 변량
    - 변량은 곧 차이, 평균보다 작거나 or 크거나
    - 각 데이터들을 평균을 기점으로 해서 얼마나 달라졌는지의 관점으로 설명할 수 있다

- Covariance
    - 공분산은 곧 공변량
        - covariate을 공변량으로 해석하는데 이에 대해서는 공변수가 더 적절하지 않을까라는 개인적인 의견

    - 편차끼리의 곱<br>
        ![image](https://github.com/zacinthepark/TIL/assets/86648892/5e7b38e7-a185-4757-8f60-98035f031fff)

    - 공변량은 변량이 평균보다 크냐 작냐에 대한 것
        - 하나는 평균 위, 하나는 평균 아래라면 값이 음수
        - 둘 다 평균 위라면 값이 양수
    
    - '곱'이라는 것도 중요
        - 1000 + 0.1 = 1000.01
        - 1000 x 0.1 = 10
        - 둘다 편차가 커야 공분산 값도 커짐
        - 하나 값이 평균과 근사하다면 공분산도 작아질 것
        - 공분산이 크다는 것은 곧 둘다 변량이 크다는 것

- Correlation<br>
    ![cov](https://github.com/zacinthepark/TIL/assets/86648892/cb7a621f-c986-48d3-b07c-d694a2f89dc7)

    - 공분산인데 단위만 맞춰준 것
    - 공분산을 각자의 표준편차로 나눠주어서 -1 ~ 1 범위 값으로 맞춰준 것
    - corr 값이 강하다는 것은 x의 변량과 그 때의 y 변량이 크다는 것이고
    - corr 값이 약하는 것은 x, y 중 적어도 하나의 변량이 미미하다는 것

## t, 카이제곱, f 통계량에 대해

- 일반적으로 통계량이라 한다면 t값, 카이제곱값, f값이 있다 (그 외에도 있긴 하다)

- 값들의 크고 작음이 중요하다
    - 각각의 값들이 context가 있어서 사용되는 상황에 따라, 어떤 기법이냐에 따라 같은 값이더라도 디테일하게 해석되는 방법이 달라짐
    - 대신 어떤 값이 나와야 좋은걸까, 어떤 값을 집중해야 되고, 어떤 값을 무시해야 되는 등 판단
    - 어떤 값이 의미있는 결과인가?
        - 보통 큰 값들에 주목함
    - 값 자체의 해석은 context에 따라 달라지지만, 자세하게 해석하는 방식이 미묘하게 달라지는 것이지 중요한 것은 이 값들이 큰지 작은지
        - t 값이 크다
            - 어떤 분석이냐에 따라
            - 차이가 있다
            - 모델이 의미가 있다
            - 라고 해석할 수 있다

- 값들의 크고 작음은 단위가 있어야 한다
    - 분포를 통해 알 수 있다

- 통계량 -> 분포를 통해 크고 작음 확인 -> 해석 (차이가 있다, 모델이 의미있다)

## t 분포에 대해

![image](https://github.com/zacinthepark/TIL/assets/86648892/8ce8e8aa-77c6-4175-b075-7279f6ceca6c)

- 샘플사이즈가 적은 표본평균들의 분포
- 평균의 차이 분포 (두 그룹 간)
    - 그러므로 그 값을 구했을 때 값이 큰지 작은지 판단 가능

- 여러 범주로 나눴을 때 어떤 값의 평균 차이는 0이 제일 많을 것이고 그 여태가 표준정규분포 형태를 따를 것이라 예상
- 하지만 샘플사이즈가 적을 때 두 그룹을 정했을 때 평균 차이들을 분석해보니 양쪽이 더 두꺼운 분포가 있음을 알게됨
- 샘플의 크기가 작은 경우에 한해서 정규분포보다 t 분포가 훨씬 가깝다는 것을 경험적으로 알게됨
- 현실세계에서 샘플 사이즈가 유한한 경우가 많기에 샘플에 대해서 그 평균을 계산한 것의 분포를 알고 싶다면 그것은 t 분포
- 샘플 사이즈가 크다면 (자유도가 높다면, 더 많은 샘플들을 뽑는다면) 범주 간의 차이가 더 줄어들 것
    - 자유도가 높을수록 표준정규분포 형태에 더 가까움

- 정규분포는 평균, 표준편차만 알게 된다면 수의 크고 작음을 알 수 있음
- t 분포는 여기에 sample size라는 새로운 정보를 알아야 수의 크고 작음을 알 수 있음

### t-test

![t_test_1](https://github.com/zacinthepark/TIL/assets/86648892/b58fa961-1f60-46ef-a118-4e82c9e7fb08)
![t_test_2](https://github.com/zacinthepark/TIL/assets/86648892/1f572187-f488-4c26-bf26-22ca3c0968f4)

- t-test의 자유도는 평균값은 정해져 있어서 n-1

- t-value
    - 두 집단의 평균의 차이를 보기위한 통계량
    - t는 그룹 간 평균 차이에 비례하는 변수
    - `표본평균값의 차이 / 표본평균이 갖는 불확실도`
    - 표본평균값의 차이를 불확실도로 나눠줘서 차이가 유의하게 큰지 확인할 수 있음

1. 그룹 간 평균 차가 클수록 t-value는 크다
2. t-value는 평균 차이를 불확실도로 나눈 것
3. 즉, 평균차가 클수록 t값은 커진다
4. 불확실도가 적을수록 t값은 커진다

- 독립표본 t-test: 두 그룹에 들어있는 사람들이 전혀 다른 사람들이다
- 대응표본 t-test: 두 그룹에 들어있는 사람들이 같은 사람들이다 (비포-애프터 검정)

## 카이제곱 분포에 대해

![image](https://github.com/zacinthepark/TIL/assets/86648892/888c965b-6e59-45f1-a434-1fe812fd793d)

- t 분포를 제곱한 분포
- 표본분산의 분포

- 표준정규분포와 카이스퀘어 분포
    - 어떤 확률변수 x가 표준정규분포(평균0, 분산1)를 따를 때
    - x^2은 (0-x)^2, 분산을 의미
    - 표준정규분포의 분산은 카이스퀘어 분포를 따름
    - 카이스퀘어 분포는 곧 표준정규분포에서 분산의 분포
        - 표준정규분포의 분산의 크고 작음을 이야기해준다
        - 표준정규분포가 좁아지거나 넓어지거나의 확률이 어느정도 되는가
    - 이를 유사한 분포인 t 분포에 적용 가능
        - 표본평균들의 분산 정도를 파악 가능

- 결국 분산에 대한 값의 크고 작음을 판단할 수 있다

- 기대빈도와 관측빈도에 적용한다면
    - 기대빈도는 각 값의 전체평균이라 이해할 수 있음
    - `(기대빈도 - 각 관측값) / (기대빈도)`의 제곱 = `(전체평균 - 각 관측값) / (기대빈도)`의 제곱 = 편차제곱의 합
    - 분산으로 이해할 수 있음
    - 실제 기대빈도로부터 값이 얼마나 차이가 있는가
    - 차이가 있다면 집중할만한 값, 관계

### 비모수 검정에 대해

- 모수가 있으면 단위(분포)를 기준으로 판단
- 비모수 검정은 분포가 유도되지 않을 때, 분포 가정이 만족이 안될 때, 모수의 크고 작음을 판단할 수 없을 때 사용
- 보통 분포가 안 나오는 상황이 샘플 사이즈가 작을 때
- 그래서 샘플 사이즈가 작을 때 비모수 검정을 많이 사용
- 비모수 검정 보통 순위 정보를 사용한다

### 카이스퀘어와 비모수 검정

- 카이스퀘어 검정은 비모수 검정에 해당 (t, f 검정은 모수 검정에 해당)
- 분산이 일반적으로 카이스퀘어를 따르는 건 맞다
- 근데 카이스퀘어 검정은 교차표에서 기대값과 관측값이 얼마나 다른지를 알아보는 검정 방법이고, 이 때 검정에 사용되는 변수들에게 특정 분포 가정을 강요하지 않기 때문에 비모수 검정

1. 모수검정을 쓰려면 변수에 대한 분포가 가정되어야 함
2. 그래야 그 분포로 만들어지 검정통계량이 의미가 있음
3. 그런 맥락에서 분포를 쓸 수 잇는 검정통계량을 모수적 방법이라고 했는데
4. 카이스퀘어처럼 해당 변수의 분포에 대한 특별 가정 없이도 계산된 통계량이 특정 분포을 잘 따르는 경우가 있는데, 그게 바로 교차표 검정할 때 카이스퀘어다

## f 분포에 대해

![image](https://github.com/zacinthepark/TIL/assets/86648892/0a614c65-f1f9-4943-9c70-abfdd8a901d4)

![anova_1](https://github.com/zacinthepark/TIL/assets/86648892/72b183d1-a1c2-4836-8550-c1cc6c43c1e0)
![anova_2](https://github.com/zacinthepark/TIL/assets/86648892/2bc79d39-cd4f-4512-9cf9-4a56314e0087)
![anova_3](https://github.com/zacinthepark/TIL/assets/86648892/a7d8a122-6ae6-4760-8f0b-5e119467f213)
![anova_4](https://github.com/zacinthepark/TIL/assets/86648892/62de3b44-f8d3-435d-8e76-4190fa9e59a4)
![anova_5](https://github.com/zacinthepark/TIL/assets/86648892/baf89db7-345f-42b7-b2a8-4f759d4f86a3)

- 여러 집단 평균의 차이를 보기 위한 통계량
- 모든 그룹의 평균이 동일한지, 최소한 하나의 그룹의 평균은 유의하게 다른지 판별할 수 있음

- 카이스퀘어 분포들의 비율이 F 분포
    - `카이제곱 분포 / 카이제곱 분포`
    - `분산 / 분산`
    - `집단 간 분산` / `집단 내 분산`
        - ANOVA

- ANOVA에서 사용되는 표본분산, 즉 표본평균들의 변화량을 나타낸다
    - F 값이 큰 경우
        - 각 그룹의 평균들이 멀리 떨어진 경우
        - 각 그룹 내 분산이 작은 경우

- 기준값이 1
    - F=1 이라면
    - 집단 간 분산 = 집단 내 분산
    - 집단을 어떤 식으로 나누든 집단 간 분산과 집단 내 분산이 같다는 것
    - 집단 간 분산이 집단 내 분산보다 얼마나 큰지를 통해서 집단 간 차이가 있다는 논리
        - 그러므로 F가 1보다 작은 경우는 거의 논하지 않음
    - F>1 이라면
        - 집단 간 분산이 집단 내 분산보다 몇 배 큰가?
        - 집단 간 차이가 있는가?

### ANOVA (분산분석)

- Analysis Of Variance
    - 분산을 이용한 분석

- 집단이라고 볼 수 있는가? 집단 간 차이가 큰가?

- 분자
    - 집단 간 분석
    - 세 그룹이 있을 때 표본 평균들의 차이를 어떻게 계산할 수 있을까?
        - 세 그룹의 평균의 평균으로부터 떨어진 정도를 이용하는 것이 아이디어
        - 즉, **표본 평균 간 분산** 을 이용해 여러 그룹의 차이를 표현
- 분모
    - 집단 내 분석
    - 비교하고 있는 집단들이 집단이라고 할 수 있는가?
    - 분자에 분산을 넣었으니 분모도 분산을 이용해서 불확실도를 표현
    - 불확실도는 결국 각 그룹의 데이터들이 평균적으로 퍼진 정도를 의미
    - 즉, **표본 그룹 내의 분산** 을 이용해 여러 그룹의 평균적 불확실도 표현

- 변화량을 불확실도로 나눠줘서 변화량이 유의하게 큰지 확인할 수 있음

1. 등분산성(Homosedasticity) 가정 검정

- 집단 간 집단 내 분산이 서로 비슷한가?
    - 집단 내 분산들이 다르다?
        - 동등하게 비교하기 어렵다
 - 가정의 경우 영가설이 채택되어야함
    - H0: 모든 집단의 집단 내 분산이 같다
    - H1: 모든 집단의 집단 내 분산이 같지는 않다
    - p-value가 0.05보다 커야 한다

2. 분산 분석 가설 (Omnibus test)

- H0: 모든 집단의 평균이 같다
- H1: 모든 집단의 평균이 같지는 않다
- F 값이 크다면 사후 분석할 필요가 있다

3. 사후 분석 (Post-hoc test)

- 그룹 pair 별로 t-test
- 사후 분석 시 correction이 필요
    - 1종 오류를 저지르지 않을 확률 0.95라고 하면
        - A와 B는 다르다, A와 C는 다르다, B와 C는 다르다 ... 의 확률은 0.95 * 0.95 * 0.95 * ... 로 그 확률이 떨어짐
    - Bonferroni correction은 1종 오류를 저지르지 않을 확률을 0.99로 올려줌

## 1종 오류, 2종 오류에 대해

- 1종 오류
    - 대립가설을 채택하지 않아야 하는데, 귀무가설을 기가함
- 2종 오류
    - 대립가설을 채택해야 하는데, 귀무가설을 기각하지 않음
- 1종 오류가 더 조심해야하는 오류

### 유의확률과 양측검정 및 단측검정에 대해

![p_value_1](https://github.com/zacinthepark/TIL/assets/86648892/ec620dbf-c8da-48ff-8d62-4e51827ba317)
![p_value_2](https://github.com/zacinthepark/TIL/assets/86648892/afb34163-f57a-48b2-90f0-4aacd0be30a8)
![p_value_3](https://github.com/zacinthepark/TIL/assets/86648892/047f67a5-0514-494b-b5cf-421f1f4ae479)

- 어떤 차이에 대한 **분포** 가 있을 때
- 이 때 **판단 기준** 으로 p-value를 계산
    - 어떤 사건이 우연히 발생할 확률 (Probability Value)
    - 통상적으로 값이 0.05보다 작으면 우연히 발생한 것이 아닌, x에 따른 y의 차이가 있는 것으로 봄 (귀무가설 기각)

- 위 예시는 수요량의 차이 분포
- A-B의 차이가 8이다
    - 해당 확률분포에서 8이나 그 이상의 차이가 나려면 그 확률이 x축 절대값 8기준 양쪽 부분 면적이다
    - 즉, 그 확률면적을 봤을 때, 이런 차이가 날며녀 5% 이하의 확률 정도로 가질 정도로 드물어야 우연히 발생한 것이 아니다, 즉 유의한 차이다 라고 생각하는 것
    - 그렇다면 A-B 매장 관계에 대해서 더 조사해볼 명분이 생기는거겠지

- 통상적으로 유의수준은 0.05이지만 제조, 의료 분야 등 더 보수적인 기준이 필요할 땐 0.01을 사용하기도 하며, 이때는 해당 차이값이 일어날 확률이 1%도 안될 때 유의한 것으로 보고, 관심을 두겠지

- x -> y 가설 검정을 한다면
    - x에 따른 y의 차이에 사용하는 값을 검정 통계량이라 하며, t 통계량, 카이제곱 통계량, f 통계량 등이 있음
    - 통계량들 역시 차이에 대한 분포가 존재 (이 분포들은 경험적으로 이미 알고 있음)
    - 그렇다면 이 분포를 바탕으로 해당 통계량을 봤을 때 그 차이값이 유의한가? 관계가 있다고 볼 수 있는가? 판단

- **단측검정**
    - A 매장과 B 매장 중 **어디의 수요량이 더 큰가?**

- **양측검정**
    - 매장 간에 **수요량의 차이가 있나?**

- 양측검정 시에는 단측검정에 비해 구간 조정을 통해 p-value에 대한 조정을 한다

## 회귀분석에 대해

![reg_1](https://github.com/zacinthepark/TIL/assets/86648892/55ab67fd-c180-4a66-bbd4-277fd5821345)

![reg_2](https://github.com/zacinthepark/TIL/assets/86648892/52c6dcad-81ad-4d4e-a93b-0325fef573d5)

- 회귀분석은 유전학 용어에서 기원
- 자녀의 키들이 전체 평균 값에 몰리는 경향이 생기는데, 이러한 현상을 회귀라 불렀음
- 이처럼 평균으로의 회귀 현상(regression)은 유전적 현상을 넘어 모든 자연, 사회적 현상에 적용할 수 있음을 여러 연구를 통해 결론

- 회귀: 평균으로의 회귀 (현상)
- 회귀분석: 데이터를 이용해서 선을 그어 분석하는 방법 (기법)

- 회귀분석은 인과 관계에 대해 말한다?
    - 존 스튜어트의 인과관계 조건
        - contiguity
        - temporal precedence
        - constant conjunction
    - 인과관계는 모델을 통해 밝혀낼 수 있는 것이 아니라, 연구설계를 통해 밝혀지는 것
        - 연구설계 시에 시간적 선행성, 메커니즘 등을 정의

![reg_3](https://github.com/zacinthepark/TIL/assets/86648892/3c5083f8-1c72-41ef-9aff-5434aa538c4a)


- 종속변수를 오징어, 3개의 블럭을 독립변수 1, 2, 3이라 하고 모형을 만들어보자
    - '오징어의 모습은 3개의 세모, 네모, 동그라미 블록과 이 3개의 블록으로 표현되지 않는 자투리로 구성되었다'
- 회귀계수: 블럭의 크기를 정해짐
    - 세모의 회귀계수가 큰 경우, 세모 블럭은 머리가 큰 오징어를 설명하는데 중요한 블럭, 중요한 변수다라고 하는 것

![reg_4](https://github.com/zacinthepark/TIL/assets/86648892/a1daa20d-85de-49f1-8fa5-2889df86512b)

- 중요한 것은 '3개의 블럭이 오징어다' 라고 주장해야함
    - reality와 통계적 모형
    - 통계적 모델이 reality라고 우기는 것이 주장
    - R 스퀘어는 블럭 3개로 설명되는 넓이 실제 오징어 넓이에 비해 얼마나 차지하고 있는가
    - 오차가 적을수록 R 스퀘어가 커지고, R 스퀘어가 커질수록 설득력이 올라감

- 블럭을 3개 쓰는 이유에 대한 이론적 설명 후
    - R 스퀘어 값을 통해 설득

- 다중공선성, VIF
    - 변수 여러 개 쓰는 다중회귀분석 시, 블럭들끼리 겹치지 않도록 VIF(분산팽창계수)를 통해 판단
    - 90% 정도 겹쳐지면 블럭 둘 중 하나만 쓰라는 것

## 구조방정식(Structural Equation Modeling)에 대해

![sem_1](https://github.com/zacinthepark/TIL/assets/86648892/5e5529b1-2861-45a6-abe9-5aa1d50fbf29)
![sem_2](https://github.com/zacinthepark/TIL/assets/86648892/396387f1-9053-4d7a-afcc-fd4c49322525)

### 잠재변수와 측정변수

- 머리라고 하는 잠재변수는 4개의 측정변수로 이루어져 있다
- 몸통이라고 하는 잠재변수는 4개의 측정변수로 이루어져 있다
- 다리라고 하는 잠재변수는 여러 개의 측정변수로 이루어져 있다

![sem_3](https://github.com/zacinthepark/TIL/assets/86648892/e9e36b3b-8c56-425e-a7bf-2cdfec515369)
![sem_4](https://github.com/zacinthepark/TIL/assets/86648892/2f4f60c3-08ac-4971-9e27-497de483c322)

### 구조모형

- 잠재변수들 간의 관계
    - 일반적인 경우는 가장 오른쪽 경우
    - 머리가 몸통 크기, 다리 크기에 영향을 미치고, 몸통 크기도 다리 크기에 영향을 미친다
    - 이를 통계적으로 이야기하면 **구조모형** , **경로모형** 이라 한다
- **외생변수** : 잠재변수 중 영향을 주기만 하는 변수
    - 머리
- **내생변수** : 잠재변수 중 영향을 받기만 하거나, 영향을 받거나 주는 변수
    - 몸통, 다리

### 측정모형

![sem_5](https://github.com/zacinthepark/TIL/assets/86648892/e0ea4a4f-46e9-483e-83f7-508189939eaa)

- 머리라는 하나의 잠재변수를 4개의 측정변수로 재고, 에러가 남는다

### 전체모형

![sem_6](https://github.com/zacinthepark/TIL/assets/86648892/96b9cb10-d323-4e5a-a156-955402dcd3d0)

### 회귀분석과의 차이

1. y는 잠재되어있다

- 회귀분석의 경우 y(오징어), x(퍼즐들)이 있을 때 y와 x 모두 데이터를 수집했을 때 실제로 수집한 것들
    - 실제와 모델과의 차이를 보는 것이 모델 적합성
- 구조방정식의 경우 y1(오징어 머리), y2(오징어 몸통), y3(오징어 다리)의 이상적인 모습과 모델로 만들 수 있는 이상적인 모습의 차이를 보는 것

2. 퍼즐들 간의 관련성이 있다

![sem_7](https://github.com/zacinthepark/TIL/assets/86648892/d79205d3-fd54-4401-b966-71d320e4944b)
![sem_8](https://github.com/zacinthepark/TIL/assets/86648892/bafd5176-1ca4-442b-98ba-f018727d9d46)
![sem_9](https://github.com/zacinthepark/TIL/assets/86648892/8117ea84-f06c-4cfb-8d86-113ec409ca43)

- 그렇다면 이상적인 모습이 통통한 오징어 모습이라고 한다면?
- 퍼즐들을 통해서 충분히 만들 수 있지 않을까?
- 하지만 구조방정식에서는 퍼즐들 간의 관련성이 있다
    - 네모와 세모는 같이 커지는 관계, 세모와 동그라미는 하나가 커지면 작아지는 관계라 해보자
    - 세모를 늘린다면 네모도 늘어나겠지만, 동그라미는 줄어들 것
    - 동그라미를 늘려본다면 세모와 네모가 줄어들 것

### 절대적합지수와 상대적합지수

- 그렇기에 이상적인 것에 비교해 현실적인 것을 적당히 맞춰가는 것
- y(이상)과 모델의 차이를 말해주는 것이 절대적합지수
- 상대적합지수는 만들어진 여러 모형들 간의 우열을 정하는 것

## 인공신경망(Artificial Neural Network)에 대해

![ann_1](https://github.com/zacinthepark/TIL/assets/86648892/163a58a2-cfc3-4a0a-8810-c12054e35ee1)
![ann_2](https://github.com/zacinthepark/TIL/assets/86648892/cea22215-a81d-4b7e-8514-ad27cd52c06d)

- 선형변환: 퍼즐의 크기를 바꾸는 단순한 변환
- 비선형변환: 퍼즐의 색과 질감 모양까지 변환 

- 더 비슷하게 만들 수 있을까?
- 데이터(퍼즐)을 투입했을 때
- 완벽한 퍼즐이 되어가는 과정
    1. 비선형적 변환이 일어난다 (크기 뿐 아니라 색, 질감 등도 변환)
    2. 이 단계가 여러 번 일어난다

### 입력층, 은닉층 출력층

![ann_3](https://github.com/zacinthepark/TIL/assets/86648892/7f25d8ba-9cba-4922-9dc1-a156b7643a38)

- 입력층: 투입된 퍼즐
- 은닉층: 비선형변환을 거치는 과정
- 출력층: 완벽한 퍼즐의 형태

- Deep Learning에서 deep의 의미는 은닉층이 길다는 뜻
- 변형되는 과정이 길고 깊다

### 왜 인공신경망이라고 할까?

- 변형되는 과정이 사람의 신경과 유사한 원리
- 복잡한 자극에 대한 단순하고 직관적인 해석
- 입력층에서 복잡한 데이터들이 들어오면 결론에 도달하기 위해 실제 중요한 정보들을 은닉층에서 필터링
- 출력층에서 어떤 것이 제일 확률이 높은지를 바탕으로 결론 도달

- 처음에는 연결 강도가 랜덤
- 데이터를 계속 투입하다보면 이에 대해 어떻게 처리할지 바뀌어가면서 신경망 완성되어 가는 과정이 머신러닝
- 은닉층이 길수록 딥러닝

## 요인분석에 대해

- 변수를 몇 가지 요인으로 만들 수 있을까?
- anova, t 분석 등 통계적 모형, 모델의 목적은 데이터를 단순화시키는데 있다
- 변수를 요인으로 어떻게 줄여서 설명할 수 있을까?

- 주성분 분석(PCA)
    - 변수들의 조합을 통해 주성분을 설명
    - 요리를 재료를 이용해서 설명하면, 주성분 분석
    - `김치볶음밥 = 김치 1밥그릇 + 밥 1밥그릇 + ... + 스팸 반 캔 + 계란 1개 + 양파 반개`
- 요인분석
    - 요인을 통해 변수를 설명
    - 재료를 요리를 이용해서 설명하면, 요인 분석
    - `간장2스푼 = 1*김치볶음밥 + 1*오뎅탕`

- 요인을 선택하는 방법은?
    - 요인을 몇 개 뽑아내냐가 중요
    - 요인을 뽑아내는 논리는 거기에 있는 변수들을 얼마나 잘 소모하냐
    - 잘 소모했는가에 대한 통계적 기법으로 고유값(Eigen value), 스크리 도표(Scree plot)를 사용

- 고유값
    - 요인분석해서 나온 결과의 요인의 고유값이 1보다 크면 보통 그럴듯한 요인으로 채택
    - 요리로 치면 '1인분 이상 나왔다', '요리라고 할만한 정도의 크기로 나왔다', '재료를 어느정도 유의미하게 소진을 잘했다'

- 스크리 도표
    - x축이 요인 수(요리 수), y축이 재료량이라 할 때
    - 요인 수가 1, 2, 3, 4 ... 늘어나면 재료량은 점점 소모되어 줄어들 것
    - 요인 수를 늘릴 때 재료가 소모되는 정도를 봐서 요인을 몇 개 정도로 하는 것이 적절한지 판단

## 고유값과 고유벡터에 대해

- Eigen은 독일어로 characteristics, 어떤 대상을 잘 나타내는 값, 특성
- 직육면체와 직사각형
    - 직육면체: 가로 * 세로 * 높이
    - 직사각형: 가로 * 세로
    - 종이 뭉치가 있다고 가정
        - 종이 뭉치는 가로 * 세로 * 높이로 설명 가능
        - 종이 한장 역시 사실 매우 작은 높이를 가진 부피로 설명 가능
        - 하지만 그 값이 다른 값에 비해 너무나도 작으면 실용적인 목적으로 무시할 수 있음

![eigen](https://github.com/zacinthepark/TIL/assets/86648892/841c8f8c-8a54-4256-a86b-eeff05343e39)

- 데이터를 수집했을 때 상관관계 행렬을 만들어보면 변수1-변수2의 상관관계가 높고, 변수1-변수3의 상관관계가 매우 높고, 변수2-변수3의 상관관계가 높지 않음
- 이 행렬을 통해 고유벡터를 계산하면 `2, 1, 0` 으로 나옴
- 고유값은 변수의 수만큼 나옴
- 마지막 값 0은 작기에 2개의 요인으로 데이터를 설명하는 것이 충분하다고 판단

## 오즈와 로짓에 대해

- 오즈는 확률을 바꾼 것
- `odds(p) = p / 1-p`
    - 50%일 때 1, 90%일 때 9, 100%일 때 inf
    - 확률은 범위가 0 ~ 1
    - 오즈로 변환시키면 범위가 0 ~ 무한대

- 양념장을 설탕과 간장을 1:3으로 만들어라
    - (설탕은 `1` 오즈) 간장을 `3` 오즈로 만들어라
    - (간장을 `1` 오즈) 설탕을 `1/3` 오즈로 만들어라
- 설탕을 전체 양념장의 25%에 맞추어 넣어라

- 로짓
    - `log(오즈비)`
    - 오즈를 정규분포와 연결하는 다리

    - 오즈비
    - `설탕 오즈 / 간장 오즈`
        - 설탕 1 : 간장 3: `1/9`
        - `log(1/9) = -2.19`
        - 설탕 1: 간장 1: `1`
        - `log(1)` = 0

    - 정규분포와 매우 유사한 형태
        - 로짓값의 크고 작음을 통계적으로 판단하기 아주 유용함

- 로짓을 이용한 것이 로지스틱 리그레션

## 우도와 최대우도추정법에 대해

- 모델과 추정치의 우도

- `(우울감) = 회귀계수(자존감)`
    - 종속변수 우울감을 독립변수 자존감이 있을 때, 독립변수와 종속변수 간 회귀계수를 곱해 종속변수를 예측하는 회귀분석
    - 모델: 위의 식
    - 추정치: 추정하는 회귀계수 값

- 우도
    - 모델과 추정치가 데이터와 잘 맞으면, 데이터를 잘 설명하면 높아지는 값
    - 모델과 추정치가 데이터와 잘 맞는 정도를 확률로 표현한 것
    - 최우추정치: 데이터에 가장 잘 맞는 추정치
    - 최대우도법: 데이터에 제일 잘 맞는 모델과 추정치를 계산하는 방법

- 우도와 확률의 차이
    - 둘 다 데이터, 모델과 추정치가 얼마나 가까운지를 나타내는 것이지만 원인과 결과가 다르다

    - 확률: `모델과 추정치 -> 데이터?`
        - 모델과 추정치가 있을 때 해당 데이터가 나올 확률은?
        - 모델: 100원짜리 동전 3번 던져 앞면이 나올 확률은?
        - P = `(1/2)**3`

    - 우도: `데이터 -> 모델과 추정치?`
        - 데이터가 있을 때 데이터와 모델이 잘 맞을 우도는?

    - 우도의 단위도 결국 확률이라 0 ~ 1 사이의 값

## AIC와 BIC에 대해

- 모델 적합성 판단 시 사용
    - Akaike Information Criterion
    - Bayesian Information Criterion

-  AIC, BIC는 표준오차랑 비슷해서 클수록 안좋은 것인데, 이를 계산하기 위해서는 우도를 알아야 한다

![acacac](https://github.com/zacinthepark/TIL/assets/86648892/fce15673-6bce-4a48-a392-5cea684c2b70)
![acacac1](https://github.com/zacinthepark/TIL/assets/86648892/5c65bf0a-d547-44e6-89c1-aedfae39f136)

- AIC, BIC = `-2log(우도) + (something more)`
    - `-2log(우도)`는 Deviance
    - AIC: `-2log(우도) + 2p
    - BIC: `-2log(우도) + plog(n)
