# 통계와 친해지기

> 첫 술에 배부를 순 없다. 친해져가며 익히자.

## SPSS (statistical Package for Social Sciences) 기본 앱 기능

- 기술 통계: 교차표, 빈도, 기술 자료, 탐색, 기술 비용 통계 등
- 이변량 통계: 평균, t-검정, 분산 분석, 상관 관계, 비모수 검정 등
- 수치 결과 예측: 선형 회귀 분석
- 집단 식별 예측: 요인 분석, 클러스터 분석

## 분산과 표준편차에 대해

- 세상의 모든 것을 같은 단위로 잴 수 있는 것
    - 평균을 기준으로 얼마나 큰지 작은지
    - 평균이 4인 어떤 값에 대해 1, 2, 3, 4, 5, 6, 7와 같이 척도를 줄 수 있는 것
    - 180cm는 어느정도로 큰지? 180kg는 어느정도로 무거운지?
    - Dimensionless Quantity

- 평균에서의 이탈도

- 분산은 평균과의 편차를 제곱한 것들의 평균
    - 제곱은 음수 방지용
    - 평균과의 편차 합은 항상 0으로 수학적으로 의미가 없음
    - 표준편차는 제곱의 반대 행동을 한 루트를 적용해 힘을 뺀 값

## 중심극한정리에 대해

- 표본평균의 분포의 평균은 모평균의 평균을 따른다
- 표본평균의 분포는 모분산을 샘플크기로 나눈다
    - 표본평균들은 더 빽빽히 모평균을 중심으로 모여있을 것임
- 표본 크기를 극한으로 근사시킬 때, (그리고 해당 표본들의 평균들을 계속 구해서 얻은) 표본평균의 분포는 중심(모평균)으로 모이게 된다

## 표본평균의 분포와 표준오차에 대해

- 분포는 어떤 변수에 대해서 표집을 했을 때 믿을 수 있는 정도(확률)

- 분포는 한 번만 표집해도 생긴다
    - 예를 들어, 평균 연봉을 알고 싶은데, 샘플링을 한 번만 했다고 하자
    - 이게 믿을만한가? 라는 생각이 드는 순간부터 분포가 있는 것
    - 여기서 더 나아가 샘플링을 더 하다보면 중심극한정리에 의해 중심값에 점점 값들이 모일 것임

- 분포의 장점은 작은 정보(평균, 표준편차)로 많은 정보(어떤 수의 크고 작음)을 알 수 있다는 것

> 우리가 표본을 뽑기 전에 모집단에 대한 가정 (모집단의 평균과 표준편차) 을 이미 하였기 때문에, 중심 극한 정리에 의해 표본의 평균이 정규분포 (평균이 모평균이고 분산이 모분산/표본의개수) 를 따르는 것을 이미 알고 있어서, 한번 추출된 표본의 평균이 앞서 이야기한 표본평균의 분포에서 어디쯤에 위치하는지를 표준오차 (표본평균 분포의 표준편차) 와 비교해 보면, 모집단의 가정이 참이라고 했을 때 얼마나 말이 되는 표본인지를 가늠할 수 있다.

- 표집들의 분포의 표준편차가 표준오차
- 표준오차는 왜 필요할까?
- 우리가 표집을 통해 얻은 값이 얼마나 믿을 수 있는지 알고 싶어서

## 신뢰구간에 대해

- 95% 신뢰구간은 100번
- 데이터가 많을수록, 편차가 적을수록 신뢰구간은 좁아짐


![ci_1](https://github.com/zacinthepark/TIL/assets/86648892/2694796f-f9e2-42f7-af0b-4e7f0005e903)

![ci](https://github.com/zacinthepark/TIL/assets/86648892/e4ce664b-d9a7-4b39-9c06-e09912f45f8a)

- 표본평균은 중심극한정리에 의해 모평균을 중심으로 하는 정규분포를 따른다

- 이때 표준오차(표본평균들의 표준편차)에 의해 해당 표본평균 값(통계량)은 그림과 같이 점이 아닌 잔상을 같는 형태가 될 것
    - 샘플사이즈를 늘려 표준오차를 줄여 오차범위를 줄일 수 있음

- 신뢰구간은 저 잔상 중 어디까지를 인정할까에 대한 것
    - 신뢰수준이 커질수록 더 많은 표집 분포의 더 많은 영역을 인정함
    - 과학적으로 통용되는 신뢰수준이 99%, 95%
    - 95% 신뢰구간은 1.96 * SE 를 통해 구함

- 유의한 차이가 있다는 것은 잔상이 겹치지 않는다는 것

## 표본분산 n-1로 나누는 이유에 대해

- 표본분산: `(편차제곱합) / (n-1)`
- 모분산: `(편차제곱합) / (n)`

- 모든 정보의 공개된 모집단에 비해 일부 정보가 공개된 표본의 편차제곱합은 작게 추정됨
    - 그렇기에 더 작은 값인 (n-1)로 나눠줘서 더 작게 추정되는 편차제곱합을 올려줘야함
    - 그렇다면 왜 표본분산(편차제곱)은 모분산보다 값이 작게 추정되는가?

- 표본평균은 절대 모평균과 같은 값이 아니다
    - 표본평균은 표본값들에 더 가까움
    - (평균-표본값)의 제곱인 분산은 (모평균-표본값)에 비해 (표본평균-표본값)은 작게 계산됨
    - 즉, 모평균이 표본평균보다 크거나 작을 수도 있지만, 분산은 제곱을 사용하여 음수건 양수건 무조건 양수로 바꾸므로 모평균과 표본평균에 차이가 있다는 사실이 무조건 표본분산을 크게 만드는 결과를 가져옴

## 자유도(dof)에 대해

- 계산의 자유도
    - 평균의 자유도, 분산의 자유도, 분포의 자유도, 모델의 자유도 ... 모든 곳에 쓸 수 있는 말
    - 계산식으로 나타낼 수 있는 순간 자유도를 갖는다

- 계산식의 서로 독립적인 정보의 수는 자유도를 올린다
    - 미지수

- 계산식의 추정치는 자유도를 낮춘다
    - 어떤 추정치가 계산식에 들어간다면, 산식의 결과가 특정값으로 수렴되거나 고정됨

- 쉽게 생각한다면 (계산식의 미지수 수) - (계산식의 추정지 수) 로 생각할 수 있다

- 분산 분석의 자유도는?
    - 집단간 분산
        - (전체 평균 - X1의 평균) + (전체 평균 - X2의 평균) + ... (전체 평균 - Xk의 평균)
        - k개의 독립된 정보
        - 전체 평균이라는 1개의 추정치
        - 자유도는 k-1

## 공분산과 상관에 대해

- 분산의 단위는 제곱
    - 평균이 171 센티미터라면 분산은 25 제곱 센티미터
    - 이렇게 단위가 바뀌기에 평균에 비해 분산은 우리가 받아들이기에 직관적이지 않을 수 있다

- 분산, Variance는 곧 변량
    - 변량은 곧 차이, 평균보다 작거나 or 크거나
    - 각 데이터들을 평균을 기점으로 해서 얼마나 달라졌는지의 관점으로 설명할 수 있다

- Covariance
    - 공분산은 곧 공변량
        - covariate을 공변량으로 해석하는데 이에 대해서는 공변수가 더 적절하지 않을까라는 개인적인 의견

    - 편차끼리의 곱
        ![image](https://github.com/zacinthepark/TIL/assets/86648892/5e7b38e7-a185-4757-8f60-98035f031fff)

    - 공변량은 변량이 평균보다 크냐 작냐에 대한 것
        - 하나는 평균 위, 하나는 평균 아래라면 값이 음수
        - 둘 다 평균 위라면 값이 양수
    
    - '곱'이라는 것도 중요
        - 1000 + 0.1 = 1000.01
        - 1000 x 0.1 = 10
        - 둘다 편차가 커야 공분산 값도 커짐
        - 하나 값이 평균과 근사하다면 공분산도 작아질 것
        - 공분산이 크다는 것은 곧 둘다 변량이 크다는 것

- Correlation
    ![cov](https://github.com/zacinthepark/TIL/assets/86648892/cb7a621f-c986-48d3-b07c-d694a2f89dc7)

    - 공분산인데 단위만 맞춰준 것
    - 공분산을 각자의 표준편차로 나눠주어서 -1 ~ 1 범위 값으로 맞춰준 것
    - corr 값이 강하다는 것은 x의 변량과 그 때의 y 변량이 크다는 것이고
    - corr 값이 약하는 것은 x, y 중 적어도 하나의 변량이 미미하다는 것

## t, 카이제곱, f 통계량에 대해

- 일반적으로 통계량이라 한다면 t값, 카이제곱값, f값이 있다 (그 외에도 있긴 하다)

- 값들의 크고 작음이 중요하다
    - 각각의 값들이 context가 있어서 사용되는 상황에 따라, 어떤 기법이냐에 따라 같은 값이더라도 디테일하게 해석되는 방법이 달라짐
    - 대신 어떤 값이 나와야 좋은걸까, 어떤 값을 집중해야 되고, 어떤 값을 무시해야 되는 등 판단
    - 어떤 값이 의미있는 결과인가?
        - 보통 큰 값들에 주목함
    - 값 자체의 해석은 context에 따라 달라지지만, 자세하게 해석하는 방식이 미묘하게 달라지는 것이지 중요한 것은 이 값들이 큰지 작은지
        - t 값이 크다
            - 어떤 분석이냐에 따라
            - 차이가 있다
            - 모델이 의미가 있다
            - 라고 해석할 수 있다

- 값들의 크고 작음은 단위가 있어야 한다
    - 분포를 통해 알 수 있다

- 통계량 -> 분포를 통해 크고 작음 확인 -> 해석 (차이가 있다, 모델이 의미있다)

## t 분포에 대해

![image](https://github.com/zacinthepark/TIL/assets/86648892/8ce8e8aa-77c6-4175-b075-7279f6ceca6c)

- 샘플사이즈가 적은 표본평균들의 분포
- 평균의 차이 분포 (두 그룹 간)
    - 그러므로 그 값을 구했을 때 값이 큰지 작은지 판단 가능

- 여러 범주로 나눴을 때 어떤 값의 평균 차이는 0이 제일 많을 것이고 그 여태가 표준정규분포 형태를 따를 것이라 예상
- 하지만 샘플사이즈가 적을 때 두 그룹을 정했을 때 평균 차이들을 분석해보니 양쪽이 더 두꺼운 분포가 있음을 알게됨
- 샘플의 크기가 작은 경우에 한해서 정규분포보다 t 분포가 훨씬 가깝다는 것을 경험적으로 알게됨
- 현실세계에서 샘플 사이즈가 유한한 경우가 많기에 샘플에 대해서 그 평균을 계산한 것의 분포를 알고 싶다면 그것은 t 분포
- 샘플 사이즈가 크다면 (자유도가 높다면, 더 많은 샘플들을 뽑는다면) 범주 간의 차이가 더 줄어들 것
    - 자유도가 높을수록 표준정규분포 형태에 더 가까움

- 정규분포는 평균, 표준편차만 알게 된다면 수의 크고 작음을 알 수 있음
- t 분포는 여기에 sample size라는 새로운 정보를 알아야 수의 크고 작음을 알 수 있음

### t-test

![t_test_1](https://github.com/zacinthepark/TIL/assets/86648892/b58fa961-1f60-46ef-a118-4e82c9e7fb08)
![t_test_2](https://github.com/zacinthepark/TIL/assets/86648892/1f572187-f488-4c26-bf26-22ca3c0968f4)

- t-test의 자유도는 평균값은 정해져 있어서 n-1

- t-value
    - 두 집단의 평균의 차이를 보기위한 통계량
    - t는 그룹 간 평균 차이에 비례하는 변수
    - `표본평균값의 차이 / 표본평균이 갖는 불확실도`
    - 표본평균값의 차이를 불확실도로 나눠줘서 차이가 유의하게 큰지 확인할 수 있음

1. 그룹 간 평균 차가 클수록 t-value는 크다
2. t-value는 평균 차이를 불확실도로 나눈 것
3. 즉, 평균차가 클수록 t값은 커진다
4. 불확실도가 적을수록 t값은 커진다

- 독립표본 t-test: 두 그룹에 들어있는 사람들이 전혀 다른 사람들이다
- 대응표본 t-test: 두 그룹에 들어있는 사람들이 같은 사람들이다 (비포-애프터 검정)

## 카이제곱 분포에 대해

![image](https://github.com/zacinthepark/TIL/assets/86648892/888c965b-6e59-45f1-a434-1fe812fd793d)

- t 분포를 제곱한 분포
- 표본분산의 분포

- 표준정규분포와 카이스퀘어 분포
    - 어떤 확률변수 x가 표준정규분포(평균0, 분산1)를 따를 때
    - x^2은 (0-x)^2, 분산을 의미
    - 표준정규분포의 분산은 카이스퀘어 분포를 따름
    - 카이스퀘어 분포는 곧 표준정규분포에서 분산의 분포
        - 표준정규분포의 분산의 크고 작음을 이야기해준다
        - 표준정규분포가 좁아지거나 넓어지거나의 확률이 어느정도 되는가
    - 이를 유사한 분포인 t 분포에 적용 가능
        - 표본평균들의 분산 정도를 파악 가능

- 결국 분산에 대한 값의 크고 작음을 판단할 수 있다

- 기대빈도와 관측빈도에 적용한다면
    - 기대빈도는 각 값의 전체평균이라 이해할 수 있음
    - `(기대빈도 - 각 관측값) / (기대빈도)`의 제곱 = `(전체평균 - 각 관측값) / (기대빈도)`의 제곱 = 편차제곱의 합
    - 분산으로 이해할 수 있음
    - 실제 기대빈도로부터 값이 얼마나 차이가 있는가
    - 차이가 있다면 집중할만한 값, 관계

## f 분포에 대해

![image](https://github.com/zacinthepark/TIL/assets/86648892/0a614c65-f1f9-4943-9c70-abfdd8a901d4)

![anova_1](https://github.com/zacinthepark/TIL/assets/86648892/72b183d1-a1c2-4836-8550-c1cc6c43c1e0)
![anova_2](https://github.com/zacinthepark/TIL/assets/86648892/2bc79d39-cd4f-4512-9cf9-4a56314e0087)
![anova_3](https://github.com/zacinthepark/TIL/assets/86648892/a7d8a122-6ae6-4760-8f0b-5e119467f213)
![anova_4](https://github.com/zacinthepark/TIL/assets/86648892/62de3b44-f8d3-435d-8e76-4190fa9e59a4)
![anova_5](https://github.com/zacinthepark/TIL/assets/86648892/baf89db7-345f-42b7-b2a8-4f759d4f86a3)

- 여러 집단 평균의 차이를 보기 위한 통계량
- 모든 그룹의 평균이 동일한지, 최소한 하나의 그룹의 평균은 유의하게 다른지 판별할 수 있음

- 카이스퀘어 분포들의 비율이 F 분포
    - `카이제곱 분포 / 카이제곱 분포`
    - `분산 / 분산`
    - `집단 간 분산` / `집단 내 분산`
        - ANOVA

- ANOVA에서 사용되는 표본분산, 즉 표본평균들의 변화량을 나타낸다
    - F 값이 큰 경우
        - 각 그룹의 평균들이 멀리 떨어진 경우
        - 각 그룹 내 분산이 작은 경우

- 기준값이 1
    - F=1 이라면
    - 집단 간 분산 = 집단 내 분산
    - 집단을 어떤 식으로 나누든 집단 간 분산과 집단 내 분산이 같다는 것
    - 집단 간 분산이 집단 내 분산보다 얼마나 큰지를 통해서 집단 간 차이가 있다는 논리
        - 그러므로 F가 1보다 작은 경우는 거의 논하지 않음
    - F>1 이라면
        - 집단 간 분산이 집단 내 분산보다 몇 배 큰가?
        - 집단 간 차이가 있는가?

### ANOVA (분산분석)

- Analysis Of Variance
    - 분산을 이용한 분석

- 집단이라고 볼 수 있는가? 집단 간 차이가 큰가?

- 분자
    - 집단 간 분석
    - 세 그룹이 있을 때 표본 평균들의 차이를 어떻게 계산할 수 있을까?
        - 세 그룹의 평균의 평균으로부터 떨어진 정도를 이용하는 것이 아이디어
        - 즉, **표본 평균 간 분산** 을 이용해 여러 그룹의 차이를 표현
- 분모
    - 집단 내 분석
    - 비교하고 있는 집단들이 집단이라고 할 수 있는가?
    - 분자에 분산을 넣었으니 분모도 분산을 이용해서 불확실도를 표현
    - 불확실도는 결국 각 그룹의 데이터들이 평균적으로 퍼진 정도를 의미
    - 즉, **표본 그룹 내의 분산** 을 이용해 여러 그룹의 평균적 불확실도 표현

- 변화량을 불확실도로 나눠줘서 변화량이 유의하게 큰지 확인할 수 있음

1. 등분산성(Homosedasticity) 가정 검정

- 집단 간 집단 내 분산이 서로 비슷한가?
    - 집단 내 분산들이 다르다?
        - 동등하게 비교하기 어렵다
 - 가정의 경우 영가설이 채택되어야함
    - H0: 모든 집단의 집단 내 분산이 같다
    - H1: 모든 집단의 집단 내 분산이 같지는 않다
    - p-value가 0.05보다 커야 한다

2. 분산 분석 가설 (Omnibus test)

- H0: 모든 집단의 평균이 같다
- H1: 모든 집단의 평균이 같지는 않다
- F 값이 크다면 사후 분석할 필요가 있다

3. 사후 분석 (Post-hoc test)

- 그룹 pair 별로 t-test
- 사후 분석 시 correction이 필요
    - 1종 오류를 저지르지 않을 확률 0.95라고 하면
        - A와 B는 다르다, A와 C는 다르다, B와 C는 다르다 ... 의 확률은 0.95 * 0.95 * 0.95 * ... 로 그 확률이 떨어짐
    - Bonferroni correction은 1종 오류를 저지르지 않을 확률을 0.99로 올려줌

## 1종 오류, 2종 오류에 대해

- 1종 오류
    - 대립가설을 채택하지 않아야 하는데, 귀무가설을 기가함
- 2종 오류
    - 대립가설을 채택해야 하는데, 귀무가설을 기각하지 않음
- 1종 오류가 더 조심해야하는 오류

### 유의확률과 양측검정 및 단측검정에 대해

![p_value_1](https://github.com/zacinthepark/TIL/assets/86648892/ec620dbf-c8da-48ff-8d62-4e51827ba317)
![p_value_2](https://github.com/zacinthepark/TIL/assets/86648892/afb34163-f57a-48b2-90f0-4aacd0be30a8)
![p_value_3](https://github.com/zacinthepark/TIL/assets/86648892/047f67a5-0514-494b-b5cf-421f1f4ae479)

- 어떤 차이에 대한 **분포** 가 있을 때
- 이 때 **판단 기준** 으로 p-value를 계산
    - 어떤 사건이 우연히 발생할 확률 (Probability Value)
    - 통상적으로 값이 0.05보다 작으면 우연히 발생한 것이 아닌, x에 따른 y의 차이가 있는 것으로 봄 (귀무가설 기각)

- 위 예시는 수요량의 차이 분포
- A-B의 차이가 8이다
    - 해당 확률분포에서 8이나 그 이상의 차이가 나려면 그 확률이 x축 절대값 8기준 양쪽 부분 면적이다
    - 즉, 그 확률면적을 봤을 때, 이런 차이가 날며녀 5% 이하의 확률 정도로 가질 정도로 드물어야 우연히 발생한 것이 아니다, 즉 유의한 차이다 라고 생각하는 것
    - 그렇다면 A-B 매장 관계에 대해서 더 조사해볼 명분이 생기는거겠지

- 통상적으로 유의수준은 0.05이지만 제조, 의료 분야 등 더 보수적인 기준이 필요할 땐 0.01을 사용하기도 하며, 이때는 해당 차이값이 일어날 확률이 1%도 안될 때 유의한 것으로 보고, 관심을 두겠지

- x -> y 가설 검정을 한다면
    - x에 따른 y의 차이에 사용하는 값을 검정 통계량이라 하며, t 통계량, 카이제곱 통계량, f 통계량 등이 있음
    - 통계량들 역시 차이에 대한 분포가 존재 (이 분포들은 경험적으로 이미 알고 있음)
    - 그렇다면 이 분포를 바탕으로 해당 통계량을 봤을 때 그 차이값이 유의한가? 관계가 있다고 볼 수 있는가? 판단

- **단측검정**
    - A 매장과 B 매장 중 **어디의 수요량이 더 큰가?**

- **양측검정**
    - 매장 간에 **수요량의 차이가 있나?**

- 양측검정 시에는 단측검정에 비해 구간 조정을 통해 p-value에 대한 조정을 한다




## 회귀분석에 대해

![reg_1](https://github.com/zacinthepark/TIL/assets/86648892/55ab67fd-c180-4a66-bbd4-277fd5821345)

![reg_2](https://github.com/zacinthepark/TIL/assets/86648892/52c6dcad-81ad-4d4e-a93b-0325fef573d5)

- 회귀분석은 유전학 용어에서 기원
- 자녀의 키들이 전체 평균 값에 몰리는 경향이 생기는데, 이러한 현상을 회귀라 불렀음
- 이처럼 평균으로의 회귀 현상(regression)은 유전적 현상을 넘어 모든 자연, 사회적 현상에 적용할 수 있음을 여러 연구를 통해 결론

- 회귀: 평균으로의 회귀 (현상)
- 회귀분석: 데이터를 이용해서 선을 그어 분석하는 방법 (기법)

- 회귀분석은 인과 관계에 대해 말한다?
    - 존 스튜어트의 인과관계 조건
        - contiguity
        - temporal precedence
        - constant conjunction
    - 인과관계는 모델을 통해 밝혀낼 수 있는 것이 아니라, 연구설계를 통해 밝혀지는 것
        - 연구설계 시에 시간적 선행성, 메커니즘 등을 정의

![reg_3](https://github.com/zacinthepark/TIL/assets/86648892/3c5083f8-1c72-41ef-9aff-5434aa538c4a)


- 종속변수를 오징어, 3개의 블럭을 독립변수 1, 2, 3이라 하고 모형을 만들어보자
    - '오징어의 모습은 3개의 세모, 네모, 동그라미 블록과 이 3개의 블록으로 표현되지 않는 자투리로 구성되었다'
- 회귀계수: 블럭의 크기를 정해짐
    - 세모의 회귀계수가 큰 경우, 세모 블럭은 머리가 큰 오징어를 설명하는데 중요한 블럭, 중요한 변수다라고 하는 것

![reg_4](https://github.com/zacinthepark/TIL/assets/86648892/a1daa20d-85de-49f1-8fa5-2889df86512b)

- 중요한 것은 '3개의 블럭이 오징어다' 라고 주장해야함
    - reality와 통계적 모형
    - 통계적 모델이 reality라고 우기는 것이 주장
    - R 스퀘어는 블럭 3개로 설명되는 넓이 실제 오징어 넓이에 비해 얼마나 차지하고 있는가
    - 오차가 적을수록 R 스퀘어가 커지고, R 스퀘어가 커질수록 설득력이 올라감

- 블럭을 3개 쓰는 이유에 대한 이론적 설명 후
    - R 스퀘어 값을 통해 설득

- 다중공선성, VIF
    - 변수 여러 개 쓰는 다중회귀분석 시, 블럭들끼리 겹치지 않도록 VIF(분산팽창계수)를 통해 판단
    - 90% 정도 겹쳐지면 블럭 둘 중 하나만 쓰라는 것
