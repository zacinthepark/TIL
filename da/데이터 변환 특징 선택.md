## 데이터 변환 특징 선택

---

- 기존 데이터 공간에서 유의미한 특징을 골라내고 선택하는 방안
- Filter 방식
    - 데이터 유형 별 통계적 기밥 기반의 점수 및 순위 부여하여 주요 특징 선택
- Wrapper 방식
    - 최적의 데이터 조합을 찾기 위한 변수 추가, 삭제를 반복적으로 수행하여 특징 선택
- Embedded 방식
    - 모델 내 변수 선택 기능을 활용하여 모델 성능에 기여도가 높은 특징 선택
- Boruta Algorithm
    - 특징 선택의 주요 알고리즘 중 하나

### Feature Selection

- 가장 좋은 성능을 보여줄 수 있는 데이터의 부분 집합(Subset)을 찾아내는 방법
- 모델 생성에 밀접한 데이터의 부분 집합을 선택하여 연산 효율성 및 모델 성능을 확보

### 목적 및 필요성

<img width="714" alt="feature_selection" src="https://github.com/zacinthepark/TIL/assets/86648892/555c560a-8c19-42a3-9c37-018b74d7239c">

- 특징 생성은 원본 데이터 내 존재하지 않는 새로운 특징을 생성하여 대체하는 것이라면, 특징 선택은 원본에서 좋은 특징을 골라내는 것

### 특징 선택 방안

<img width="752" alt="feature_selection_methods" src="https://github.com/zacinthepark/TIL/assets/86648892/2c1a055c-2edf-4d21-9ad2-e5129e3b987d">

- Filter
- Wrapper
- Embedded

### Filter 방식

<img width="752" alt="filter" src="https://github.com/zacinthepark/TIL/assets/86648892/d2f604c5-57c6-4ebf-ba32-a0a0537f03af">

- 특징들에 대한 통계적 기법 기반의 점수 및 순위 부여하여 선택
- Wrapper 등 연산 비용이 오래 걸리는 방안 적용 전에 활용하기도 함
- 단순히 해당 변수별 통계적 관계가 높다고 해서 해당 특징이 모델에 적합한 변수라 확신할 수는 없음

### Wrapper 방식

<img width="749" alt="wrapper" src="https://github.com/zacinthepark/TIL/assets/86648892/87909a94-582d-419c-9302-1ac514594995">

- 원본 데이터 내 변수 간 조합을 탐색하여 특징 선택
- 성능적으로 바람직하나 연산 비용이 큼
- Wrapper 방식에는 변수가 없는 상태에서 하나씩 추가해가는 방법, 전체에서 중요하지 않은 변수를 하나씩 제거해가는 방식이 있음

### Embedded 방식

<img width="752" alt="embedded" src="https://github.com/zacinthepark/TIL/assets/86648892/d0eddec1-c978-4e05-81b3-d2556ad91142">

- 모델을 학습하여 정확도에 기여하는 특징을 선택하는 방안
- 모델 자체에 변수 선택이 포함된 알고리즘을 활용하여 모델의 정확도에 기여도가 높은 특징을 선택하고, 학습하여 결과를 도출하는 방식
- Filter와 Wrapper의 장점을 결합한 방식
    - 모델의 정확도에 기여하는 특징을 선택하기 때문에, 각 특징들의 중요도, 가중치 등을 평가할 수 있는 모델이 선택이 되어야 하고
    - Wrapper와 같이 해당 모델들의 파마리터, 알고리즘 자체의 완성도가 높아야함

### 특징 선택 알고리즘

#### 랜덤포레스트 모형 기반의 알고리즘

<img width="749" alt="boruta_algorithm" src="https://github.com/zacinthepark/TIL/assets/86648892/51f7839c-7822-4b76-8cfb-bab4bf21a912">

- Boruta Algorithm
    - 기본적인 아이디어는 기존 데이터를 복원 추출해서 만든 랜덤 변수(shadow 변수)보다 모형 생성에 영향을 주지 못했다고 하면, 이는 가치가 크지 않은 변수로 인식해서 제거하는 방안의 특징 선택 알고리즘
    - 랜덤 변수는 원본 데이터를 복제하고, 임의의 값으로 섞어 만든 임의의 변수
    - 이러한 무작위성을 통해 생성된 변수는 기존 특징과의 중요도를 판단하기 위한 참조값으로 활용됨
    - 즉, 예측 성능과 강하게 연관을 지닌 기존 특징들이라면, 랜덤하게 생성된 변수보다 중요도는 높을 것이기 때문에, 랜덤 변수의 중요도보다 낮은 기존 특징들은 중요하지 않을 것이다라고 접근하는 것이 핵심

### 실습

#### Wrapper 기반 특징 선택

- 원 데이터 내 변수 간 조합을 탐색하여 특징 선택하는 방안

```python
import numpy as np
import pandas as pd
```

```python
# 데이터 로딩
cancer = pd.read_csv('./data/wdbc.data', header=None)

# 데이터 컬럼명 지정
cancer.columns = ['id', 'diagnosis', 'radius1', 'texture1', 'perimeter1', 'area1', 'smoothness1', 'compactness1',
                  'concavity1', 'concave_points1', 'symmetry1', 'fractal_dimentsion1', 'radius2', 'texture2',
                  'perimeter2', 'area2', 'smoothness2', 'compactness2', 'concavity2', 'concave_points2', 'symmetry2',
                  'fractal_dimension2', 'radius3', 'texture3', 'perimeter3', 'area3', 'smoothness3', 'compactness3', 
                  'concavity3', 'concave_points3', 'symmetry3', 'fractal_dimension3']

# ID를 인덱스화
cancer = cancer.set_index('id')
cancer
```

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>diagnosis</th>
      <th>radius1</th>
      <th>texture1</th>
      <th>perimeter1</th>
      <th>area1</th>
      <th>smoothness1</th>
      <th>compactness1</th>
      <th>concavity1</th>
      <th>concave_points1</th>
      <th>symmetry1</th>
      <th>...</th>
      <th>radius3</th>
      <th>texture3</th>
      <th>perimeter3</th>
      <th>area3</th>
      <th>smoothness3</th>
      <th>compactness3</th>
      <th>concavity3</th>
      <th>concave_points3</th>
      <th>symmetry3</th>
      <th>fractal_dimension3</th>
    </tr>
    <tr>
      <th>id</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>842302</th>
      <td>M</td>
      <td>17.99</td>
      <td>10.38</td>
      <td>122.80</td>
      <td>1001.0</td>
      <td>0.11840</td>
      <td>0.27760</td>
      <td>0.30010</td>
      <td>0.14710</td>
      <td>0.2419</td>
      <td>...</td>
      <td>25.380</td>
      <td>17.33</td>
      <td>184.60</td>
      <td>2019.0</td>
      <td>0.16220</td>
      <td>0.66560</td>
      <td>0.7119</td>
      <td>0.2654</td>
      <td>0.4601</td>
      <td>0.11890</td>
    </tr>
    <tr>
      <th>842517</th>
      <td>M</td>
      <td>20.57</td>
      <td>17.77</td>
      <td>132.90</td>
      <td>1326.0</td>
      <td>0.08474</td>
      <td>0.07864</td>
      <td>0.08690</td>
      <td>0.07017</td>
      <td>0.1812</td>
      <td>...</td>
      <td>24.990</td>
      <td>23.41</td>
      <td>158.80</td>
      <td>1956.0</td>
      <td>0.12380</td>
      <td>0.18660</td>
      <td>0.2416</td>
      <td>0.1860</td>
      <td>0.2750</td>
      <td>0.08902</td>
    </tr>
    <tr>
      <th>84300903</th>
      <td>M</td>
      <td>19.69</td>
      <td>21.25</td>
      <td>130.00</td>
      <td>1203.0</td>
      <td>0.10960</td>
      <td>0.15990</td>
      <td>0.19740</td>
      <td>0.12790</td>
      <td>0.2069</td>
      <td>...</td>
      <td>23.570</td>
      <td>25.53</td>
      <td>152.50</td>
      <td>1709.0</td>
      <td>0.14440</td>
      <td>0.42450</td>
      <td>0.4504</td>
      <td>0.2430</td>
      <td>0.3613</td>
      <td>0.08758</td>
    </tr>
    <tr>
      <th>84348301</th>
      <td>M</td>
      <td>11.42</td>
      <td>20.38</td>
      <td>77.58</td>
      <td>386.1</td>
      <td>0.14250</td>
      <td>0.28390</td>
      <td>0.24140</td>
      <td>0.10520</td>
      <td>0.2597</td>
      <td>...</td>
      <td>14.910</td>
      <td>26.50</td>
      <td>98.87</td>
      <td>567.7</td>
      <td>0.20980</td>
      <td>0.86630</td>
      <td>0.6869</td>
      <td>0.2575</td>
      <td>0.6638</td>
      <td>0.17300</td>
    </tr>
    <tr>
      <th>84358402</th>
      <td>M</td>
      <td>20.29</td>
      <td>14.34</td>
      <td>135.10</td>
      <td>1297.0</td>
      <td>0.10030</td>
      <td>0.13280</td>
      <td>0.19800</td>
      <td>0.10430</td>
      <td>0.1809</td>
      <td>...</td>
      <td>22.540</td>
      <td>16.67</td>
      <td>152.20</td>
      <td>1575.0</td>
      <td>0.13740</td>
      <td>0.20500</td>
      <td>0.4000</td>
      <td>0.1625</td>
      <td>0.2364</td>
      <td>0.07678</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>926424</th>
      <td>M</td>
      <td>21.56</td>
      <td>22.39</td>
      <td>142.00</td>
      <td>1479.0</td>
      <td>0.11100</td>
      <td>0.11590</td>
      <td>0.24390</td>
      <td>0.13890</td>
      <td>0.1726</td>
      <td>...</td>
      <td>25.450</td>
      <td>26.40</td>
      <td>166.10</td>
      <td>2027.0</td>
      <td>0.14100</td>
      <td>0.21130</td>
      <td>0.4107</td>
      <td>0.2216</td>
      <td>0.2060</td>
      <td>0.07115</td>
    </tr>
    <tr>
      <th>926682</th>
      <td>M</td>
      <td>20.13</td>
      <td>28.25</td>
      <td>131.20</td>
      <td>1261.0</td>
      <td>0.09780</td>
      <td>0.10340</td>
      <td>0.14400</td>
      <td>0.09791</td>
      <td>0.1752</td>
      <td>...</td>
      <td>23.690</td>
      <td>38.25</td>
      <td>155.00</td>
      <td>1731.0</td>
      <td>0.11660</td>
      <td>0.19220</td>
      <td>0.3215</td>
      <td>0.1628</td>
      <td>0.2572</td>
      <td>0.06637</td>
    </tr>
    <tr>
      <th>926954</th>
      <td>M</td>
      <td>16.60</td>
      <td>28.08</td>
      <td>108.30</td>
      <td>858.1</td>
      <td>0.08455</td>
      <td>0.10230</td>
      <td>0.09251</td>
      <td>0.05302</td>
      <td>0.1590</td>
      <td>...</td>
      <td>18.980</td>
      <td>34.12</td>
      <td>126.70</td>
      <td>1124.0</td>
      <td>0.11390</td>
      <td>0.30940</td>
      <td>0.3403</td>
      <td>0.1418</td>
      <td>0.2218</td>
      <td>0.07820</td>
    </tr>
    <tr>
      <th>927241</th>
      <td>M</td>
      <td>20.60</td>
      <td>29.33</td>
      <td>140.10</td>
      <td>1265.0</td>
      <td>0.11780</td>
      <td>0.27700</td>
      <td>0.35140</td>
      <td>0.15200</td>
      <td>0.2397</td>
      <td>...</td>
      <td>25.740</td>
      <td>39.42</td>
      <td>184.60</td>
      <td>1821.0</td>
      <td>0.16500</td>
      <td>0.86810</td>
      <td>0.9387</td>
      <td>0.2650</td>
      <td>0.4087</td>
      <td>0.12400</td>
    </tr>
    <tr>
      <th>92751</th>
      <td>B</td>
      <td>7.76</td>
      <td>24.54</td>
      <td>47.92</td>
      <td>181.0</td>
      <td>0.05263</td>
      <td>0.04362</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.1587</td>
      <td>...</td>
      <td>9.456</td>
      <td>30.37</td>
      <td>59.16</td>
      <td>268.6</td>
      <td>0.08996</td>
      <td>0.06444</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>0.2871</td>
      <td>0.07039</td>
    </tr>
  </tbody>
</table>
<p>569 rows × 31 columns</p>
</div>

##### 데이터 분석에서 모델을 학습하고, 해당 모델의 성능을 평가하기 위한 방안으로 데이터를 분할해서 train_data로 모델을 학습하고, test_data로 학습한 모델의 성능을 측정하게 됨

```python
# train_test_split 함수를 사용하면 타겟 데이터를 기준으로 데이터를 분할하게 됨
# 테스트 데이터의 사이즈를 지정하지 않으면 기본적으로 3:1 (train_data를 75%, test_data를 25% 기준으로 분할)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(cancer.drop(['diagnosis'], axis=1), 
                                                    cancer[['diagnosis']], 
                                                    random_state=1)
print(X_train.shape)
print(X_test.shape)
```

<pre>
(426, 30)
(143, 30)
</pre>

```python
# train_data의 X_train은 독립변수 영역, y_train은 타겟변수 영역 
train_df = pd.merge(X_train, y_train, left_index=True, right_index=True, how='inner')
train_df
```

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>radius1</th>
      <th>texture1</th>
      <th>perimeter1</th>
      <th>area1</th>
      <th>smoothness1</th>
      <th>compactness1</th>
      <th>concavity1</th>
      <th>concave_points1</th>
      <th>symmetry1</th>
      <th>fractal_dimentsion1</th>
      <th>...</th>
      <th>texture3</th>
      <th>perimeter3</th>
      <th>area3</th>
      <th>smoothness3</th>
      <th>compactness3</th>
      <th>concavity3</th>
      <th>concave_points3</th>
      <th>symmetry3</th>
      <th>fractal_dimension3</th>
      <th>diagnosis</th>
    </tr>
    <tr>
      <th>id</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>925622</th>
      <td>15.22</td>
      <td>30.62</td>
      <td>103.40</td>
      <td>716.9</td>
      <td>0.10480</td>
      <td>0.20870</td>
      <td>0.25500</td>
      <td>0.094290</td>
      <td>0.2128</td>
      <td>0.07152</td>
      <td>...</td>
      <td>42.79</td>
      <td>128.70</td>
      <td>915.0</td>
      <td>0.14170</td>
      <td>0.79170</td>
      <td>1.17000</td>
      <td>0.23560</td>
      <td>0.4089</td>
      <td>0.14090</td>
      <td>M</td>
    </tr>
    <tr>
      <th>8915</th>
      <td>14.96</td>
      <td>19.10</td>
      <td>97.03</td>
      <td>687.3</td>
      <td>0.08992</td>
      <td>0.09823</td>
      <td>0.05940</td>
      <td>0.048190</td>
      <td>0.1879</td>
      <td>0.05852</td>
      <td>...</td>
      <td>26.19</td>
      <td>109.10</td>
      <td>809.8</td>
      <td>0.13130</td>
      <td>0.30300</td>
      <td>0.18040</td>
      <td>0.14890</td>
      <td>0.2962</td>
      <td>0.08472</td>
      <td>B</td>
    </tr>
    <tr>
      <th>848406</th>
      <td>14.68</td>
      <td>20.13</td>
      <td>94.74</td>
      <td>684.5</td>
      <td>0.09867</td>
      <td>0.07200</td>
      <td>0.07395</td>
      <td>0.052590</td>
      <td>0.1586</td>
      <td>0.05922</td>
      <td>...</td>
      <td>30.88</td>
      <td>123.40</td>
      <td>1138.0</td>
      <td>0.14640</td>
      <td>0.18710</td>
      <td>0.29140</td>
      <td>0.16090</td>
      <td>0.3029</td>
      <td>0.08216</td>
      <td>M</td>
    </tr>
    <tr>
      <th>922577</th>
      <td>10.32</td>
      <td>16.35</td>
      <td>65.31</td>
      <td>324.9</td>
      <td>0.09434</td>
      <td>0.04994</td>
      <td>0.01012</td>
      <td>0.005495</td>
      <td>0.1885</td>
      <td>0.06201</td>
      <td>...</td>
      <td>21.77</td>
      <td>71.12</td>
      <td>384.9</td>
      <td>0.12850</td>
      <td>0.08842</td>
      <td>0.04384</td>
      <td>0.02381</td>
      <td>0.2681</td>
      <td>0.07399</td>
      <td>B</td>
    </tr>
    <tr>
      <th>891703</th>
      <td>11.85</td>
      <td>17.46</td>
      <td>75.54</td>
      <td>432.7</td>
      <td>0.08372</td>
      <td>0.05642</td>
      <td>0.02688</td>
      <td>0.022800</td>
      <td>0.1875</td>
      <td>0.05715</td>
      <td>...</td>
      <td>25.75</td>
      <td>84.35</td>
      <td>517.8</td>
      <td>0.13690</td>
      <td>0.17580</td>
      <td>0.13160</td>
      <td>0.09140</td>
      <td>0.3101</td>
      <td>0.07007</td>
      <td>B</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>866674</th>
      <td>19.79</td>
      <td>25.12</td>
      <td>130.40</td>
      <td>1192.0</td>
      <td>0.10150</td>
      <td>0.15890</td>
      <td>0.25450</td>
      <td>0.114900</td>
      <td>0.2202</td>
      <td>0.06113</td>
      <td>...</td>
      <td>33.58</td>
      <td>148.70</td>
      <td>1589.0</td>
      <td>0.12750</td>
      <td>0.38610</td>
      <td>0.56730</td>
      <td>0.17320</td>
      <td>0.3305</td>
      <td>0.08465</td>
      <td>M</td>
    </tr>
    <tr>
      <th>869254</th>
      <td>10.75</td>
      <td>14.97</td>
      <td>68.26</td>
      <td>355.3</td>
      <td>0.07793</td>
      <td>0.05139</td>
      <td>0.02251</td>
      <td>0.007875</td>
      <td>0.1399</td>
      <td>0.05688</td>
      <td>...</td>
      <td>20.72</td>
      <td>77.79</td>
      <td>441.2</td>
      <td>0.10760</td>
      <td>0.12230</td>
      <td>0.09755</td>
      <td>0.03413</td>
      <td>0.2300</td>
      <td>0.06769</td>
      <td>B</td>
    </tr>
    <tr>
      <th>859717</th>
      <td>17.20</td>
      <td>24.52</td>
      <td>114.20</td>
      <td>929.4</td>
      <td>0.10710</td>
      <td>0.18300</td>
      <td>0.16920</td>
      <td>0.079440</td>
      <td>0.1927</td>
      <td>0.06487</td>
      <td>...</td>
      <td>33.82</td>
      <td>151.60</td>
      <td>1681.0</td>
      <td>0.15850</td>
      <td>0.73940</td>
      <td>0.65660</td>
      <td>0.18990</td>
      <td>0.3313</td>
      <td>0.13390</td>
      <td>M</td>
    </tr>
    <tr>
      <th>88249602</th>
      <td>14.03</td>
      <td>21.25</td>
      <td>89.79</td>
      <td>603.4</td>
      <td>0.09070</td>
      <td>0.06945</td>
      <td>0.01462</td>
      <td>0.018960</td>
      <td>0.1517</td>
      <td>0.05835</td>
      <td>...</td>
      <td>30.28</td>
      <td>98.27</td>
      <td>715.5</td>
      <td>0.12870</td>
      <td>0.15130</td>
      <td>0.06231</td>
      <td>0.07963</td>
      <td>0.2226</td>
      <td>0.07617</td>
      <td>B</td>
    </tr>
    <tr>
      <th>854941</th>
      <td>13.03</td>
      <td>18.42</td>
      <td>82.61</td>
      <td>523.8</td>
      <td>0.08983</td>
      <td>0.03766</td>
      <td>0.02562</td>
      <td>0.029230</td>
      <td>0.1467</td>
      <td>0.05863</td>
      <td>...</td>
      <td>22.81</td>
      <td>84.46</td>
      <td>545.9</td>
      <td>0.09701</td>
      <td>0.04619</td>
      <td>0.04833</td>
      <td>0.05013</td>
      <td>0.1987</td>
      <td>0.06169</td>
      <td>B</td>
    </tr>
  </tbody>
</table>
<p>426 rows × 31 columns</p>
</div>

```python
# 타겟별 악성과 양성 간의 변수별 분포가 어떻게 이루어져있는지 보자
import matplotlib.pyplot as plt
import seaborn as sns
```

```python
# 변수별 히스토그램 생성
# 악성, 양성을 타겟별로 분포를 구분하여 plotting
fig, axes = plt.subplots(10, 3, figsize=(20, 40))
axes = axes.flatten()

for i in range(30):
    sns.histplot(data=train_df, x=train_df.columns[i], hue='diagnosis', multiple='layer', ax=axes[i])

plt.show()
```

<img width="1082" alt="plt1" src="https://github.com/zacinthepark/TIL/assets/86648892/07eb66d0-c780-4826-a572-2769d0369289">
<img width="1076" alt="plt2" src="https://github.com/zacinthepark/TIL/assets/86648892/e6287e49-4200-493e-a848-6aba49cb561c">
<img width="1082" alt="plt3" src="https://github.com/zacinthepark/TIL/assets/86648892/451fa6e2-7dbc-49b5-af83-ee862766fc44">
<img width="1068" alt="plt4" src="https://github.com/zacinthepark/TIL/assets/86648892/8e1009ec-e618-4170-b261-7f8054fc208a">
<img width="1063" alt="plt5" src="https://github.com/zacinthepark/TIL/assets/86648892/5f15ab52-ffa4-4e84-9564-1d4269731313">
<img width="1067" alt="plt6" src="https://github.com/zacinthepark/TIL/assets/86648892/75aa083a-b711-4daa-82e8-a4bf199ea61b">
<img width="1078" alt="plt7" src="https://github.com/zacinthepark/TIL/assets/86648892/6961a4f8-3d77-40cd-b692-e904ff941358">
<img width="1079" alt="plt8" src="https://github.com/zacinthepark/TIL/assets/86648892/05703614-78cd-47ee-bb17-6ee8c6c24ad0">
<img width="1091" alt="plt9" src="https://github.com/zacinthepark/TIL/assets/86648892/9604520a-d0fd-4e15-8556-c9f270a616ee">
<img width="1076" alt="plt10" src="https://github.com/zacinthepark/TIL/assets/86648892/5690485d-9032-46ab-baa3-44554800ef57">

##### 유의할 것이라고 예상되는 변수

- radius1, perimeter1, area1, concave_points1, radius3, perimeter3, area3, concave_points3 등

##### 유의하지 않을 것이라 예상되는 변수

- smoothness1, symmetry1, fractal_dimension1, texture2, smoothness2, symmetry2, fractal_dimension2 등

#### RFE 기반 주요 특징 선택 (Wrapper 방식의 대표적 방안)

```python
from sklearn.svm import SVC
from sklearn.feature_selection import RFE
```

##### SVM 기반의 RFE 방식 Reference

Guyon, Isabelle, et al. "Gene selection for cancer classification using support vector machines." Machine learning 46.1-3 (2002): 389-422.

- 위 논문은 SVM을 기반의 RFE를 적용하여 대장암과 관련된 유전자 선별을 진행한 연구
- 4개의 유전자를 최종적으로 선택하여 classification accuracy를 86%에서 98% 개선하였다고 결과 발표

```python
# SVM 기반 RFE 수행
# 스케일에 민감한 SVM의 특징에 따라 변수의 scaling을 따로 진행
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)
scale_X_train = scaler.transform(X_train)
```

```python
# RFE를 적용할 모델 SVM 지정
estimator_mdl = SVC(kernel='linear')
# SVM 학습 기반의 RFE 실행 및 유의미한 개수 지정: 5개
svm_rfe = RFE(estimator=estimator_mdl, n_features_to_select=5)

# SVM 기반 RFE 모델을 scaling한 훈련 데이터의 입력 변수들과 타겟 변수를 활용해 적용
svm_rfe_rst = svm_rfe.fit(scale_X_train, y_train.values.ravel())
# 어떠한 컬럼이 선택되었는지 확인
svm_rfe_rst.ranking_
# 1로 나온 변수가 최종 유의미한 특징으로 도출된 컬럼
```

<pre>
array([17, 23, 11, 16, 20, 25, 14,  9, 21, 13, 12, 24, 18,  1, 15, 10, 22,
        6, 26,  7,  1,  1,  2,  1, 19,  4,  3,  1,  5,  8])
</pre>

```python
# 도출한 특징 조합으로 test 진행
# train과 test를 동일한 기준으로 scaling
# 스케일링한 테스트 데이터를 적용해서 훈련한 SVM RFE 모델로 예측
scale_X_test = scaler.transform(X_test)
prediction = pd.DataFrame(svm_rfe.predict(scale_X_test), columns = ['pred_rst'])
prediction
```

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>pred_rst</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>B</td>
    </tr>
    <tr>
      <th>1</th>
      <td>M</td>
    </tr>
    <tr>
      <th>2</th>
      <td>B</td>
    </tr>
    <tr>
      <th>3</th>
      <td>M</td>
    </tr>
    <tr>
      <th>4</th>
      <td>M</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
    </tr>
    <tr>
      <th>138</th>
      <td>B</td>
    </tr>
    <tr>
      <th>139</th>
      <td>M</td>
    </tr>
    <tr>
      <th>140</th>
      <td>M</td>
    </tr>
    <tr>
      <th>141</th>
      <td>M</td>
    </tr>
    <tr>
      <th>142</th>
      <td>B</td>
    </tr>
  </tbody>
</table>
<p>143 rows × 1 columns</p>
</div>

```python
# 예측 성능 평가
# 이를 위해 문자로 이루어진 타겟 변수들을 수치형으로 타입 변경
y_test['diagnosis'] = y_test['diagnosis'].replace('M', 1)
y_test['diagnosis'] = y_test['diagnosis'].replace('B', 0)
prediction['pred_rst'] = prediction['pred_rst'].replace('M', 1)
prediction['pred_rst'] = prediction['pred_rst'].replace('B', 0)
```

```python
# 결과 확인
# 모델의 성능 평가를 할 때의 주요 평가 지표로 accuracy, auc가 있음

# accuracy
from sklearn.metrics import accuracy_score
print('accuracy: ', round(accuracy_score(y_test['diagnosis'], prediction['pred_rst']), 5))

# auc
from sklearn.metrics import roc_auc_score
print('auc: ', round(roc_auc_score(y_test['diagnosis'], prediction['pred_rst']), 5))
```

<pre>
accuracy:  0.95105
auc:  0.94659
</pre>

```python
# 동일한 SVM RFE 수행하되 10개의 특징을 선택 방안으로 실행
estimator_mdl = SVC(kernel='linear')

re_svm_rfe = RFE(estimator=estimator_mdl, n_features_to_select=10)
re_svm_rfe_rst = re_svm_rfe.fit(scale_X_train, y_train.values.ravel())

re_prediction = pd.DataFrame(re_svm_rfe.predict(scale_X_test), columns = ['pred_rst'])
re_prediction['pred_rst'] = re_prediction['pred_rst'].replace('M', 1)
re_prediction['pred_rst'] = re_prediction['pred_rst'].replace('B', 0)

# accuracy
from sklearn.metrics import accuracy_score
print('accuracy: ', round(accuracy_score(y_test['diagnosis'], re_prediction['pred_rst']), 5))

# auc
from sklearn.metrics import roc_auc_score
print('auc: ', round(roc_auc_score(y_test['diagnosis'], re_prediction['pred_rst']), 5))
```

<pre>
accuracy:  0.93706
auc:  0.93523
</pre>

##### 10개 특징 조합보다 5개 특징 조합이 더 정확함을 확인

```python
# 최종 선택된 5개의 특징 컬럼이 어떠한 컬럼인지 확인
chk_var = []
for name, svm_rfe_rst.ranking_ in zip(X_train.columns, svm_rfe_rst.ranking_):  # 같이 묶어서 호출
    lst_chk = [name, svm_rfe_rst.ranking_]
    chk_var.append(lst_chk)

chk_svm_rfe = pd.DataFrame(chk_var, columns=['feature_names', 'svm_rfe_feature'])
chk_svm_rfe = chk_svm_rfe[chk_svm_rfe['svm_rfe_feature']==1]
chk_svm_rfe
```

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature_names</th>
      <th>svm_rfe_feature</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>13</th>
      <td>area2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>20</th>
      <td>radius3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>21</th>
      <td>texture3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>23</th>
      <td>area3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>27</th>
      <td>concave_points3</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

##### 시각화 기반 유의할 것이라고 예상되었던 특징

- radius1, perimeter1, area1, concave_points1, radius3, perimeter3, area3, concave_points3 등

##### SVM-RFE 기반 판단

- 위 특징 중 radius3, area3, concave_points3 등 3개 특징이 포함됨
    - SVM 이외에도 decision tree, logistic regression 등의 알고리즘을 자유롭게 활용하여 RFE 가능
    - RFE 내에서 사용된 모델이 어떤 특징을 선택하고 예측 문제에 대한 성능을 결정하는데 중요한 차이를 만들 수 있음

#### Embedded 기반 특징 선택

- 모델 학습 성능에 기여하는 특징 선택하는 방안

```python
# RandomForest 모형 기반 특징 선택
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier
```

```python
# RF 모형 생성
# 나무 개수, 즉 estimator의 개수가 100개인 RF 모형을 통해 특징 추출
# 나무의 개수가 많아질수록 좋은 성능을 기대할 수는 있지만, 연산량이 많아지기에 수행 시간은 비례적으로 길어짐
embedded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=1))
embedded_rf_selector.fit(scale_X_train, y_train.values.ravel())
```

<img width="314" alt="embedded_rf" src="https://github.com/zacinthepark/TIL/assets/86648892/5c3ee57b-381a-452b-9c50-d850538e3169">

```python
# RF 기반 Embedded 특징 선택 결과
# RF 기반 특징 선택 개체에서 get_support() 입력하면 데이터프레임 내 컬럼 수를 기반으로 True, False 값을 도출함
embedded_rf_support = embedded_rf_selector.get_support()
embedded_rf_feature = X_train.loc[:,embedded_rf_support].columns.tolist()
print(str(len(embedded_rf_feature)), 'selected features')
```

<pre>
9 selected features
</pre>

```python
# 선택된 9개 특징
embedded_rf_feature
```

<pre>
['radius1',
 'perimeter1',
 'area1',
 'concavity1',
 'concave_points1',
 'radius3',
 'perimeter3',
 'area3',
 'concave_points3']
</pre>

##### 시각화 기반 유의할 것이라고 예상되었던 특징

- radius1, perimeter1, area1, concave_points1, radius3, perimeter3, area3, concave_points3 등

##### Embedded (Random Forest) 기반 판단

- 위 특징 중 radius1, perimeter1, area1, concave_points1, radius3, perimeter3, area3, concave_points3 등 8개 특징이 포함됨

#### Boruta Algorithm

- Boruta Algorithm은 랜덤포레스트 모형 기반 특징 선택 알고리즘
- 기존 데이터를 임의로 복제하여 랜덤변수(Shadow 변수)를 생성하고 그보다 낮은 중요도를 지닌 특징을 제외함

```python
from boruta import BorutaPy
```

```python
# boruta 알고리즘은 랜덤포레스트 모형 기반이므로 먼저 RF 모형 설정
rf = RandomForestClassifier(random_state=1)

# boruta 기반 특징 선택
boruta_selector = BorutaPy(rf, n_estimators='auto', random_state=1)

# 'auto'인 경우는 자동으로 데이터셋 사이즈를 고려하여 자동 설정
boruta_selector.fit(scale_X_train, y_train.values.ravel())
boruta_selector.ranking_
```

<pre>
array([1, 1, 1, 1, 1, 1, 1, 1, 3, 5, 1, 5, 1, 1, 7, 1, 1, 1, 6, 3, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 2])
</pre>

```python
# 최종 선택된 특징 컬럼이 어떠한 컬럼인지 확인
chk_var_boruta = []
zip(X_train.columns, boruta_selector.ranking_)
for name, rank in zip(X_train.columns, boruta_selector.ranking_):  # 같이 묶어서 호출
    lst_chk = [name, rank]
    chk_var_boruta.append(lst_chk)

chk_boruta = pd.DataFrame(chk_var_boruta, columns=['feature_names', 'boruta_feature'])
chk_boruta
```

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature_names</th>
      <th>boruta_feature</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>radius1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>texture1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>perimeter1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>area1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>smoothness1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>compactness1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>6</th>
      <td>concavity1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>concave_points1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>8</th>
      <td>symmetry1</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>fractal_dimentsion1</td>
      <td>5</td>
    </tr>
    <tr>
      <th>10</th>
      <td>radius2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>11</th>
      <td>texture2</td>
      <td>5</td>
    </tr>
    <tr>
      <th>12</th>
      <td>perimeter2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>13</th>
      <td>area2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>14</th>
      <td>smoothness2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>15</th>
      <td>compactness2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>16</th>
      <td>concavity2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>17</th>
      <td>concave_points2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>18</th>
      <td>symmetry2</td>
      <td>6</td>
    </tr>
    <tr>
      <th>19</th>
      <td>fractal_dimension2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>radius3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>21</th>
      <td>texture3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22</th>
      <td>perimeter3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>23</th>
      <td>area3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>24</th>
      <td>smoothness3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>25</th>
      <td>compactness3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>26</th>
      <td>concavity3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>27</th>
      <td>concave_points3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>28</th>
      <td>symmetry3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>29</th>
      <td>fractal_dimension3</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>

```python
# Boruta에 의해 선택된 특징 (23개)
chk_boruta = chk_boruta[chk_boruta['boruta_feature'] == 1]
chk_boruta
```

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature_names</th>
      <th>boruta_feature</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>radius1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>texture1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>perimeter1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>area1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>smoothness1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>compactness1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>6</th>
      <td>concavity1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>concave_points1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>10</th>
      <td>radius2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>12</th>
      <td>perimeter2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>13</th>
      <td>area2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>15</th>
      <td>compactness2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>16</th>
      <td>concavity2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>17</th>
      <td>concave_points2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>20</th>
      <td>radius3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>21</th>
      <td>texture3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22</th>
      <td>perimeter3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>23</th>
      <td>area3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>24</th>
      <td>smoothness3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>25</th>
      <td>compactness3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>26</th>
      <td>concavity3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>27</th>
      <td>concave_points3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>28</th>
      <td>symmetry3</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

```python
# 최종 제외된 특징들 리스트 확인
non_chk_boruta = pd.DataFrame(chk_var_boruta, columns=['feature_names', 'boruta_feature'])
non_chk_boruta = non_chk_boruta[non_chk_boruta['boruta_feature'] != 1]
non_chk_boruta
```

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature_names</th>
      <th>boruta_feature</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>8</th>
      <td>symmetry1</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>fractal_dimentsion1</td>
      <td>5</td>
    </tr>
    <tr>
      <th>11</th>
      <td>texture2</td>
      <td>5</td>
    </tr>
    <tr>
      <th>14</th>
      <td>smoothness2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>18</th>
      <td>symmetry2</td>
      <td>6</td>
    </tr>
    <tr>
      <th>19</th>
      <td>fractal_dimension2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>29</th>
      <td>fractal_dimension3</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>

##### 시각화 기반 유의하지 않을 것이라 예상되었던 특징

- smoothness1, symmetry1, fractal_dimension1, texture2, smoothness2, symmetry2, fractal_dimension2 등

##### Boruta 기반 판단 시

- symmetry1, fractal_dimension1, texture2, smoothness2, symmetry2, fractal_dimension2 등 6개 특징 포함됨
