### 크롤링 정책

- `robots.txt` : 크롤링 정책을 설명한 페이지
  - `https://www.ted.com/robots.txt`

- 과도한 크롤링으로 서비스에 영향을 주었을때 법적 문제가 있을수 있습니다.

- 사람인, 잡코리아 : 2008년 > 2018년

- api 사용 > `robots.txt` > 서비스에 피해가 가지 않는 선에서 수집

- 서비스 피해

    - 지적재산권
    - 서비스 과부화
    - 데이터 사용표준

```python
# robots.txt
# 웹 서비스에서 제공하는 api 사용이 가장 좋은 방법
# sniffing
# 동적, 정적 페이지
# 개발자도구는 모든 브라우저에서 사용가능
```

```python
# summary

# web : client-server : url
# python : requests : request > response
# 동적페이지 : URL의 변화 없이 페이지의 데이터를 변환 : json
# 정적페이지 : URL을 변환해서 새로운 데이터를 출력 : html

# 동적 페이지에서 데이터 수집 절차
# 1. 웹서비스분석(개발자도구) : URL
# 2. request(url, params, headers) > response(data) : data(json(str))
# 3. data(json(str)) > list, dict > DataFrame

# API를 이용한 데이터 수집
# 1. 어플리케이션 등록 : app_key
# 2. api 문서 확인 : url, params, headers
# 3. request(url, params, headers(app_key)) > response(data) : data(json(str))
# 4. data(json(str)) > list, dict > DataFrame

# 정적 페이지에서 데이터 수집 절차
# 1. 웹서비스분석(개발자도구) : URL
# 2. request(url, params, headers) > response(data) : data(html(str))
# 3. data(html(str)) > bs object > css selector > DataFrame
```

```python
# class Marine
# naver stock : index price, exchange rate, correlation coefficient analysis
# daum exchange rate : 403 error, headers(user-agent, referer)
# naver api : translate
# zigbang : requests 3번, geohash
```
